%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Ben Crestel at 2020-06-02 21:58:15 -0400 


%% Saved with string encoding Unicode (UTF-8) 


@comment{jabref-meta: databaseType:bibtex;}



@article{weinan2017proposal,
	Abstract = {We discuss the idea of using continuous dynamical systems to model general
high-dimensional nonlinear functions used in machine learning. We also discuss the
connection with deep learning.},
	Author = {Weinan, E},
	Date-Added = {2020-06-02 21:57:32 -0400},
	Date-Modified = {2020-06-02 21:58:07 -0400},
	Journal = {Communications in Mathematics and Statistics},
	Keywords = {neural ODE},
	Number = {1},
	Pages = {1--11},
	Publisher = {Springer},
	Title = {A proposal on machine learning via dynamical systems},
	Volume = {5},
	Year = {2017},
	Bdsk-Url-1 = {https://link.springer.com/content/pdf/10.1007/s40304-017-0103-z.pdf}}

@article{haber2017stable,
	Abstract = {Deep neural networks have become invaluable tools for supervised machine learning, e.g.,
classification of text or images. While often offering superior results over traditional techniques
and successfully expressing complicated patterns in data, deep architectures are known to be
challenging to design and train such that they generalize well to new data. Critical issues with
deep architectures are numerical instabilities in derivative-based learning algorithms commonly
called exploding or vanishing gradients. In this paper, we propose new forward propagation
techniques inspired by systems of Ordinary Differential Equations (ODE) that overcome this
challenge and lead to well-posed learning problems for arbitrarily deep networks.
The backbone of our approach is our interpretation of deep learning as a parameter estimation problem of nonlinear dynamical systems. Given this formulation, we analyze stability
and well-posedness of deep learning and use this new understanding to develop new network
architectures. We relate the exploding and vanishing gradient phenomenon to the stability of
the discrete ODE and present several strategies for stabilizing deep learning for very deep networks. While our new architectures restrict the solution space, several numerical experiments
show their competitiveness with state-of-the-art networks.},
	Author = {Haber, Eldad and Ruthotto, Lars},
	Date-Added = {2020-06-02 21:55:57 -0400},
	Date-Modified = {2020-06-02 21:56:24 -0400},
	Journal = {Inverse Problems},
	Keywords = {neural ODE},
	Number = {1},
	Pages = {014004},
	Publisher = {IOP Publishing},
	Title = {Stable architectures for deep neural networks},
	Volume = {34},
	Year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1705.03341.pdf}}

@inproceedings{chen2018neural,
	Abstract = {We introduce a new family of deep neural network models. Instead of specifying a
discrete sequence of hidden layers, we parameterize the derivative of the hidden
state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant
memory cost, adapt their evaluation strategy to each input, and can explicitly trade
numerical precision for speed. We demonstrate these properties in continuous-depth
residual networks and continuous-time latent variable models. We also construct
continuous normalizing flows, a generative model that can train by maximum
likelihood, without partitioning or ordering the data dimensions. For training, we
show how to scalably backpropagate through any ODE solver, without access to its
internal operations. This allows end-to-end training of ODEs within larger models.},
	Author = {Chen, Tian Qi and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
	Booktitle = {Advances in neural information processing systems},
	Date-Added = {2020-06-02 15:28:55 -0400},
	Date-Modified = {2020-06-02 15:29:22 -0400},
	Keywords = {neural ODE},
	Pages = {6571--6583},
	Title = {Neural ordinary differential equations},
	Year = {2018},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/7892-neural-ordinary-differential-equations.pdf}}

@article{gholami2019anode,
	Abstract = {Residual neural networks can be viewed as the
forward Euler discretization of an Ordinary Differential Equation (ODE) with a unit time step. This has recently motivated
researchers to explore other discretization approaches and train
ODE based networks. However, an important challenge of neural
ODEs is their prohibitive memory cost during gradient backpropogation. Recently a method proposed in [8], claimed that
this memory overhead can be reduced from O(LNt), where
Nt is the number of time steps, down to O(L) by solving
forward ODE backwards in time, where L is the depth of
the network. However, we will show that this approach may
lead to several problems: (i) it may be numerically unstable for
ReLU/non-ReLU activations and general convolution operators,
and (ii) the proposed optimize-then-discretize approach may lead
to divergent training due to inconsistent gradients for small
time step sizes. We discuss the underlying problems, and to
address them we propose ANODE, an Adjoint based Neural
ODE framework which avoids the numerical instability related
problems noted above, and provides unconditionally accurate
gradients. ANODE has a memory footprint of O(L) + O(Nt),
with the same computational cost as reversing ODE solve. We
furthermore, discuss a memory efficient algorithm which can
further reduce this footprint with a trade-off of additional
computational cost. We show results on Cifar-10/100 datasets
using ResNet and SqueezeNext neural networks.},
	Author = {Gholami, Amir and Keutzer, Kurt and Biros, George},
	Date-Added = {2020-06-02 15:24:13 -0400},
	Date-Modified = {2020-06-02 15:24:46 -0400},
	Journal = {arXiv preprint arXiv:1902.10298},
	Keywords = {neural ODE},
	Title = {Anode: Unconditionally accurate memory-efficient gradients for neural odes},
	Year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1902.10298.pdf}}

@article{onken2020discretize,
	Abstract = {We compare the discretize-optimize (Disc-Opt) and optimize-discretize (Opt-Disc) approaches for
time-series regression and continuous normalizing flows using neural ODEs. Neural ODEs, first
described in Chen et al. (2018), are ordinary differential equations (ODEs) with neural network
components; these models have competitively solved a variety of machine learning applications.
Training a neural ODE can be phrased as an optimal control problem where the neural network
weights are the controls and the hidden features are the states. Every iteration of gradient-based
training involves solving an ODE forward in time and another backward in time, which can require
large amounts of computation, time, and memory. Gholami et al. (2019) compared the Opt-Disc
and Disc-Opt approaches for neural ODEs arising as continuous limits of residual neural networks
used in image classification tasks. Their findings suggest that Disc-Opt achieves preferable performance due to the guaranteed accuracy of gradients. In this paper, we extend this comparison to
neural ODEs applied to time-series regression and continuous normalizing flows (CNFs). Timeseries regression and CNFs differ from classification in that the actual ODE model is needed in the
prediction and inference phase, respectively. Meaningful models must also satisfy additional requirements, e.g., the invertibility of the CNF. As the continuous model satisfies these requirements
by design, Opt-Disc approaches may appear advantageous. Through our numerical experiments,
we demonstrate that with careful numerical treatment, Disc-Opt methods can achieve similar performance as Opt-Disc at inference with drastically reduced training costs. Disc-Opt reduced costs
in six out of seven separate problems with training time reduction ranging from 39% to 97%, and
in one case, Disc-Opt reduced training from nine days to less than one day.},
	Author = {Onken, Derek and Ruthotto, Lars},
	Date-Added = {2020-06-02 15:23:00 -0400},
	Date-Modified = {2020-06-02 15:23:30 -0400},
	Journal = {arXiv preprint arXiv:2005.13420},
	Keywords = {neural ODE},
	Title = {Discretize-Optimize vs. Optimize-Discretize for Time-Series Regression and Continuous Normalizing Flows},
	Year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2005.13420.pdf}}

@article{hatalis2017smooth,
	Abstract = {Uncertainty analysis in the form of probabilistic
forecasting can significantly improve decision making processes
in the smart power grid for better integrating renewable energy sources such as wind. Whereas point forecasting provides
a single expected value, probabilistic forecasts provide more
information in the form of quantiles, prediction intervals, or
full predictive densities. This paper analyzes the effectiveness
of a novel approach for nonparametric probabilistic forecasting
of wind power that combines a smooth approximation of the
pinball loss function with a neural network architecture and
a weighting initialization scheme to prevent the quantile cross
over problem. A numerical case study is conducted using publicly available wind data from the Global Energy Forecasting
Competition 2014. Multiple quantiles are estimated to form
10%, to 90% prediction intervals which are evaluated using a
quantile score and reliability measures. Benchmark models such
as the persistence and climatology distributions, multiple quantile
regression, and support vector quantile regression are used for
comparison where results demonstrate the proposed approach
leads to improved performance while preventing the problem of
overlapping quantile estimates},
	Annote = {It seems that this paper just put together a few pieces, but it's an interesting summary.
In particular, they discuss:
* a smooth approximation to the pinball loss, for more efficient training
* a ranking technique to work around quantile crossing},
	Author = {Hatalis, Kostas and Lamadrid, Alberto J and Scheinberg, Katya and Kishore, Shalinee},
	Date-Added = {2020-06-02 10:19:55 -0400},
	Date-Modified = {2020-06-02 10:21:32 -0400},
	Journal = {arXiv preprint arXiv:1710.01720},
	Keywords = {uncertainty, pinball loss, smooth approximation},
	Read = {1},
	Title = {Smooth pinball neural network for probabilistic forecasting of wind power},
	Year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1710.01720.pdf}}

@inproceedings{lakshminarayanan2017simple,
	Abstract = {Deep neural networks (NNs) are powerful black box predictors that have recently
achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian
NNs, which learn a distribution over weights, are currently the state-of-the-art
for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to
standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that
is simple to implement, readily parallelizable, requires very little hyperparameter
tuning, and yields high quality predictive uncertainty estimates. Through a series
of experiments on classification and regression benchmarks, we demonstrate that
our method produces well-calibrated uncertainty estimates which are as good or
better than approximate Bayesian NNs. To assess robustness to dataset shift, we
evaluate the predictive uncertainty on test examples from known and unknown
distributions, and show that our method is able to express higher uncertainty on
out-of-distribution examples. We demonstrate the scalability of our method by
evaluating predictive uncertainty estimates on ImageNet.},
	Author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
	Booktitle = {Advances in neural information processing systems},
	Date-Added = {2020-05-28 08:18:21 -0400},
	Date-Modified = {2020-05-28 08:19:00 -0400},
	Keywords = {uncertainty, ensemble},
	Pages = {6402--6413},
	Title = {Simple and scalable predictive uncertainty estimation using deep ensembles},
	Year = {2017},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/7219-simple-and-scalable-predictive-uncertainty-estimation-using-deep-ensembles.pdf}}

@inproceedings{liu2019accurate,
	Abstract = {Ensemble learning is a standard approach to building machine learning systems
that capture complex phenomena in real-world data. An important aspect of
these systems is the complete and valid quantification of model uncertainty. We
introduce a Bayesian nonparametric ensemble (BNE) approach that augments an
existing ensemble model to account for different sources of model uncertainty.
BNE augments a model's prediction and distribution functions using Bayesian
nonparametric machinery. It has a theoretical guarantee in that it robustly estimates
the uncertainty patterns in the data distribution, and can decompose its overall
predictive uncertainty into distinct components that are due to different sources of
noise and error. We show that our method achieves accurate uncertainty estimates
under complex observational noise, and illustrate its real-world utility in terms of
uncertainty decomposition and model bias detection for an ensemble in predict air
pollution exposures in Eastern Massachusetts, USA.},
	Annote = {They provide a complete decomposition of uncertainty in their model, which they illustrate in a nice diagram

							Overall Uncertainty
						/					\
			epistemic							aleatoric
			/		\
	parametric			structural
						/		\
					prediction		distribution
},
	Author = {Liu, Jeremiah and Paisley, John and Kioumourtzoglou, Marianthi-Anna and Coull, Brent},
	Booktitle = {Advances in Neural Information Processing Systems},
	Date-Added = {2020-05-27 11:28:17 -0400},
	Date-Modified = {2020-05-27 11:31:53 -0400},
	Keywords = {uncertainty, bayesian, ensemble},
	Pages = {8950--8961},
	Title = {Accurate Uncertainty Estimation and Decomposition in Ensemble Learning},
	Year = {2019},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/9097-accurate-uncertainty-estimation-and-decomposition-in-ensemble-learning.pdf}}

@inproceedings{tagasovska2019single,
	Abstract = {We provide single-model estimates of aleatoric and epistemic uncertainty for deep
neural networks. To estimate aleatoric uncertainty, we propose Simultaneous
Quantile Regression (SQR), a loss function to learn all the conditional quantiles
of a given target variable. These quantiles can be used to compute well-calibrated
prediction intervals. To estimate epistemic uncertainty, we propose Orthonormal
Certificates (OCs), a collection of diverse non-constant functions that map all
training samples to zero. These certificates map out-of-distribution examples to
non-zero values, signaling epistemic uncertainty. Our uncertainty estimators are
computationally attractive, as they do not require ensembling or retraining deep
models, and achieve competitive performance.},
	Annote = {They propose a model to handle both epistemic and aleatoric uncertainty.

y_true - y_predict,W = F_true(x1, x2)  - F_W*(x1)	=> approximation uncertainty
				+ F_W*(x1) - F_W(x1)		=> epistemic uncertainty
				+ epsilon					=> aleatoric uncertainty

They also mention approximation uncertainty, which could be defined as model misspecification. That is,
the erorr due to the fact that the model doesn't have enought capacity. In their own words, 
the ``approximation uncertainty describes the errors made by simplistic models unable to fit complex data 
(e.g., the error made by a linear model fitting a sinusoidal curve)''. However, they make the assumption that this
error is always zero since they focus on deep learning models that are known to have large capacity.

Note: I think there is also the case of hidden variables, and this could happen to any model. 
Some authors (e.g., WIkipedia) consider the case of hidden variables to be epistemic uncertainty, probably because
this is something we may be able to act upon (find out that data and measure it). This could be reconciled
if we consider approximation uncertainty as a special case of epistemic uncertainty (understood in a broad sense). That's
exactly what they do in the paper ``Accurate Uncertainty Estimation and Decomposition in Ensemble Learning''.

Aleatoric uncertainty:
They define aleatoric uncertainty as the uncertainty that accounts for the sotchasticity of the data.
They propose a method called Simultaneous Quantile Regression (SQR) which attempts to estimate all quantiles at once. 
It does so by training for different quantiles, chosent randomly at train time.

Epistemic uncertainty:
They say: ``epistemic uncertainty (from the Greek word episteme, meaning
``knowledge'') describes the errors associated to the lack of experience of our model at certain regions
of the feature space. Therefore, epistemic uncertainty is inversely proportional to the density of
training examples, and could be reduced by collecting data in those low density regions.''
Later in the paper, they clarify their position as the one of ``out-of-domain'' data, that is data
that was seen during training.
I think epistmic uncertainty, in general, includes everything you could theoretically act upon, or
throw more money at it (more data collection, more compute power,{\ldots}). But probably they assume
that everything within our control has already been tried. So only remains the lack of sufficient data 
in certain region of the data space, i.e., out-of-distribution samples.
How they handle epistemic uncertainty: they train certificates, that is functions that are trained to return
0 for all in-sample data and 1 for out-of-sample data, but are only trained on in-sample data (1-class classification).
They train an ensemble of certificates. You can use linear certificates or more complicated non-linear models. When 
using more complicated models (like DL), you need to constrain the certificates to prevent having them learn the zero map.
The solution is to impose orthogonality.
They apply their technique to out-of-dsitribution detection, and compare favorably with the BALD method (used in active learning).
However, I don't see how that technique could be used to estimate the variance error due to the epistemic uncertainty. Which is
what we would need to do uncertainty quantification.},
	Author = {Tagasovska, Natasa and Lopez-Paz, David},
	Booktitle = {Advances in Neural Information Processing Systems},
	Date-Added = {2020-05-27 08:25:52 -0400},
	Date-Modified = {2020-05-27 12:01:45 -0400},
	Pages = {6414--6425},
	Read = {1},
	Title = {Single-Model Uncertainties for Deep Learning},
	Year = {2019},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/8870-single-model-uncertainties-for-deep-learning.pdf}}

@article{krueger2016zoneout,
	Abstract = {We propose zoneout, a novel method for regularizing RNNs. At each timestep,
zoneout stochastically forces some hidden units to maintain their previous values.
Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving
generalization. But by preserving instead of dropping hidden units, gradient
information and state information are more readily propagated through time, as
in feedforward stochastic depth networks. We perform an empirical investigation
of various RNN regularizers, and find that zoneout gives significant performance
improvements across tasks. We achieve competitive results with relatively simple
models in character- and word-level language modelling on the Penn Treebank
and Text8 datasets, and combining with recurrent batch normalization (Cooijmans
et al., 2016) yields state-of-the-art results on permuted sequential MNIST},
	Author = {Krueger, David and Maharaj, Tegan and Kram{\'a}r, J{\'a}nos and Pezeshki, Mohammad and Ballas, Nicolas and Ke, Nan Rosemary and Goyal, Anirudh and Bengio, Yoshua and Courville, Aaron and Pal, Chris},
	Date-Added = {2020-05-26 22:19:34 -0400},
	Date-Modified = {2020-05-26 22:20:31 -0400},
	Journal = {arXiv preprint arXiv:1606.01305},
	Keywords = {regularization, rnn},
	Title = {Zoneout: Regularizing rnns by randomly preserving hidden activations},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1606.01305.pdf}}

@inproceedings{zhu2017deep,
	Abstract = {Reliable uncertainty estimation for time series prediction is critical in many fields, including physics, biology,
and manufacturing. At Uber, probabilistic time series forecasting is used for robust prediction of number of trips during
special events, driver incentive allocation, as well as real-time
anomaly detection across millions of metrics. Classical time
series models are often used in conjunction with a probabilistic
formulation for uncertainty estimation. However, such models
are hard to tune, scale, and add exogenous variables to. Motivated by the recent resurgence of Long Short Term Memory
networks, we propose a novel end-to-end Bayesian deep model
that provides time series prediction along with uncertainty
estimation. We provide detailed experiments of the proposed
solution on completed trips data, and successfully apply it to
large-scale time series anomaly detection at Uber.},
	Annote = {Also see the blog post: https://eng.uber.com/neural-networks-uncertainty-estimation/

They detail a forecasting model to handle uncertainty (epistemic and aleatoric):
* epistemic uncertainty (defined as model uncertainty and model misspecification) is handled via Dropout
* aleatoric uncertainty is handled by a constant variance estimated as the empirical variance of the model error on a validation set. For the prediciton, I believe they use the same model but w/o Dropout

Overall the model has a pre-trained encoder-decoder part. The encoder is used to generate features that are passed to a fully-connected network.
Variational Dropout is applied to the encoder, while standard Dropout is applied to the fc model.
},
	Author = {Zhu, Lingxue and Laptev, Nikolay},
	Booktitle = {2017 IEEE International Conference on Data Mining Workshops (ICDMW)},
	Date-Added = {2020-05-26 20:52:07 -0400},
	Date-Modified = {2020-05-27 12:04:34 -0400},
	Keywords = {time series, forecasting, bayesian, uncertainty},
	Organization = {IEEE},
	Pages = {103--110},
	Read = {1},
	Title = {Deep and confident prediction for time series at uber},
	Year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1709.01907.pdf}}

@inproceedings{guo2017calibration,
	Abstract = {Confidence calibration -- the problem of predicting probability estimates representative of the
true correctness likelihood -- is important for
classification models in many applications. We
discover that modern neural networks, unlike
those from a decade ago, are poorly calibrated.
Through extensive experiments, we observe that
depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various
post-processing calibration methods on state-ofthe-art architectures with image and document
classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and
straightforward recipe for practical settings: on
most datasets, temperature scaling -- a singleparameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.},
	Author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
	Booktitle = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
	Date-Added = {2020-05-25 11:44:37 -0400},
	Date-Modified = {2020-05-25 11:45:15 -0400},
	Organization = {JMLR. org},
	Pages = {1321--1330},
	Title = {On calibration of modern neural networks},
	Year = {2017},
	Bdsk-Url-1 = {https://dl.acm.org/doi/pdf/10.5555/3305381.3305518?download=true}}

@inproceedings{vaswani2017attention,
	Abstract = {The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer,
based solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including
ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
our model establishes a new single-model state-of-the-art BLEU score of 41.0 after
training for 3.5 days on eight GPUs, a small fraction of the training costs of the
best models from the literature.},
	Author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	Booktitle = {Advances in neural information processing systems},
	Date-Added = {2020-05-25 10:27:11 -0400},
	Date-Modified = {2020-05-25 10:27:26 -0400},
	Pages = {5998--6008},
	Title = {Attention is all you need},
	Year = {2017},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}}

@article{rasul2020multi,
	Abstract = {Time series forecasting is often fundamental to
scientific and engineering problems and enables
decision making. With ever increasing data set
sizes, a trivial solution to scale up predictions is to
assume independence between interacting time series. However, modeling statistical dependencies
can improve accuracy and enable analysis of interaction effects. Deep learning methods are well
suited for this problem, but multi-variate models
often assume a simple parametric distribution and
do not scale to high dimensions. In this work
we model the multi-variate temporal dynamics of
time series via an autoregressive deep learning
model, where the data distribution is represented
by a conditioned normalizing flow. This combination retains the power of autoregressive models,
such as good performance in extrapolation into
the future, with the flexibility of flows as a general purpose high-dimensional distribution model,
while remaining computationally tractable. We
show that it improves over the state-of-the-art for
standard metrics on many real-world data sets
with several thousand interacting time-series},
	Annote = {Paper a bit hard to read. They have a blog post that is much clearer: https://research.zalando.com/2020/03/

In its simplest form, their model is a multivariate version (MIMO) of DeepAR that does not constrain on the distribution to sample from (DeepAR asks you to pick a fixed distribution). Instead their parametrize the sampling stage via a normalizing flow, that is a sequence of simple, invertible transformations that map a given sample (typically isotropic Gaussian) to the predicted sample. That normalizing flow is parametrized by the output of a LSTM (like DeepAR).

They also have a mode where they replace the RNN part with an attention-based model},
	Author = {Rasul, Kashif and Sheikh, Abdul-Saboor and Schuster, Ingmar and Bergmann, Urs and Vollgraf, Roland},
	Date-Added = {2020-05-25 10:20:39 -0400},
	Date-Modified = {2020-05-25 10:25:05 -0400},
	Journal = {arXiv preprint arXiv:2002.06103},
	Keywords = {time series, forecasting, distributional forecast,},
	Read = {1},
	Title = {Multi-variate Probabilistic Time Series Forecasting via Conditioned Normalizing Flows},
	Year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2002.06103.pdf}}

@article{chen2018confidence,
	Abstract = {We propose a novel confidence scoring mechanism for deep neural networks based on a
two-model paradigm involving a base model
and a meta-model. The confidence score is
learned by the meta-model observing the base
model succeeding/failing at its task. As features to the meta-model, we investigate linear
classifier probes inserted between the various
layers of the base model. Our experiments
demonstrate that this approach outperforms
multiple baselines in a filtering task, i.e., task
of rejecting samples with low confidence. Experimental results are presented using CIFAR10 and CIFAR-100 dataset with and without
added noise. We discuss the importance of
confidence scoring to bridge the gap between
experimental and real-world applications},
	Annote = {hey use Linear Classifier Probes [4] to extract inform-ation from within the hidden layers of an image classifier, and use these probes data, along withthe output of the classifier, to train a a confidence score predictor. They compare the select-ive prediction technique described in [24] against multiple variations of their model: with probesdata from the hidden layers or with only the output of the classifier; using a logistic regression ora gradient boosting model as their meta-model. The classifier and the meta-model are trainedon separate datasets. Performance is computed on a held-out test set. Besides this standard set-up, theyalsoattempttoverifytheperformanceoftheirconfidencescorepredictorunderdif-ferent regimes by corrupting their dataset. In one experiment, they introduce noise in the labelsof the training set, to reduce the accuracy of the classifier. In their second experiment, to assessthe performance of their confidence score under dataset shift, they pass, at test time, imageswith labels that were not in the training set. From their experiments, they conclude that a gradi-ent boosting model using the probes data slightly outperforms the other models. The differencein performance widens a bit more when testing on out-of-distribution data, and with a classifiertrained on noisy labels. They also tried to apply temperature scaling [27] as a pre-processingstep to the SGR method [24], but reported no improvement},
	Author = {Chen, Tongfei and Navr{\'a}til, Ji{\v{r}}{\'\i} and Iyengar, Vijay and Shanmugam, Karthikeyan},
	Date-Added = {2020-05-22 08:38:29 -0400},
	Date-Modified = {2020-05-22 08:48:13 -0400},
	Journal = {arXiv preprint arXiv:1805.05396},
	Keywords = {confidence score},
	Read = {1},
	Title = {Confidence scoring using whitebox meta-models with linear classifier probes},
	Year = {2018},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1805.05396.pdf}}

@inproceedings{geifman2017selective,
	Abstract = {Selective classification techniques (also known as reject option) have not yet been
considered in the context of deep neural networks (DNNs). These techniques
can potentially significantly improve DNNs prediction performance by trading-off
coverage. In this paper we propose a method to construct a selective classifier
given a trained neural network. Our method allows a user to set a desired risk
level. At test time, the classifier rejects instances as needed, to grant the desired
risk (with high probability). Empirical results over CIFAR and ImageNet convincingly demonstrate the viability of our method, which opens up possibilities to
operate DNNs in mission-critical applications. For example, using our method an
unprecedented 2% error in top-5 ImageNet classification can be guaranteed with
probability 99.9%, and almost 60% test coverage.},
	Annote = {The authors leverages the field of selective prediction (or reject option) to design a Selec-tionwithGuaranteedRisk(SGR)algorithmthatwillreturnaselectionfunctiongivenatrainedclas-sifier,arisktarget,aconfidenceparameter,andaconfidence-ratefunction. Thisconfidence-ratefunctions can be, for instance, derived from an approximate Bayesian inference methods, likeBayesian dropout, or a softmax response (SR). The resulting selection function calculated theacceptable threshold for the confidence-rate, given the risk parameters specified by the user.The selection function can then be used to decide whether a prediction should be trusted or not.Overall, it is an interesting method that is simple, requires no training, yet provides mathematicalguarantees about the properties of the confidence score.},
	Author = {Geifman, Yonatan and El-Yaniv, Ran},
	Booktitle = {Advances in neural information processing systems},
	Date-Added = {2020-05-22 08:28:41 -0400},
	Date-Modified = {2020-05-22 08:48:36 -0400},
	Keywords = {classification},
	Pages = {4878--4887},
	Read = {1},
	Title = {Selective classification for deep neural networks},
	Year = {2017},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/7073-selective-classification-for-deep-neural-networks.pdf}}

@article{angelopoulosidentifying,
	Author = {Angelopoulos, Anastasios Nikolas and Pathak, Reese and Varma, Rohit and Jordan, Michael I},
	Date-Added = {2020-05-21 14:54:52 -0400},
	Date-Modified = {2020-05-21 14:55:21 -0400},
	Journal = {Harvard Data Science Review},
	Keywords = {statistics, bias},
	Title = {On Identifying and Mitigating Bias in the Estimation of the COVID-19 Case Fatality Rate},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2003.08592.pdf}}

@inproceedings{ren2019time,
	Abstract = {Large companies need to monitor various metrics (for example,
Page Views and Revenue) of their applications and services in real
time. At Microsoft, we develop a time-series anomaly detection service which helps customers to monitor the time-series continuously
and alert for potential incidents on time. In this paper, we introduce the pipeline and algorithm of our anomaly detection service,
which is designed to be accurate, efficient and general. The pipeline
consists of three major modules, including data ingestion, experimentation platform and online compute. To tackle the problem
of time-series anomaly detection, we propose a novel algorithm
based on Spectral Residual (SR) and Convolutional Neural Network
(CNN). Our work is the first attempt to borrow the SR model from
visual saliency detection domain to time-series anomaly detection.
Moreover, we innovatively combine SR and CNN together to improve the performance of SR model. Our approach achieves superior
experimental results compared with state-of-the-art baselines on
both public datasets and Microsoft production data.},
	Annote = {Introduce the pipeline and algorithm of our anomaly detection service

Pipelines comprise of 3 major modules:
*  data ingestion, 
* experimentation platform and 
* online compute

Challenges in designing an industrial service for time-series anomaly detection:
* Lack of labels => supervised setting inappropriate
* Generalization: may different patterns, and no method works for all time series patterns
* Efficiency/Scalability: needs model that can process lots of time series very fast (large accurate models are too slow)

Their method is based on
* saliency map: this is generated by some sort of average of the log(amplitude) in Fourier space (over a sliding window), then reverting that average back to time domain (with the phase untouched)
* followed by a CNN trained on synthetic saliency maps to automatically idenfity where an anomaly occured

Overall an interesting paper. They also mention a lot of the standard methods.
Their appliations seem to be mostly periodic time series. It would need to be tested on non-stationary time series. But in the case of trend stationarity, it seems the Fourier transform would be able to handle that.},
	Author = {Ren, Hansheng and Xu, Bixiong and Wang, Yujing and Yi, Chao and Huang, Congrui and Kou, Xiaoyu and Xing, Tony and Yang, Mao and Tong, Jie and Zhang, Qi},
	Booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
	Date-Added = {2020-05-21 11:39:12 -0400},
	Date-Modified = {2020-05-21 14:42:36 -0400},
	Keywords = {anomaly detection, time series},
	Pages = {3009--3017},
	Read = {1},
	Title = {Time-Series Anomaly Detection Service at Microsoft},
	Year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1906.03821.pdf}}

@mastersthesis{melancon2019uncertainty,
	Author = {Gauthier Melan{\c c}on, Gabrielle},
	Date-Added = {2020-05-15 10:49:21 -0400},
	Date-Modified = {2020-05-21 14:57:06 -0400},
	Keywords = {uncertainty},
	Read = {1},
	School = {Ecole Polytechnique de Montr\'eal},
	Title = {Quantifying Uncertainty in Systems - Two Practical Use Cases Using Machine Learning to Predict and Explain Systems Failure},
	Year = {2019}}

@article{cawley2010over,
	Author = {Cawley, Gavin C and Talbot, Nicola LC},
	Date-Added = {2020-05-06 17:29:12 -0400},
	Date-Modified = {2020-05-06 17:29:12 -0400},
	Journal = {Journal of Machine Learning Research},
	Number = {Jul},
	Pages = {2079--2107},
	Title = {On over-fitting in model selection and subsequent selection bias in performance evaluation},
	Volume = {11},
	Year = {2010}}

@inproceedings{snoek2019can,
	Author = {Snoek, Jasper and Ovadia, Yaniv and Fertig, Emily and Lakshminarayanan, Balaji and Nowozin, Sebastian and Sculley, D and Dillon, Joshua and Ren, Jie and Nado, Zachary},
	Booktitle = {Advances in Neural Information Processing Systems},
	Date-Added = {2020-05-03 00:53:17 -0400},
	Date-Modified = {2020-05-26 20:53:19 -0400},
	Keywords = {concept drift, uncertainty, bayesian, ensemble},
	Pages = {13969--13980},
	Read = {1},
	Title = {Can you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift},
	Year = {2019},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/9547-can-you-trust-your-models-uncertainty-evaluating-predictive-uncertainty-under-dataset-shift.pdf}}

@book{spirtes2000causation,
	Annote = {That's afaik the best reference for the (Fast Causal Inference) FCI algorithm },
	Author = {Spirtes, Peter and Glymour, Clark N and Scheines, Richard and Heckerman, David},
	Date-Added = {2020-05-01 14:44:46 -0400},
	Date-Modified = {2020-05-21 14:43:47 -0400},
	Keywords = {causality},
	Publisher = {MIT press},
	Read = {1},
	Title = {Causation, prediction, and search},
	Year = {2000}}

@inproceedings{ballard1987modular,
	Author = {Ballard, Dana H},
	Booktitle = {AAAI},
	Date-Added = {2020-04-30 15:39:59 -0400},
	Date-Modified = {2020-05-21 14:57:35 -0400},
	Keywords = {autoencoder, deep learning},
	Pages = {279--284},
	Read = {1},
	Title = {Modular Learning in Neural Networks.},
	Year = {1987}}

@book{goodfellow2016deep,
	Abstract = {Table of Contents
Acknowledgements
Notation
1 Introduction
Part I: Applied Math and Machine Learning Basics
2 Linear Algebra
3 Probability and Information Theory
4 Numerical Computation
5 Machine Learning Basics
Part II: Modern Practical Deep Networks
6 Deep Feedforward Networks
7 Regularization for Deep Learning
8 Optimization for Training Deep Models
9 Convolutional Networks
10 Sequence Modeling: Recurrent and Recursive Nets
11 Practical Methodology
12 Applications
Part III: Deep Learning Research
13 Linear Factor Models
14 Autoencoders
15 Representation Learning
16 Structured Probabilistic Models for Deep Learning
17 Monte Carlo Methods
18 Confronting the Partition Function
19 Approximate Inference
20 Deep Generative Models
Bibliography
Index},
	Author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	Date-Added = {2020-04-30 15:36:47 -0400},
	Date-Modified = {2020-05-21 14:44:17 -0400},
	Keywords = {deep learning},
	Publisher = {MIT press},
	Read = {1},
	Title = {Deep learning},
	Year = {2016},
	Bdsk-Url-1 = {http://www.deeplearningbook.org/}}

@article{xu2010principal,
	Abstract = {We consider the dimensionality-reduction problem (finding a subspace approximation of observed data) for
contaminated data in the high dimensional regime, where the number of observations is of the same magnitude as
the number of variables of each observation, and the data set contains some (arbitrarily) corrupted observations. We
propose a High-dimensional Robust Principal Component Analysis (HR-PCA) algorithm that is tractable, robust
to contaminated points, and easily kernelizable. The resulting subspace has a bounded deviation from the desired
one, achieves maximal robustness -- a breakdown point of 50% while all existing algorithms have a breakdown
point of zero, and unlike ordinary PCA algorithms, achieves optimality in the limit case where the proportion of
corrupted points goes to zero.},
	Author = {Xu, Huan and Caramanis, Constantine and Mannor, Shie},
	Date-Added = {2020-04-30 14:27:10 -0400},
	Date-Modified = {2020-05-21 14:57:57 -0400},
	Journal = {arXiv preprint arXiv:1002.4658},
	Keywords = {pca, high dimension},
	Read = {1},
	Title = {Principal component analysis with contaminated data: The high dimensional case},
	Year = {2010},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1002.4658.pdf}}

@article{goldberg2014word2vec,
	Abstract = {The word2vec software of Tomas Mikolov and colleagues1 has gained a lot
of traction lately, and provides state-of-the-art word embeddings. The learning
models behind the software are described in two research papers [1, 2]. We
found the description of the models in these papers to be somewhat cryptic
and hard to follow. While the motivations and presentation may be obvious to
the neural-networks language-modeling crowd, we had to struggle quite a bit to
figure out the rationale behind the equations.
This note is an attempt to explain equation (4) (negative sampling) in ``Distributed Representations of Words and Phrases and their Compositionality'' by
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean [2].},
	Author = {Goldberg, Yoav and Levy, Omer},
	Date-Added = {2020-04-30 12:27:28 -0400},
	Date-Modified = {2020-04-30 12:27:55 -0400},
	Journal = {arXiv preprint arXiv:1402.3722},
	Keywords = {nlp, word2vec, negative-sampling},
	Title = {word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method},
	Year = {2014},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1402.3722v1.pdf}}

@article{runge2019detecting,
	Abstract = {Identifying causal relationships and quantifying their strength from observational time series data are key
problems in disciplines dealing with complex dynamical systems such as the Earth system or the human body.
Data-driven causal inference in such systems is challenging since datasets are often high dimensional and nonlinear with
limited sample sizes. Here, we introduce a novel method that flexibly combines linear or nonlinear conditional independence tests with a causal discovery algorithm to estimate causal networks from large-scale time series datasets.
We validate the method on time series of well-understood physical mechanisms in the climate system and the human
heart and using large-scale synthetic datasets mimicking the typical properties of real-world data. The experiments
demonstrate that our method outperforms state-of-the-art techniques in detection power, which opens up entirely
new possibilities to discover and quantify causal networks from time series across a range of research fields.},
	Annote = {Paper where is introduced the PCMCI algorithm, which is an example of causal network learning algorithm specialized for time series data.
The authors also provide an implementation of their algorithm on github: https://github.com/jakobrunge/tigramite
Additional material: https://advances.sciencemag.org/content/advances/suppl/2019/11/21/5.11.eaau4996.DC1/aau4996_SM.pdf

The claim to fame of PCMCI is its capacity to handle high-dimensional datasets and highly interdependent time series, along with linear and nonlinear relationship in the data. In the paper, they also show some robustness to nonstationarity. The main shortcomings remain the reliance on the Causality Sufficiency hypothesis. The PCMCI algorithm builds on top of the PC algorithm. The latter is a Markov discovery algorithm that relies on the causal Markov property to identify parents of a variable. However, as shown in the paper, the PC algorithm cannot be used as is with time series. So the authors add a momentary condition independence (MCI) test after the PC step. The PC step (PC1) iteratively removes links that are independent conditioned on the most important drivers in the previous step. After that PC1 step, each variable is left with the causal parents and potentially some false positives. The MCI step tests each potential causal link and assess causal strenght. The MCI step conditions on the parents and the parents of the potential parent tested (to account for autocorrelation). The significance level to reject/accept the independence test in the MCI step is the one parameter of the method that can be adjusted; it can selected via typical techniques (information theory, or cross-validation). Any kinf of conditional independence test can be used in this algorithm, whether linear (e.g., linear partical correlation) or nonlinear (e.g., GPDC, or CMI).},
	Author = {Runge, Jakob and Nowack, Peer and Kretschmer, Marlene and Flaxman, Seth and Sejdinovic, Dino},
	Date-Added = {2020-04-28 13:14:57 -0400},
	Date-Modified = {2020-05-21 14:44:37 -0400},
	Journal = {Science Advances},
	Keywords = {causality, time series},
	Number = {11},
	Pages = {eaau4996},
	Publisher = {American Association for the Advancement of Science},
	Read = {1},
	Title = {Detecting and quantifying causal associations in large nonlinear time series datasets},
	Volume = {5},
	Year = {2019},
	Bdsk-Url-1 = {https://advances.sciencemag.org/content/advances/5/11/eaau4996.full.pdf}}

@article{runge2019inferring,
	Abstract = {The heart of the scientific enterprise is a rational effort to understand the causes behind
the phenomena we observe. In large-scale complex dynamical systems such as the Earth
system, real experiments are rarely feasible. However, a rapidly increasing amount of
observational and simulated data opens up the use of novel data-driven causal methods
beyond the commonly adopted correlation techniques. Here, we give an overview of causal
inference frameworks and identify promising generic application cases common in Earth
system sciences and beyond. We discuss challenges and initiate the benchmark platform
causeme.net to close the gap between method users and developers.},
	Annote = {Because intervention are hard (impossible?) to put in place in Earth science, there is a strong need to rely on recent data-driven methods (observational causal discovery). They give a few examples of situations where causality was used in Earth science, in particular examples where Granger causality and/or correlation-based methods failed to identify the correct causal links. Next, they introduced the main methods for causality inference with time seriesa. Many (?) causal inference methods for time series are grounded on a few common assumptions: time-order (past events influence future events, not the other way around), causal sufficiency (no unobserved common factor exists), for graphical models the Causal Markov condition (conditional independence on all variables who don't have a direct impact),{\ldots} They first introduce Granger causality, which is probably the foundational method of causal inference in time series. The high-level idea to assess whether variable X causes variable Y is to test whether knowledge of past values of X help improve or not the prediction of future values of Y (reduction in residual variance). Different variations of that idea have been developped over the years, mainly depending on the type of model that is used to predict Y: AR, nonlinear, or transfer entropy (a non-parametric statistic measuring the amount of directed time-asymmetric transfer of information between 2 random processes; the amount of information is measured using Shannon's entropy). One potential limitation of Granger causality is that it can only detect ``lagged causality'', i.e., causal relation coming from past data; if the idea is to apply a causality screening prior to forecast, then this is probably sufficient as we'll only rely on past information. On the other hand, Granger causality typically fails to identify conditionally independent links. Also, multivariate extensions of GC fail if too many variables are considered. Next, they discuss nonlinear state-space methods, in particular convergent cross-mapping (CCM) methods. These methods look for interactions in the underlying dynamic process that generated the data. They don't say much about that class of methods, except that CCM is not well suited for multivariate, purely stochastic processes, as it doesn't explicitly condition on other variables. Causal network learning algorithms use probabilistic graphical models (think Bayesian networks) to infer causality structure. They work well in large-scale applications. There are 2 main families of methods depending whether one starts from an empty graph and add links, or one starts from a fully connected graph and remove links. The decision to add/remove links is based on the results of some tests (e.g., conditional independence statistical test, or some score function,{\ldots}). They highlight 2 methods: PCMCI, designed to handle auto-correlated and nonlinear time series, and FCI which does not rely on the Causal Sufficiency assumption. Lastly, the structural causal model (SCM) framework, promoted by Judea Pearl. Causal graphs cannot always identify the direction of contemporaneous causality links (i.e., within the Markov equivalence class). SCM allows to make assumptions about the possible causal relationships we accept.},
	Author = {Runge, Jakob and Bathiany, Sebastian and Bollt, Erik and Camps-Valls, Gustau and Coumou, Dim and Deyle, Ethan and Glymour, Clark and Kretschmer, Marlene and Mahecha, Miguel D and Mu{\~n}oz-Mar{\'\i}, Jordi and others},
	Date-Added = {2020-04-28 08:16:26 -0400},
	Date-Modified = {2020-04-28 13:16:12 -0400},
	Journal = {Nature communications},
	Keywords = {causality, time series},
	Number = {1},
	Pages = {1--13},
	Publisher = {Nature Publishing Group},
	Read = {1},
	Title = {Inferring causation from time series in Earth system sciences},
	Volume = {10},
	Year = {2019},
	Bdsk-Url-1 = {https://www.nature.com/articles/s41467-019-10105-3.pdf}}

@inproceedings{varoquaux2019comparing,
	Abstract = {Are two sets of observations drawn from the same distribution? This problem is
a two-sample test. Kernel methods lead to many appealing properties. Indeed
state-of-the-art approaches use the L2 distance between kernel-based distribution
representatives to derive their test statistics. Here, we show that Lp distances
(with p  1) between these distribution representatives give metrics on the space
of distributions that are well-behaved to detect differences between distributions
as they metrize the weak convergence. Moreover, for analytic kernels, we show
that the L1 geometry gives improved testing power for scalable computational
procedures. Specifically, we derive a finite dimensional approximation of the
metric given as the `1 norm of a vector which captures differences of expectations
of analytic functions evaluated at spatial locations or frequencies (i.e, features).
The features can be chosen to maximize the differences of the distributions and
give interpretable indications of how they differs. Using an `1 norm gives better
detection because differences between representatives are dense as we use analytic
kernels (non-zero almost everywhere). The tests are consistent, while much faster
than state-of-the-art quadratic-time kernel-based tests. Experiments on artificial
and real-world problems demonstrate improved power/time tradeoff than the state
of the art, based on `2 norms, and in some cases, better outright power than even
the most expensive quadratic-time tests},
	Author = {Varoquaux, Gael and others},
	Booktitle = {Advances in Neural Information Processing Systems},
	Date-Added = {2020-04-27 08:00:12 -0400},
	Date-Modified = {2020-05-21 14:57:26 -0400},
	Keywords = {statistical test, out of distribution},
	Pages = {12306--12316},
	Read = {1},
	Title = {Comparing distributions: $$\backslash$ell\_1 $ geometry improves kernel two-sample testing},
	Year = {2019},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/9398-comparing-distributions-ell_1-geometry-improves-kernel-two-sample-testing.pdf}}

@article{josse2019consistency,
	Abstract = {In many application settings, the data have missing features which
make data analysis challenging. An abundant literature addresses
missing data in an inferential framework: estimating parameters and
their variance from incomplete tables. Here, we consider supervisedlearning settings: predicting a target when missing values appear in
both training and testing data.
We show the consistency of two approaches in prediction. A striking
result is that the widely-used method of imputing with the mean prior
to learning is consistent when missing values are not informative. This
contrasts with inferential settings where mean imputation is pointed
at for distorting the distribution of the data. That such a simple
approach can be consistent is important in practice. We also show that
a predictor suited for complete observations can predict optimally on
incomplete data, through multiple imputation.
We analyze further decision trees. These can naturally tackle empirical risk minimization with missing values, due to their ability to
handle the half-discrete nature of incomplete variables. After comparing theoretically and empirically different missing values strategies
in trees, we recommend using the ``missing incorporated in attribute''
method as it can handle both non-informative and informative missing
values.},
	Author = {Josse, Julie and Prost, Nicolas and Scornet, Erwan and Varoquaux, Ga{\"e}l},
	Date-Added = {2020-04-27 07:55:03 -0400},
	Date-Modified = {2020-04-27 07:55:31 -0400},
	Journal = {arXiv preprint arXiv:1902.06931},
	Keywords = {missing data, tree, supervised learning},
	Title = {On the consistency of supervised learning with missing values},
	Year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1902.06931.pdf}}

@article{gonccalves2014comparative,
	Author = {Gon{\c{c}}alves Jr, Paulo M and de Carvalho Santos, Silas GT and Barros, Roberto SM and Vieira, Davi CL},
	Date-Added = {2020-04-24 09:55:22 -0400},
	Date-Modified = {2020-04-24 09:56:27 -0400},
	Journal = {Expert Systems with Applications},
	Keywords = {concept drift, experiments design},
	Number = {18},
	Pages = {8144--8156},
	Publisher = {Elsevier},
	Title = {A comparative study on concept drift detectors},
	Volume = {41},
	Year = {2014},
	Bdsk-Url-1 = {https://www.researchgate.net/profile/Roberto_Barros/publication/264081451_A_Comparative_Study_on_Concept_Drift_Detectors/links/5acf5340a6fdcc87840fd665/A-Comparative-Study-on-Concept-Drift-Detectors.pdf}}

@inproceedings{zhao2019visual,
	Abstract = {Technical and fundamental analysis are traditional tools
used to analyze stocks; however, the finance literature has
shown that the price movement of each individual stock is
highly correlated with that of other stocks, especially those
within the same sector. In this paper we propose a generalpurpose market representation that incorporates fundamental
and technical indicators and relationships between individual
stocks. We treat the daily stock market as a `market image'
where rows (grouped by market sector) represent individual
stocks and columns represent indicators. We apply a convolutional neural network over this market image to build market features in a hierarchical way. We use a recurrent neural
network, with an attention mechanism over the market feature maps, to model temporal dynamics in the market. Our
model outperforms strong baselines in both short-term and
long-term stock return prediction tasks. We also show another
use for our market image: to construct concise and dense market embeddings suitable for downstream prediction tasks.},
	Author = {Zhao, Ran and Deng, Yuntian and Dredze, Mark and Verma, Arun and Rosenberg, David and Stent, Amanda},
	Booktitle = {The Thirty-Second International Flairs Conference},
	Date-Added = {2020-04-23 13:26:14 -0400},
	Date-Modified = {2020-05-21 14:55:55 -0400},
	Keywords = {time series, forecasting, cnn, rnn, attention},
	Read = {1},
	Title = {Visual Attention Model for Cross-sectional Stock Return Prediction and End-to-End Multimodal Market Representation Learning},
	Year = {2019},
	Bdsk-Url-1 = {http://www.cs.jhu.edu/~mdredze/publications/2019_zhao_flairs.pdf}}

@article{bai2018empirical,
	Abstract = {For most deep learning practitioners, sequence
modeling is synonymous with recurrent networks.
Yet recent results indicate that convolutional architectures can outperform recurrent networks on
tasks such as audio synthesis and machine translation. Given a new sequence modeling task or
dataset, which architecture should one use? We
conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence
modeling. The models are evaluated across a
broad range of standard tasks that are commonly
used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks
such as LSTMs across a diverse range of tasks
and datasets, while demonstrating longer effective
memory. We conclude that the common association between sequence modeling and recurrent
networks should be reconsidered, and convolutional networks should be regarded as a natural
starting point for sequence modeling tasks. To
assist related work, we have made code available
at http://github.com/locuslab/TCN},
	Annote = {They provide code for a Temporal Convolutional Network unit at
https://github.com/locuslab/TCN

They compare TCN with LSTM and GRU and show that TCN does a better job at dealing with sequences.
Highly cited paper.},
	Author = {Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
	Date-Added = {2020-04-23 13:00:56 -0400},
	Date-Modified = {2020-05-21 14:43:21 -0400},
	Journal = {arXiv preprint arXiv:1803.01271},
	Keywords = {cnn, rnn, time series, code},
	Read = {1},
	Title = {An empirical evaluation of generic convolutional and recurrent networks for sequence modeling},
	Year = {2018},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1803.01271.pdf}}

@article{borovykh2017conditional,
	Author = {Borovykh, Anastasia and Bohte, Sander and Oosterlee, Cornelis W},
	Date-Added = {2020-04-23 12:46:31 -0400},
	Date-Modified = {2020-05-21 14:59:02 -0400},
	Journal = {arXiv preprint arXiv:1703.04691},
	Keywords = {time series, forecasting, cnn},
	Read = {1},
	Title = {Conditional time series forecasting with convolutional neural networks},
	Year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1703.04691.pdf?source=post_page---------------------------}}

@article{pearl2009causal,
	Abstract = {This review presents empirical researchers with recent advances
in causal inference, and stresses the paradigmatic shifts that must be undertaken in moving from traditional statistical analysis to causal analysis of
multivariate data. Special emphasis is placed on the assumptions that underly all causal inferences, the languages used in formulating those assumptions, the conditional nature of all causal and counterfactual claims, and
the methods that have been developed for the assessment of such claims.
These advances are illustrated using a general theory of causation based
on the Structural Causal Model (SCM) described in Pearl (2000a), which
subsumes and unifies other approaches to causation, and provides a coherent mathematical foundation for the analysis of causes and counterfactuals.
In particular, the paper surveys the development of mathematical tools for
inferring (from a combination of data and assumptions) answers to three
types of causal queries: (1) queries about the effects of potential interventions, (also called ``causal effects'' or ``policy evaluation'') (2) queries about
probabilities of counterfactuals, (including assessment of ``regret,'' ``attribution'' or ``causes of effects'') and (3) queries about direct and indirect
effects (also known as ``mediation''). Finally, the paper defines the formal
and conceptual relationships between the structural and potential-outcome
frameworks and presents tools for a symbiotic analysis that uses the strong
features of both.},
	Author = {Pearl, Judea and others},
	Date-Added = {2020-04-23 11:08:59 -0400},
	Date-Modified = {2020-05-21 14:44:04 -0400},
	Journal = {Statistics surveys},
	Keywords = {causality, structural causal model},
	Pages = {96--146},
	Publisher = {The author, under a Creative Commons Attribution License},
	Read = {1},
	Title = {Causal inference in statistics: An overview},
	Volume = {3},
	Year = {2009},
	Bdsk-Url-1 = {https://projecteuclid.org/download/pdfview_1/euclid.ssu/1255440554}}

@article{yao2020survey,
	Abstract = {Causal inference is a critical research topic across many domains, such as statistics, computer science, education, public
policy and economics, for decades. Nowadays, estimating causal effect from observational data has become an appealing
research direction owing to the large amount of available data and low budget requirement, compared with randomized
controlled trials. Embraced with the rapidly developed machine learning area, various causal effect estimation methods for
observational data have sprung up. In this survey, we provide a comprehensive review of causal inference methods under the
potential outcome framework, one of the well known causal inference framework. The methods are divided into two categories
depending on whether they require all three assumptions of the potential outcome framework or not. For each category,
both the traditional statistical methods and the recent machine learning enhanced methods are discussed and compared.
The plausible applications of these methods are also presented, including the applications in advertising, recommendation,
medicine and so on. Moreover, the commonly used benchmark datasets as well as the open-source codes are also summarized,
which facilitate researchers and practitioners to explore, evaluate and apply the causal inference methods.},
	Annote = {In that review paper, the methods are classified into 2 categories whether they rely on 3 assumptions (re-weighting methods, matching methods, tree-based methods, representation learning, multitask learning methods, meta learning methods), or if they relax some of them. 
These 3 assumptions are: 
* SUTVA (independence of the units receiving treatment, and unicity of treatments administred), 
* ignorability (treatment assignment done independently from expected outcome; so random), and 
* positivity (treatment assignmed randomly for all background).},
	Author = {Yao, Liuyi and Chu, Zhixuan and Li, Sheng and Li, Yaliang and Gao, Jing and Zhang, Aidong},
	Date-Added = {2020-04-23 11:07:24 -0400},
	Date-Modified = {2020-04-28 13:16:33 -0400},
	Journal = {arXiv preprint arXiv:2002.02770},
	Keywords = {causality, potential outcome framework},
	Read = {1},
	Title = {A Survey on Causal Inference},
	Year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2002.02770.pdf}}

@inproceedings{sen2019think,
	Abstract = {Forecasting high-dimensional time series plays a crucial role in many applications
such as demand forecasting and financial predictions. Modern datasets can have
millions of correlated time-series that evolve together, i.e they are extremely high
dimensional (one dimension for each individual time-series). There is a need
for exploiting global patterns and coupling them with local calibration for better
prediction. However, most recent deep learning approaches in the literature are
one-dimensional, i.e, even though they are trained on the whole dataset, during
prediction, the future forecast for a single dimension mainly depends on past values
from the same dimension. In this paper, we seek to correct this deficiency and
propose DeepGLO, a deep forecasting model which thinks globally and acts
locally. In particular, DeepGLO is a hybrid model that combines a global matrix
factorization model regularized by a temporal convolution network, along with
another temporal network that can capture local properties of each time-series and
associated covariates. Our model can be trained effectively on high-dimensional
but diverse time series, where different time series can have vastly different scales,
without a priori normalization or rescaling. Empirical results demonstrate that
DeepGLO can outperform state-of-the-art approaches; for example, we see more
than 25% improvement in WAPE over other methods on a public dataset that
contains more than 100K-dimensional time series.},
	Author = {Sen, Rajat and Yu, Hsiang-Fu and Dhillon, Inderjit S},
	Booktitle = {Advances in Neural Information Processing Systems},
	Date-Added = {2020-04-23 10:47:34 -0400},
	Date-Modified = {2020-05-21 14:59:23 -0400},
	Keywords = {time series, matrix factorization, cnn, high dimension, rnn},
	Pages = {4838--4847},
	Read = {1},
	Title = {Think globally, act locally: A deep neural network approach to high-dimensional time series forecasting},
	Year = {2019},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/8730-think-globally-act-locally-a-deep-neural-network-approach-to-high-dimensional-time-series-forecasting.pdf}}

@inproceedings{yu2016temporal,
	Abstract = {Time series prediction problems are becoming increasingly high-dimensional in
modern applications, such as climatology and demand forecasting. For example,
in the latter problem, the number of items for which demand needs to be forecast
might be as large as 50,000. In addition, the data is generally noisy and full of
missing values. Thus, modern applications require methods that are highly scalable,
and can deal with noisy data in terms of corruptions or missing values. However,
classical time series methods usually fall short of handling these issues. In this
paper, we present a temporal regularized matrix factorization (TRMF) framework
which supports data-driven temporal learning and forecasting. We develop novel
regularization schemes and use scalable matrix factorization methods that are
eminently suited for high-dimensional time series data that has many missing values.
Our proposed TRMF is highly general, and subsumes many existing approaches
for time series analysis. We make interesting connections to graph regularization
methods in the context of learning the dependencies in an autoregressive framework.
Experimental results show the superiority of TRMF in terms of scalability and
prediction quality. In particular, TRMF is two orders of magnitude faster than
other methods on a problem of dimension 50,000, and generates better forecasts on
real-world datasets such as Wal-mart E-commerce datasets.},
	Annote = {Interesting matrix factorization technique that allows to define (or infer) a temporal dynamics for the latent variables (e.g., AR),
yielding a causal dimensionality reduction.
They apply their method to forecasting or missing values imputation.

LImitations:
* It only captures linear relationships
* you have to re-solve an optimization problem every time you want to append new data and therefore
need to compute their corresponding value in the latent space (see ``Updates for X'' in section 4). The optimization problem is
much smaller though.},
	Author = {Yu, Hsiang-Fu and Rao, Nikhil and Dhillon, Inderjit S},
	Booktitle = {Advances in neural information processing systems},
	Date-Added = {2020-04-23 09:31:20 -0400},
	Date-Modified = {2020-05-21 14:56:42 -0400},
	Keywords = {time series, matrix factorization, high dimension},
	Pages = {847--855},
	Read = {1},
	Title = {Temporal regularized matrix factorization for high-dimensional time series prediction},
	Year = {2016},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/6160-temporal-regularized-matrix-factorization-for-high-dimensional-time-series-prediction.pdf}}

@inproceedings{chen2018dimensionality,
	Author = {Chen, Minshuo and Yang, Lin and Wang, Mengdi and Zhao, Tuo},
	Booktitle = {Advances in Neural Information Processing Systems},
	Date-Added = {2020-04-23 08:29:38 -0400},
	Date-Modified = {2020-04-23 08:30:10 -0400},
	Keywords = {optimization, stochastic, nonconvex, time series},
	Pages = {3496--3506},
	Title = {Dimensionality reduction for stationary time series via stochastic nonconvex optimization},
	Year = {2018},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/7609-dimensionality-reduction-for-stationary-time-series-via-stochastic-nonconvex-optimization.pdf}}

@article{keogh2001dimensionality,
	Abstract = {The problem of similarity search in large time series databases has attracted much attention
recently. It is a non-trivial problem because of the inherent high dimensionality of the data. The
most promising solutions involve performing dimensionality reduction on the data, then indexing
the reduced data with a spatial access method. Three major dimensionality reduction techniques
have been proposed, Singular Value Decomposition (SVD), the Discrete Fourier transform
(DFT), and more recently the Discrete Wavelets Transform (DWT). In this work we introduce a
new dimensionality reduction technique which we call PAA (Piecewise Aggregate
Approximation). We theoretically and empirically compare it to the other techniques and
demonstrate its superiority. In addition to being competitive with or faster than the other methods
our approach has numerous advantages. It is simple to understand and implement, allows more
flexible distance measures including weighted Euclidean queries and the index can be built in
linear time.},
	Annote = {Classic paper to do rapid indexing of large time series database.
Introduce PIecewise Aggregate Approximation which sub-samples the time series and
do similarity search on these sub-sampled time series.},
	Author = {Keogh, Eamonn and Chakrabarti, Kaushik and Pazzani, Michael and Mehrotra, Sharad},
	Date-Added = {2020-04-22 17:54:05 -0400},
	Date-Modified = {2020-05-21 14:58:30 -0400},
	Journal = {Knowledge and information Systems},
	Keywords = {time series, indexing, high dimension, dimensionality reduction},
	Number = {3},
	Pages = {263--286},
	Publisher = {Springer},
	Read = {1},
	Title = {Dimensionality reduction for fast similarity search in large time series databases},
	Volume = {3},
	Year = {2001},
	Bdsk-Url-1 = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/time_series_indexing.pdf}}

@article{duan2019ngboost,
	Abstract = {We present Natural Gradient Boosting (NGBoost), an algorithm for generic probabilistic
prediction via gradient boosting. Typical regression models return a point estimate, conditional on covariates, but probabilistic regression
models output a full probability distribution over
the outcome space, conditional on the covariates. This allows for predictive uncertainty estimation --- crucial in applications like healthcare and weather forecasting. NGBoost generalizes gradient boosting to probabilistic regression
by treating the parameters of the conditional distribution as targets for a multiparameter boosting algorithm. Furthermore, we show how the
Natural Gradient is required to correct the training dynamics of our multiparameter boosting approach. NGBoost can be used with any base
learner, any family of distributions with continuous parameters, and any scoring rule. NGBoost
matches or exceeds the performance of existing
methods for probabilistic prediction while offering additional benefits in flexibility, scalability,
and usability.},
	Author = {Duan, Tony and Avati, Anand and Ding, Daisy Yi and Basu, Sanjay and Ng, Andrew Y and Schuler, Alejandro},
	Date-Added = {2020-04-22 10:24:24 -0400},
	Date-Modified = {2020-04-22 10:25:12 -0400},
	Journal = {arXiv preprint arXiv:1910.03225},
	Keywords = {time series, distributional forecast, forecasting},
	Title = {NGBoost: Natural Gradient Boosting for Probabilistic Prediction},
	Year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1910.03225.pdf}}

@inproceedings{verleysen2005curse,
	Abstract = {Modern data analysis tools have to work on high-dimensional data,
whose components are not independently distributed. High-dimensional spaces
show surprising, counter-intuitive geometrical properties that have a large
influence on the performances of data analysis tools. Among these properties,
the concentration of the norm phenomenon results in the fact that Euclidean
norms and Gaussian kernels, both commonly used in models, become
inappropriate in high-dimensional spaces. This papers presents alternative
distance measures and kernels, together with geometrical methods to decrease
the dimension of the space. The methodology is applied to a typical time series
prediction example. },
	Author = {Verleysen, Michel and Fran{\c{c}}ois, Damien},
	Booktitle = {International Work-Conference on Artificial Neural Networks},
	Date-Added = {2020-04-22 10:19:26 -0400},
	Date-Modified = {2020-04-22 10:20:13 -0400},
	Keywords = {time series, data mining, high dimension},
	Organization = {Springer},
	Pages = {758--770},
	Title = {The curse of dimensionality in data mining and time series prediction},
	Year = {2005},
	Bdsk-Url-1 = {http://neuro.bstu.by/ai/To-dom/My_research/Papers-0/For-research/D-mining/Stock-market/iwann05mv.pdf}}

@article{xue2014jump,
	Abstract = {This paper introduces a new nonparametric test to identify jump arrival times in high frequency
financial time series data. The asymptotic distribution of the test is derived. We demonstrate that the
test is robust for different specifications of price processes and the presence of the microstructure
noise. A Monte Carlo simulation is conducted which shows that the test has good size and power.
Further, we examine the multi-scale jump dynamics in US equity markets. The main findings are as
follows. First, the jump dynamics of equities are sensitive to data sampling frequency with significant
underestimation of jump intensities at lower frequencies. Second, although arrival densities of positive
jumps and negative jumps are symmetric across different time scales, the magnitude of jumps is
distributed asymmetrically at high frequencies. Third, only 20% of jumps occur in the trading session
from 9:30AM to 4:00 PM, suggesting that illiquidity during after-hours trading is a strong determinant
of jumps.},
	Author = {Xue, Yi and Gencay, Ramazan and Fagan, Stephen},
	Date-Added = {2020-04-22 10:17:32 -0400},
	Date-Modified = {2020-04-22 10:18:04 -0400},
	Journal = {Quantitative Finance},
	Keywords = {time series, change point detection, wavelet},
	Number = {8},
	Pages = {1427--1444},
	Publisher = {Taylor \& Francis},
	Title = {Jump detection with wavelets for high-frequency financial time series},
	Volume = {14},
	Year = {2014},
	Bdsk-Url-1 = {https://www.researchgate.net/profile/Yi_Xue2/publication/268152725_Jump_detection_with_wavelets_for_high-frequency_financial_time_series/links/54630bad0cf2c0c6aec1c47a/Jump-detection-with-wavelets-for-high-frequency-financial-time-series.pdf}}

@article{detommaso2019stein,
	Abstract = {Bayesian online changepoint detection (BOCPD) [1] offers a rigorous and viable
way to identify changepoints in complex systems. In this work, we introduce
a Stein variational online changepoint detection (SVOCD) method to provide a
computationally tractable generalization of BOCPD beyond the exponential family
of probability distributions. We integrate the recently developed Stein variational
Newton (SVN) method [5] and BOCPD to offer a full online Bayesian treatment for
a large number of situations with significant importance in practice. We apply the
resulting method to two challenging and novel applications: Hawkes processes and
long short-term memory (LSTM) neural networks. In both cases, we successfully
demonstrate the efficacy of our method on real data.},
	Author = {Detommaso, Gianluca and Hoitzing, Hanne and Cui, Tiangang and Alamir, Ardavan},
	Date-Added = {2020-04-22 10:16:52 -0400},
	Date-Modified = {2020-05-26 20:54:40 -0400},
	Journal = {arXiv preprint arXiv:1901.07987},
	Keywords = {time series, change point detection, bayesian},
	Title = {Stein variational online changepoint detection with applications to Hawkes processes and neural networks},
	Year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1901.07987.pdf}}

@article{aminikhanghahi2017survey,
	Author = {Aminikhanghahi, Samaneh and Cook, Diane J},
	Date-Added = {2020-04-22 10:15:29 -0400},
	Date-Modified = {2020-04-22 10:15:49 -0400},
	Journal = {Knowledge and information systems},
	Keywords = {time series, change point detection},
	Number = {2},
	Pages = {339--367},
	Publisher = {Springer},
	Title = {A survey of methods for time series change point detection},
	Volume = {51},
	Year = {2017},
	Bdsk-Url-1 = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5464762/pdf/nihms860556.pdf}}

@inproceedings{keogh2001online,
	Abstract = {In recent years, there has been an explosion of interest in mining time series databases. As with most
computer science problems, representation of the data is the key to efficient and effective solutions. One of
the most commonly used representations is piecewise linear approximation. This representation has been
used by various researchers to support clustering, classification, indexing and association rule mining of
time series data. A variety of algorithms have been proposed to obtain this representation, with several
algorithms having been independently rediscovered several times. In this paper, we undertake the first
extensive review and empirical comparison of all proposed techniques. We show that all these algorithms
have fatal flaws from a data mining perspective. We introduce a novel algorithm that we empirically show
to be superior to all others in the literature.},
	Author = {Keogh, Eamonn and Chu, Selina and Hart, David and Pazzani, Michael},
	Booktitle = {Proceedings 2001 IEEE international conference on data mining},
	Date-Added = {2020-04-22 10:13:24 -0400},
	Date-Modified = {2020-04-22 10:28:58 -0400},
	Keywords = {time series, online algorithm, data mining},
	Organization = {IEEE},
	Pages = {289--296},
	Title = {An online algorithm for segmenting time series},
	Year = {2001},
	Bdsk-Url-1 = {https://sfb876.tu-dortmund.de/PublicPublicationFiles/keogh_etal_2001a.pdf}}

@article{martinsson2020randomized,
	Author = {Martinsson, Per-Gunnar and Tropp, Joel},
	Date-Added = {2020-04-22 10:10:33 -0400},
	Date-Modified = {2020-05-21 14:56:55 -0400},
	Journal = {arXiv preprint arXiv:2002.01387},
	Keywords = {numerical linear algebra, randomized, algorithm},
	Read = {1},
	Title = {Randomized numerical linear algebra: Foundations \& algorithms},
	Year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2002.01387.pdf}}

@article{demvsar2006statistical,
	Abstract = {While methods for comparing two learning algorithms on a single data set have been scrutinized for
quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple
data sets, which is even more essential to typical machine learning studies, has been all but ignored.
This article reviews the current practice and then theoretically and empirically examines several
suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric
tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of
two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more
classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly
introduced CD (critical difference) diagrams.},
	Author = {Dem{\v{s}}ar, Janez},
	Date-Added = {2020-04-21 11:53:43 -0400},
	Date-Modified = {2020-05-06 15:40:07 -0400},
	Journal = {Journal of Machine learning research},
	Keywords = {classification, statistical test,},
	Number = {Jan},
	Pages = {1--30},
	Title = {Statistical comparisons of classifiers over multiple data sets},
	Volume = {7},
	Year = {2006},
	Bdsk-Url-1 = {http://www.jmlr.org/papers/volume7/demsar06a/demsar06a.pdf}}

@article{pearl2019seven,
	Author = {Pearl, Judea},
	Date-Added = {2020-04-21 09:34:41 -0400},
	Date-Modified = {2020-05-21 14:56:10 -0400},
	Journal = {Communications of the ACM},
	Keywords = {causality},
	Number = {3},
	Pages = {54--60},
	Publisher = {ACM New York, NY, USA},
	Read = {1},
	Title = {The seven tools of causal inference, with reflections on machine learning},
	Volume = {62},
	Year = {2019},
	Bdsk-Url-1 = {http://ftp.cs.ucla.edu/pub/stat_ser/r481.pdf}}

@webpage{palachy2019,
	Annote = {re-read quickly, then move on -> need to decide on what to read

probably want to focus on graphical models
1) DynoTEARS
2) PCMCI


Brain vomit:

Long, extensive, and interesting blog post. It tries to list all different methods to estimate causality in time series data.

All methods could be grouped in 2 categories:
* test based: runing statistical test to determinie if ts1 Granger-causes ts2
* graph based: generate a Bayesian NN 

There would need to be a summary table of the different methods with their strengths/weaknesses -> time consuming + is it the best approach? Should I just focus on PCMCI

A couple of references stand out from that blog post:
- Papana et al., 2013: file:///Users/bencrestel/Downloads/entropy-15-02635.pdf
- the work of Runge: for graphical model, check out PMCI (https://advances.sciencemag.org/content/advances/5/11/eaau4996.full.pdf / https://github.com/jakobrunge/tigramite)


Focus on graphical approaches immediately? Probably need a rationale to do so -> study other methods briefly, or check with Alex

What is the advantage of graphical methods over statistical tests?
* Granger causality: popular method due to its computational simplicity
* Graphical approach: often used to model Granger causality in multivariate setting

Connection stationarity - Causality??},
	Date-Added = {2020-04-20 23:13:21 -0400},
	Date-Modified = {2020-04-28 13:16:17 -0400},
	Keywords = {causality, time series},
	Lastchecked = {Apr. 2020},
	Month = {Nov},
	Read = {1},
	Url = {https://towardsdatascience.com/inferring-causality-in-time-series-data-b8b75fe52c46#99db},
	Year = {2019},
	Bdsk-Url-1 = {https://towardsdatascience.com/inferring-causality-in-time-series-data-b8b75fe52c46#99db}}

@unpublished{haugh2015,
	Author = {Martin Haugh},
	Date-Added = {2020-04-20 07:42:28 -0400},
	Date-Modified = {2020-05-21 14:43:36 -0400},
	Keywords = {EM, algorithm, class notes},
	Note = {IEOR E4570: Machine Learning for OR&FE, Columbia},
	Read = {1},
	Title = {The EM Algorithm},
	Year = {2015},
	Bdsk-Url-1 = {http://www.columbia.edu/~mh2078/MachineLearningORFE/EM_Algorithm.pdf}}

@article{scholkopf2019causality,
	Abstract = {Graphical causal inference as pioneered by Judea Pearl arose from research on artificial intelligence
(AI), and for a long time had little connection to the field of machine learning. This article discusses
where links have been and should be established, introducing key concepts along the way. It argues
that the hard open problems of machine learning and AI are intrinsically related to causality, and
explains how the field is beginning to understand them.},
	Author = {Sch{\"o}lkopf, Bernhard},
	Date-Added = {2020-04-06 14:36:14 -0400},
	Date-Modified = {2020-05-21 14:55:31 -0400},
	Journal = {arXiv preprint arXiv:1911.10500},
	Keywords = {causality},
	Read = {1},
	Title = {Causality for Machine Learning},
	Year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1911.10500.pdf}}

@incollection{ralanamahatana2005mining,
	Abstract = {Much of the world's supply of data is in the form of time series. In the last decade,
there has been an explosion of interest in mining time series data. A number of new algorithms have been introduced to classify, cluster, segment, index, discover rules, and detect
anomalies/novelties in time series. While these many different techniques used to solve these
problems use a multitude of different techniques, they all have one common factor; they require some high level representation of the data, rather than the original raw data. These high
level representations are necessary as a feature extraction step, or simply to make the storage,
transmission, and computation of massive dataset feasible. A multitude of representations have
been proposed in the literature, including spectral transforms, wavelets transforms, piecewise
polynomials, eigenfunctions, and symbolic mappings. This chapter gives a high-level survey
of time series Data Mining tasks, with an emphasis on time series representations.},
	Annote = {* Good high-level overview of data mining approaches for time series:
indexing, clustering, classification, prediction, summarization, anomaly detection, segmentation.
* For every approach, the treatment is very superficial though.},
	Author = {Ralanamahatana, Chotirat Ann and Lin, Jessica and Gunopulos, Dimitrios and Keogh, Eamonn and Vlachos, Michail and Das, Gautam},
	Booktitle = {Data mining and knowledge discovery handbook},
	Date-Added = {2020-03-26 15:42:44 -0400},
	Date-Modified = {2020-05-21 14:58:19 -0400},
	Keywords = {time series, algorithm, data mining},
	Pages = {1069--1103},
	Publisher = {Springer},
	Read = {1},
	Title = {Mining time series data},
	Year = {2005},
	Bdsk-Url-1 = {https://www.researchgate.net/profile/Chotirat_Ratanamahatana2/publication/227001229_Mining_Time_Series_Data/links/02bfe51188657160b5000000.pdf}}

@inproceedings{ren2019likelihood,
	Abstract = {Discriminative neural networks offer little or no performance guarantees when
deployed on data not generated by the same process as the training distribution. On
such out-of-distribution (OOD) inputs, the prediction may not only be erroneous,
but confidently so, limiting the safe deployment of classifiers in real-world applications. One such challenging application is bacteria identification based on genomic
sequences, which holds the promise of early detection of diseases, but requires a
model that can output low confidence predictions on OOD genomic sequences from
new bacteria that were not present in the training data. We introduce a genomics
dataset for OOD detection that allows other researchers to benchmark progress on
this important problem. We investigate deep generative model based approaches
for OOD detection and observe that the likelihood score is heavily affected by
population level background statistics. We propose a likelihood ratio method for
deep generative models which effectively corrects for these confounding background statistics. We benchmark the OOD detection performance of the proposed
method against existing approaches on the genomics dataset and show that our
method achieves state-of-the-art performance. We demonstrate the generality of
the proposed method by showing that it significantly improves OOD detection
when applied to deep generative models of images.},
	Annote = {Research stemmed from a OOD problem in genomic, and they try to generalize it to image classification. This project stems from the limitation of existing generative-based methods for OOD; they show examples in both domains where the likelihood OOD is at least as confident as in-distribution.
Their solution is to split input into background and semantic (independence assumption), and apply a likelihood ration only to the semantic part.

It's not clear why their assumptions would be valid outside (even inside) of genomics. For instance, backgroud play a big role in confounding image classifiers (e.g., wolf in front of grass).

However, there is a nice section at the beginning of section 4 that summarizes the ``Baseline methods for comparison'':
1. The maximum class probability, p(y|x) = maxk p(y = k|x). OOD inputs tend to have lower
scores than in-distribution data (Hendrycks & Gimpel, 2016).
2. The entropy of the predicted class distribution,  P_k p(y = k|x) log p(y = k|x). High entropy
of the predicted class distribution, and therefore a high predictive uncertainty, which suggests that
the input may be OOD.
3. The ODIN method proposed by Liang et al. (2017). ODIN uses temperature scaling (Guo et al.,
2017), adds small perturbations to the input, and applies a threshold to the resulting predicted
class to distinguish in- and out-of- distribution inputs. This method was designed for continuous
inputs and cannot be directly applied to discrete genomic sequences. We propose instead to add
perturbations to the input of the last layer that is closest to the output of the neural network.
4. The Mahalanobis distance of the input to the nearest class-conditional Gaussian distribution estimated from the in-distribution data. Lee et al. (2018) fit class-conditional Gaussian distributions
to the activations from the last layer of the neural network.
5. The classifier-based ensemble method that uses the average of the predictions from multiple
independently trained models with random initialization of network parameters and random
shuffling of training inputs (Lakshminarayanan et al., 2017).
6. The log-odds of a binary classifier trained to distinguish between in-distribution inputs from all
classes as one class and randomly perturbed in-distribution inputs as the other.
7. The maximum class probability over K in-distribution classes of a (K + 1)-class classifier where
the additional class is perturbed in-distribution.
8. The maximum class probability of a K-class classifier for in-distribution classes but the predicted
class distribution is explicitly trained to output uniform distribution on perturbed in-distribution
inputs. This is similar to using simulated OOD inputs from GAN (Lee et al., 2017) or using
auxiliary datasets of outliers (Hendrycks et al., 2018) for calibration purpose.
9. The generative model-based ensemble method that measures E[log p(x)]  Var[log p(x)] of
multiple likelihood estimations from independently trained model with random initialization and
random shuffling of the inputs. (Choi et al., 2018).},
	Author = {Ren, Jie and Liu, Peter J and Fertig, Emily and Snoek, Jasper and Poplin, Ryan and Depristo, Mark and Dillon, Joshua and Lakshminarayanan, Balaji},
	Booktitle = {Advances in Neural Information Processing Systems},
	Date-Added = {2020-03-26 14:41:15 -0400},
	Date-Modified = {2020-04-22 10:28:20 -0400},
	Keywords = {out of distribution, generative model, deep learning},
	Pages = {14680--14691},
	Read = {1},
	Title = {Likelihood ratios for out-of-distribution detection},
	Year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1906.02845.pdf}}

@article{tonekaboni2020went,
	Abstract = {Multivariate time series models are poised to be
used for decision support in high-stakes applications, such as healthcare. In these contexts, it is
important to know which features at which times
most influenced a prediction. We demonstrate
a general approach for assigning importance to
observations in multivariate time series, based on
their counterfactual influence on future predictions. Specifically, we define the importance of an
observation as the change in the predictive distribution, had the observation not been seen. We integrate over plausible counterfactuals by sampling
from the corresponding conditional distributions
of generative time series models. We compare our
importance metric to gradient-based explanations,
attention mechanisms, and other baselines in simulated and clinical ICU data, and show that our
approach generates the most precise explanations.
Our method is inexpensive, model agnostic, and
can be used with arbitrarily complex time series
models and predictors},
	Annote = {Proposes a method to assess the importance of multivariate temporal features. They do so, at each time step, for each feature, by measuring the KL divergence between the exact predictive distribution and the one you would obtain if you were to not observe that feature at that time step. To replace the feature (they still need it to derive a predictive distribution), they sample from a generatitve model they train on all features at previous time.
Problems:
1) you only the importance of a feature at the last time step
2) when sampling the missing feature, they do not account for possible correlation across features at that time step (only at previous time steps).},
	Author = {Tonekaboni, Sana and Joshi, Shalmali and Duvenaud, David and Goldenberg, Anna},
	Date-Added = {2020-03-10 15:02:42 -0400},
	Date-Modified = {2020-04-22 10:29:31 -0400},
	Journal = {arXiv preprint arXiv:2003.02821},
	Keywords = {xai, time series, generative model},
	Read = {1},
	Title = {What went wrong and when? Instance-wise Feature Importance for Time-series Models},
	Year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2003.02821.pdf}}

@article{gholamiintegrated,
	Abstract = {Finding the right Neural Network model and
training it for a new task requires considerable expertise
and extensive computational resources. Moreover, the process
often includes ad-hoc rules that do not generalize to different
application domains. These issues have limited the applicability
and usefulness of DNN models, especially for new learning
tasks. This problem is becoming more acute, as datasets and
models grow larger, which increases training time, making
random/brute force search approaches quickly untenable. In
large part, this situation is due to the first-order stochastic gradient descent (SGD) methods that are widely-used for training
DNNs. Despite SGD's well-known benefits, vanilla SGD tends
to perform poorly, and thus one introduces many (essentially
ad-hoc) knobs and hyper-parameters to make it work. It has
been found that these hyper-parameters are significantly more
sensitive to tuning in large scale training with SGD, and this
has impeded effective use of supercomputing systems. Here,
we argue that a multi-faceted approach is needed to address
these challenges by considering the full stack of neural network
architecture design, large scale training, and efficient inference
on edge platforms. This requires designing mechanisms to
better understand NN training and bridge the gap between
theoretical results for optimization, second order methods, and
high performance computing.},
	Annote = {Gives an overview of the recent work of the 3 authors in Second-order methods and parallel computing. It's more of a marketing paper, but it gathers all their references},
	Author = {Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
	Date-Added = {2020-03-09 10:50:01 -0400},
	Date-Modified = {2020-04-22 10:27:21 -0400},
	Keywords = {Hessian, Newton method, deep learning},
	Read = {1},
	Title = {An Integrated Approach to Neural Network Design, Training, and Inference},
	Year = {2020},
	Bdsk-Url-1 = {http://smartci.sci.utah.edu/images/whitepapers/2020_NSF_White_Paper_UCB.pdf}}

@article{lindsten2017probabilistic,
	Abstract = {This is a text on probabilistic modeling for the master level course `Statistical Machine Learning' given at the
Department of Information Technology, Uppsala University during the spring term 2017 and it is a complement
to the course books James et al. (2013) and Hastie et al. (2009). It consists of three chapters and one appendix.
The three chapters cover an introduction to probabilistic modeling, probabilistic (Bayesian) linear regression,
and Gaussian processes. The appendix introduces the multivariate Gaussian distribution and presents key results
needed in the chapters. Consequently, the appendix has an important role in this document and should therefore
be studied carefully.},
	Author = {Lindsten, Fredrik and Sch{\"o}n, Thomas B and Svensson, Andreas and Wahlstr{\"o}m, Niklas},
	Date-Added = {2020-02-20 11:10:47 -0500},
	Date-Modified = {2020-02-20 11:11:57 -0500},
	Journal = {Uppsala: Uppsala University},
	Keywords = {gaussian process, distributional forecast},
	Read = {1},
	Title = {Probabilistic modeling--linear regression \& Gaussian processes},
	Year = {2017},
	Bdsk-Url-1 = {http://www.it.uu.se/edu/course/homepage/sml/literature/probabilistic_modeling_compendium.pdf}}

@article{marcus2018deep,
	Abstract = {Although deep learning has historical roots going back decades, neither the term ``deep
learning'' nor the approach was popular just over five years ago, when the field was
reignited by papers such as Krizhevsky, Sutskever and Hinton's now classic 2012
(Krizhevsky, Sutskever, & Hinton, 2012)deep net model of Imagenet.
What has the field discovered in the five subsequent years? Against a background of
considerable progress in areas such as speech recognition, image recognition, and game
playing, and considerable enthusiasm in the popular press, I present ten concerns for deep
learning, and suggest that deep learning must be supplemented by other techniques if we
are to reach artificial general intelligence.},
	Author = {Marcus, Gary},
	Date-Added = {2020-02-18 17:37:08 -0500},
	Date-Modified = {2020-02-18 17:37:54 -0500},
	Journal = {arXiv preprint arXiv:1801.00631},
	Keywords = {deep learning, symbolic AI},
	Read = {1},
	Title = {Deep learning: A critical appraisal},
	Year = {2018},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1801.00631.pdf}}

@inproceedings{ma2019learning,
	Abstract = {Time series clustering is an essential unsupervised technique in cases when category
information is not available. It has been widely applied to genome data, anomaly
detection, and in general, in any domain where pattern detection is important.
Although feature-based time series clustering methods are robust to noise and
outliers, and can reduce the dimensionality of the data, they typically rely on domain
knowledge to manually construct high-quality features. Sequence to sequence
(seq2seq) models can learn representations from sequence data in an unsupervised
manner by designing appropriate learning objectives, such as reconstruction and
context prediction. When applying seq2seq to time series clustering, obtaining a
representation that effectively represents the temporal dynamics of the sequence,
multi-scale features, and good clustering properties remains a challenge. How
to best improve the ability of the encoder is still an open question. Here we
propose a novel unsupervised temporal representation learning model, named
Deep Temporal Clustering Representation (DTCR), which integrates the temporal
reconstruction and K-means objective into the seq2seq model. This approach
leads to improved cluster structures and thus obtains cluster-specific temporal
representations. Also, to enhance the ability of encoder, we propose a fake-sample
generation strategy and auxiliary classification task. Experiments conducted on
extensive time series datasets show that DTCR is state-of-the-art compared to
existing methods. The visualization analysis not only shows the effectiveness of
cluster-specific representation but also shows the learning process is robust, even if
K-means makes mistakes.},
	Annote = {Propose to cluster time series in latent space, using a seq2seq model based on multi-layer Dilated RNN, a k-means loss term, and a GAN-type classifier (real/fake). 
The k-means loss applies to the centroids but also the encoder; for stability reason, the centroids are updated less frequently than the network weights (every 10 epochs in the paper). 
They added a classifier to improve the quality of the reconstruction; they generate fake time series by randomly permutating the entries of the real ones.

Interesting paper overall. Seems a bit convoluted, but show SOTA results on a few of the UCR classification datasets.},
	Author = {Ma, Qianli and Zheng, Jiawei and Li, Sen and Cottrell, Gary W},
	Booktitle = {Advances in Neural Information Processing Systems},
	Date-Added = {2020-02-11 16:23:10 -0500},
	Date-Modified = {2020-04-22 11:21:36 -0400},
	Keywords = {DTCR, clustering, k-means, seq2seq},
	Pages = {3776--3786},
	Read = {1},
	Title = {Learning Representations for Time Series Clustering},
	Year = {2019},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/8634-learning-representations-for-time-series-clustering.pdf}}

@article{aghabozorgi2015time,
	Author = {Aghabozorgi, Saeed and Shirkhorshidi, Ali Seyed and Wah, Teh Ying},
	Date-Added = {2020-02-11 14:31:16 -0500},
	Date-Modified = {2020-05-21 14:59:28 -0400},
	Journal = {Information Systems},
	Keywords = {time series, clustering, algorithm},
	Pages = {16--38},
	Publisher = {Elsevier},
	Read = {1},
	Title = {Time-series clustering--a decade review},
	Volume = {53},
	Year = {2015},
	Bdsk-Url-1 = {https://wiki.smu.edu.sg/18191isss608g1/img_auth.php/f/fd/Time_Series_Clustering_A_Decade_Review.pdf}}

@inproceedings{yi2019not,
	Abstract = {Handling missing data is one of the most fundamental problems in machine learning.
Among many approaches, the simplest and most intuitive way is zero imputation,
which treats the value of a missing entry simply as zero. However, many studies
have experimentally confirmed that zero imputation results in suboptimal performances in training neural networks. Yet, none of the existing work has explained
what brings such performance degradations. In this paper, we introduce the variable sparsity problem (VSP), which describes a phenomenon where the output
of a predictive model largely varies with respect to the rate of missingness in the
given input, and show that it adversarially affects the model performance. We first
theoretically analyze this phenomenon and propose a simple yet effective technique
to handle missingness, which we refer to as Sparsity Normalization (SN), that
directly targets and resolves the VSP. We further experimentally validate SN on
diverse benchmark datasets, to show that debiasing the effect of input-level sparsity
improves the performance and stabilizes the training of neural networks.},
	Author = {Yi, Joonyoung and Lee, Juhyuk and Hwang, Sung Ju and Yang, Eunho},
	Date-Added = {2020-02-06 14:04:23 -0500},
	Date-Modified = {2020-04-22 07:42:25 -0400},
	Keywords = {missing data, zero imputation, deep learning},
	Read = {1},
	Title = {Why Not to Use Zero Imputation? Correcting Sparsity Bias in Training Neural Networks},
	Year = {2019},
	Bdsk-Url-1 = {https://openreview.net/pdf?id=BylsKkHYvH}}

@article{bauckhage2015numpy,
	Abstract = {In this note, we study least squares optimization for parameter estimation. By means of the basic example of a linear regression task, we explore different formulations of the ordinary least squares problem, show how to solve it using NumPy or SciPy, and provide suggestions for practical applications.},
	Author = {Bauckhage, Christian},
	Date-Added = {2020-02-05 15:21:33 -0500},
	Date-Modified = {2020-04-22 10:28:01 -0400},
	Journal = {researchgate. net, Mar},
	Keywords = {linear regression, numpy/scipy},
	Title = {NumPy/SciPy Recipes for Data Science: Ordinary Least Squares Optimization},
	Year = {2015},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAmLi4vLi4vLi4vRG93bmxvYWRzL25wLXNwLXJlY2lwZXMtNS5wZGZPEQFkAAAAAAFkAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8TbnAtc3AtcmVjaXBlcy01LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAIAAAogY3UAAAAAAAAAAAAAAAAACURvd25sb2FkcwAAAgAwLzpVc2VyczpiZW5jcmVzdGVsOkRvd25sb2FkczpucC1zcC1yZWNpcGVzLTUucGRmAA4AKAATAG4AcAAtAHMAcAAtAHIAZQBjAGkAcABlAHMALQA1AC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgAuVXNlcnMvYmVuY3Jlc3RlbC9Eb3dubG9hZHMvbnAtc3AtcmVjaXBlcy01LnBkZgATAAEvAAAVAAIAEf//AAAACAANABoAJABNAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAbU=}}

@article{DBLP:journals/corr/abs-1803-03635,
	Annote = {Conjecture that over-parametrized network are easier to train b/c they contain smaller networks with similar high accuracy},
	Archiveprefix = {arXiv},
	Author = {Jonathan Frankle and Michael Carbin},
	Bibsource = {dblp computer science bibliography, https://dblp.org},
	Biburl = {https://dblp.org/rec/bib/journals/corr/abs-1803-03635},
	Date-Added = {2020-02-05 14:40:58 -0500},
	Date-Modified = {2020-02-05 14:42:58 -0500},
	Eprint = {1803.03635},
	Journal = {CoRR},
	Keywords = {lottery ticket, over-parametrized, pruning},
	Read = {1},
	Timestamp = {Mon, 13 Aug 2018 16:48:29 +0200},
	Title = {The Lottery Ticket Hypothesis: Training Pruned Neural Networks},
	Url = {http://arxiv.org/abs/1803.03635},
	Volume = {abs/1803.03635},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1803.03635}}

@article{DBLP:journals/corr/abs-1906-04705,
	Abstract = {Least-mean squares (LMS) solvers such as Linear / Ridge / Lasso-Regression,
SVD and Elastic-Net not only solve fundamental machine learning problems, but
are also the building blocks in a variety of other methods, such as decision trees
and matrix factorizations.
We suggest an algorithm that gets a finite set of n d-dimensional real vectors and
returns a weighted subset of d + 1 vectors whose sum is exactly the same. The
proof in Caratheodory's Theorem (1907) computes such a subset in O(n^2 d^2) time 
and thus not used in practice. Our algorithm computes this subset in O(nd) time,
using O(log n) calls to Caratheodory's construction on small but "smart" subsets.
This is based on a novel paradigm of fusion between different data summarization
techniques, known as sketches and coresets.
As an example application, we show how it can be used to boost the performance
of existing LMS solvers, such as those in scikit-learn library, up to x100. 
Generalization for streaming and distributed (big) data is trivial. Extensive experimental
results and complete open source code are also provided.},
	Annote = {Claim major speed-ups compared to other sketching methods.
Honorable Mention Outstanding Paper Award @ NeurIPS 2019},
	Archiveprefix = {arXiv},
	Author = {Alaa Maalouf and Ibrahim Jubran and Dan Feldman},
	Bibsource = {dblp computer science bibliography, https://dblp.org},
	Biburl = {https://dblp.org/rec/bib/journals/corr/abs-1906-04705},
	Date-Added = {2020-02-05 14:29:15 -0500},
	Date-Modified = {2020-04-22 10:20:35 -0400},
	Eprint = {1906.04705},
	Journal = {CoRR},
	Keywords = {sketching, least mean squares, big data},
	Read = {1},
	Timestamp = {Fri, 14 Jun 2019 09:38:24 +0200},
	Title = {Fast and Accurate Least-Mean-Squares Solvers},
	Url = {http://arxiv.org/abs/1906.04705},
	Volume = {abs/1906.04705},
	Year = {2019},
	Bdsk-Url-1 = {http://arxiv.org/abs/1906.04705}}

@article{araya2017automated,
	Annote = {Not clear how good of an idea that is. In the paper, they claim DNN are a good idea to approximate physics processes, but I'm not so sure how true that is.},
	Author = {Araya-Polo, Mauricio and Dahlke, Taylor and Frogner, Charlie and Zhang, Chiyuan and Poggio, Tomaso and Hohl, Detlef},
	Date-Added = {2020-01-14 14:49:38 -0500},
	Date-Modified = {2020-01-14 22:41:34 -0500},
	Journal = {The Leading Edge},
	Keywords = {seismic processing, wasserstein, image segmentation, cnn},
	Number = {3},
	Pages = {208--214},
	Publisher = {Society of Exploration Geophysicists},
	Read = {1},
	Title = {Automated fault detection without seismic processing},
	Volume = {36},
	Year = {2017}}

@inproceedings{Graves:2006aa,
	Abstract = {Many real-world sequence learning tasks require the prediction of sequences of labels
from noisy, unsegmented input data. In
speech recognition, for example, an acoustic
signal is transcribed into words or sub-word
units. Recurrent neural networks (RNNs) are
powerful sequence learners that would seem
well suited to such tasks. However, because
they require pre-segmented training data,
and post-processing to transform their outputs into label sequences, their applicability
has so far been limited. This paper presents a
novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the
TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a
hybrid HMM-RNN.},
	Annote = {Easy to digest summary of CTC: https://distill.pub/2017/ctc/. ``Connectionist Temporal Classification (CTC) is a way to get around not knowing the alignment between the input and the output''},
	Author = {Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
	Booktitle = {Proceedings of the 23rd international conference on Machine learning},
	Date-Added = {2019-12-10 13:50:44 -0800},
	Date-Modified = {2020-04-21 12:51:39 -0400},
	Keywords = {deep learning, speech-to-text, ocr, ctc},
	Organization = {ACM},
	Pages = {369--376},
	Read = {1},
	Title = {Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks},
	Year = {2006},
	Bdsk-Url-1 = {https://www.cs.toronto.edu/~graves/icml_2006.pdf}}

@article{mehra2019penalty,
	Abstract = {Bilevel optimizations are at the center of several important machine learning problems such
as hyperparameter tuning, data denoising, fewshot learning, data poisoning. Different from simultaneous or multi-objective optimization, obtaining the exact descent direction for continuous bilevel optimization requires computing the
inverse of the hessian of the lower-level cost
function, even for first order methods. In this
paper, we propose a new method for solving
bilevel optimization, using the penalty function,
which avoids computing the inverse of the hessian. We prove convergence of the method under mild conditions and show that it computes
the exact hypergradient asymptotically. Small
space and time complexity of our method allows us to solve large-scale bilevel optimization problems involving deep neural networks
with up to 3.8M upper-level and 1.4M lowerlevel variables. We present results of our method
for data denoising on MNIST/CIFAR10/SVHN
datasets, for few-shot learning on Omniglot/MiniImagenet datasets and for training-data poisoning
on MNIST/Imagenet datasets. In all experiments,
our method outperforms or is comparable to previously proposed methods both in terms of accuracy
and run-time},
	Author = {Mehra, Akshay and Hamm, Jihun},
	Date-Modified = {2019-12-10 13:52:10 -0800},
	Journal = {arXiv preprint arXiv:1911.03432},
	Keywords = {bilevel optimization, penalty method,},
	Read = {1},
	Title = {Penalty Method for Inversion-Free Deep Bilevel Optimization},
	Url = {https://arxiv.org/pdf/1911.03432.pdf},
	Year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1911.03432.pdf}}

@article{pedregosa2016hyperparameter,
	Abstract = {Most models in machine learning contain at least
one hyperparameter to control for model complexity. Choosing an appropriate set of hyperparameters is both crucial in terms of model accuracy and computationally challenging. In this
work we propose an algorithm for the optimization of continuous hyperparameters using inexact gradient information. An advantage of this
method is that hyperparameters can be updated
before model parameters have fully converged.
We also give sufficient conditions for the global
convergence of this method, based on regularity
conditions of the involved functions and summability of errors. Finally, we validate the empirical
performance of this method on the estimation of
regularization constants of `2-regularized logistic regression and kernel Ridge regression. Empirical benchmarks indicate that our approach is
highly competitive with respect to state of the art
methods.},
	Author = {Pedregosa, Fabian},
	Date-Modified = {2020-04-30 09:36:48 -0400},
	Journal = {arXiv preprint arXiv:1602.02355},
	Keywords = {hyperparameter, approximate gradient},
	Read = {1},
	Title = {Hyperparameter optimization with approximate gradient},
	Url = {https://arxiv.org/pdf/1602.02355.pdf},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1602.02355.pdf}}

@article{lorraine2019optimizing,
	Abstract = {We propose an algorithm for inexpensive
gradient-based hyperparameter optimization
that combines the implicit function theorem
(IFT) with efficient inverse Hessian approximations. We present results on the relationship between the IFT and differentiating through optimization, motivating our algorithm. We use the proposed approach to
train modern network architectures with millions of weights and millions of hyperparameters. We learn a data-augmentation network---
where every weight is a hyperparameter tuned
for validation performance---that outputs augmented training examples; we learn a distilled
dataset where each feature in each datapoint
is a hyperparameter; and we tune millions
of regularization hyperparameters. Jointly
tuning weights and hyperparameters with our
approach is only a few times more costly in
memory and compute than standard training.},
	Annote = {Not super novel, more like putting together a lot of good ideas. But it's well explained and interesting. I'm still not sure about Laplacian approximation, though. Need to read the actual paper for that.},
	Author = {Lorraine, Jonathan and Vicol, Paul and Duvenaud, David},
	Date-Modified = {2020-04-30 09:36:48 -0400},
	Journal = {arXiv preprint arXiv:1911.02590},
	Keywords = {hyperparameter, bilevel optimization, inverse Hessian approximation},
	Read = {1},
	Title = {Optimizing Millions of Hyperparameters by Implicit Differentiation},
	Url = {https://arxiv.org/pdf/1911.02590.pdf},
	Year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1911.02590.pdf}}
