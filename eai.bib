%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Ben Crestel at 2021-01-18 15:58:56 -0500 


%% Saved with string encoding Unicode (UTF-8) 


@comment{jabref-meta: databaseType:bibtex;}



@article{kidger2020neural,
	author = {Kidger, Patrick and Morrill, James and Foster, James and Lyons, Terry},
	date-added = {2021-01-18 15:55:18 -0500},
	date-modified = {2021-01-18 15:55:47 -0500},
	journal = {arXiv preprint arXiv:2005.08926},
	keywords = {neural ODE, time series, deep learning, },
	title = {Neural controlled differential equations for irregular time series},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2005.08926.pdf}}

@article{ha2018world,
	author = {Ha, David and Schmidhuber, J{\"u}rgen},
	date-added = {2021-01-17 19:41:15 -0500},
	date-modified = {2021-01-17 19:41:53 -0500},
	journal = {arXiv preprint arXiv:1803.10122},
	keywords = {reinforcement learning, world model, deep learning, **MUST-READ**},
	title = {World models},
	year = {2018},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1803.10122.pdf}}

@article{lavin2020technology,
	author = {Lavin, Alexander and Renard, Gregory},
	date-added = {2021-01-14 12:25:14 -0500},
	date-modified = {2021-01-14 12:25:31 -0500},
	journal = {arXiv preprint arXiv:2006.12497},
	keywords = {machine learning, production},
	title = {Technology readiness levels for machine learning systems},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2101.03989.pdf}}

@inproceedings{sculley2015hidden,
	author = {Sculley, David and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-Francois and Dennison, Dan},
	booktitle = {Advances in neural information processing systems},
	date-added = {2021-01-13 16:23:39 -0500},
	date-modified = {2021-01-13 16:23:57 -0500},
	keywords = {machine learning, dev},
	pages = {2503--2511},
	title = {Hidden technical debt in machine learning systems},
	year = {2015},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf}}

@article{recht2011hogwild,
	author = {Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
	date-added = {2021-01-10 23:15:28 -0500},
	date-modified = {2021-01-10 23:16:21 -0500},
	journal = {Advances in neural information processing systems},
	keywords = {stochastic gradient descent, optimization, parallel},
	pages = {693--701},
	title = {Hogwild!: A lock-free approach to parallelizing stochastic gradient descent},
	volume = {24},
	year = {2011},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1106.5730.pdf}}

@article{quadrana2018sequence,
	abstract = {Recommender systems are one of the most successful applications of data mining and machine learning
technology in practice. Academic research in the field is historically often based on the matrix completion
problem formulation, where for each user-item-pair only one interaction (e.g., a rating) is considered. In many
application domains, however, multiple user-item interactions of different types can be recorded over time.
And, a number of recent works have shown that this information can be used to build richer individual user
models and to discover additional behavioral patterns that can be leveraged in the recommendation process.
In this work we review existing works that consider information from such sequentially-ordered useritem interaction logs in the recommendation process. Based on this review, we propose a categorization
of the corresponding recommendation tasks and goals, summarize existing algorithmic solutions, discuss
methodological approaches when benchmarking what we call sequence-aware recommender systems, and
outline open challenges in the area},
	annote = {1 INTRODUCTION

``In this survey work, we therefore (i) categorize the various scenarios of sequence-aware recommendations approaches in the academic literature, (ii) we review the various algorithmic approaches that were proposed to extract and leverage patterns from interaction logs, and (iii) we finally discuss specific issues when benchmarking different recommendation methods. One of the major goals of the review in that context is to lay the path for more standardized and better reproducible research works in the field.''

2 CHARACTERIZING SEQUENCE-AWARE RECOMMENDER SYSTEMS

	2.1 Inputs, Outputs, Computational Tasks, and Abstract Problem Characterization
``Inputs. The main input to sequence-aware recommendation problems is an ordered and often timestamped list of past user actions.''
Actions are not required to be tied to either a user or an item.

Outputs. The output of a sequence-aware recommender are ordered lists of items

Keep characterizing other parts of a sequence-aware recsys.

	2.2 Relation to Other Areas
Implicit-Feedback Recommender Systems. Explicit rating can be more confused than anything else for sequence-aware recsys as time of review might differ significantly from time of purchase

Context-Aware and Time-Aware Recommender Systems (TARS). Similarity between context-aware and sequence-aware recsys. Also some similarities with TARS but not as much as sequence-aware recsys focus on the sequentiality, not the time aspect.

Research in Other Related Fields. Similarity with interest drift (user modeling) or query suggestion (information retrieval)

3 A CATEGORIZATION OF SEQUENCE-AWARE RECOMMENDATION TASKS

Four main goals for which one case use sequence-aware recsys:

	3.1 Context Adaptation
Context-aware recsys also relies on representational context, ie, set of observable context variables. For instance, the user's geographical position, the current weather, or the time of
the day.
Other contextual info has to be derived, like interactional context (user's current shopping intent, or current mood). Particularly important with anonymous users.

``Overall, understanding the users' situation and goals and making context-adapted recommendations from past interaction data represents a main goal of sequence-aware recommender systems.''

Categorization based on importance of long- and short-term interactions. Different sub-categories depending on data available and/or task at hand:
* last-N interactions based recommendation: often the case for location-aware recsys
* session-based recommendation: per user data available is within a session
* session-aware recommendation: when user log in, can use long-term and short-term info

On utility functions and recommendation purposes. Utility function is not obvious and should be carefully designed to reflect the goal of the recsys

	3.2 Trend Detection
Other possible use for seq-aware recsys, but less explored
* Community trends. Detect change in popularity for items
* Individual trends. Detect change in user's interest

	3.3 Repeated Recommendation
Can be meaningful in some domains.
* Identifying repeated user behavior patterns. eg, printer ink cartridge, or assist use of a device
* Repeated recommendations as reminders. Remind user of sthg they found interesting in the past

	3.4 Consideration of Order Constraints and Observed Sequential Patterns
Additional constraints when considering order of recommendations:
a) external domain knowledge (eg, movie sequels, or what courses to take)
b) predict sequential pattern
Factors that can influence ordering:
* Importance of the order of the recommendations. 
* Importance of the exact order of the past events.
* Existence of implicit order constraints. 

	3.5 Categorization of Existing Works
Organize existing work according to their categorization:
Context SA: Session-aware, SB: Session-based, LI: Last-N interactions; Order Constraints W: Weak, I: Inferred, S: Strong; Application domain APP: App recommendation, MUS: Music domain, QRY: Query recommendation, EC: E-Commerce domain, CS: Learning courses
recommendations, AD: Advertisements, WWW: Web navigation recommendation, OTH: Others.
Existing work was hand-selected from a search on ACM Digital Library.

		3.5.1 Categorization based on Context-Adaptation Type.
Most papers relied on last-N interaction (32%), followed by session-based (22%), and session-aware (17%).
However, session-based systems have grown increasingly more popular over the last few years, and is connected to new deep learning based sequence-learning techniques.

		3.5.2 Order Constraints.
Most work relied on implicitly derived ordering constraints (consequently weak). Might be due to academic bias toward certain industries.

		3.5.3 Categorization based on Domain.
Most applications in e-commerce (25%; session-based, session-aware, next-basket rec,{\ldots}) and music (15%)

		3.5.4 Specific Tasks of Sequence-aware Recommendation.
* Repeated Recommendation (rarely the focus in research)
	Find items for repeated recommendation
	Find timing for repeated recommendation
* Trend detection (not investigated to a large extent)
	Detect individual trends
	Detect community trend
	Detect seasonal trends

4 ALGORITHMS FOR SEQUENCE-AWARE RECOMMENDER SYSTEMS

	4.1 Sequence Learning
Most popular type of approach for domains with a natural sequentiality (NLP, genetic, forecasting,{\ldots})

		4.1.1 Frequent Pattern Mining (FPM).
List rules of frequent pattern from training dataset. At predict time, search all rules for a matching pattern. Rules come with metrics (confidence, support,{\ldots}) to express their strenght

Personalization can be obtained by filtering patterns to search based on user features

Discussion. Method well explored and easy to implement. Problem with scalability, and with hyperparameter tuning (threshold value).

		4.1.2 Sequence modeling.
Could treat the input sequence as a discrete time series and turn recommendation problem as one of time series prediction. In practice, it's a bit different. Methods come from 3 categories:
* Markov Models: usual data sparsity leads to problem with Markov models. Requires tricks (skipping, clustering, finite mixture modeling). Also challenge choosing the right order of the MC chain. HMMs solves a few of these problems naturally; interesting applications using HMMs.
* Reinforcement Learning: has been used for sequence-aware recsys in online shops.
* RNNs: looks like an active area of research (RNNs, GRUs, LSTMs)

Discussion. Not clear if Markov models are a practical option. DL on the rise thanks to growing size of datasets.

		4.1.3 Distributed Item Representations
Build a lower-dimensional representation of the items that accounts for the sequentiality.

Discussion. Works well in NLP and for recsys applied to text data. But heavy computationally.
Embedding methods leverage sequentiality to build embedding, but don't directly use it to make prediction. Therefore requires other method to generate sequence-aware prediciton. Not ideal.

		4.1.4 Supervised Learning with Sliding Windows
Slide window forward; at each step, uses past n-actions as features and try to predict the next one; so multi-class classification problem, with auto-regressive features.

Discussion. Good b/c many models exist to solve that problem. 
However, scales poorly with number of possible actions/items/{\ldots}. Can be tricky to set up (hyperparameters + feature engineering required).

	4.2 Sequence-aware Matrix Factorization
Integrate time-depending parameters into matrix factorization techniques, typically on the loss (weighted loss).
Not to be confused with more general time-aware matrix factorization.

Discussion. Good: can use standard matrix factorization alg.
Bad: Factorization model needs to be updated when new ratings are provided for suggested next step.

	4.3 Hybrid methods
Typically combine sequence learning methods with matrix-completion techniques. Several applications include a topic extraction step via LDA.

Discussion. Hybrid methods are a popular approach as they can solve many of the shortcomings of individual methods. But trick is how to optimally combine these different techniques.

	4.4 Other methods
Graph-based methods.
Discrete optimization methods. Discrete optimization is often employed for sequence-aware recommendation problems when weak or strict ordering constraints between items exist

	4.5 Summary and Pros and Cons of Selected Approaches
Frequent Pattern Mining: Discover patterns in user action sequences
+: Easy implementation; Explainable results
-: Complex configuration; Suffers from data sparsity; Limited scalability
Markov Chains: Compute transition probabilities
over fixed-length sequences
+ Explainable results
- Fixed transition order; Suffers from data sparsity; Limited scalability
Variable-order Markov Models: Compute transition probabilities over variable-length sequences
+ Variable transition orders; Explainable results
- Suffers from data sparsity
HMM Model the causal factors in user sequences as transitions between discrete hidden states
+ Learns from variable-length inputs; Robust to data sparsity
- Limited explainability; Huge number of discrete parameters
RL Directly maximize the customer and seller reward over time
+ Dynamically adapt recommendations to future (unknown) rewards; Under active research
- MDP-based approaches have same issues as MCs; Limited explainability
RNN Model the causal factors in user sequences with non-linear transitions between continuous hidden states
+ Learns from variable-length inputs; Learns long-term dependencies; Robust to data sparsity; Compact hidden states; Under active research
- Complex configuration; Limited explainability; Benefits not fully clear in some domains
Distributed Item Reprensetations: Embed items into latent spaces that preserves sequential transition properties
+ Robust to data sparsity; Visually interpetable embeddings; Under active research
- Need auxiliary methods to make recommendations; Limited explainability
Supervised Learning: Use supervised learning over features extracted from fixed-size sliding windows over sequences
+ Easy implementation; Use off-the-shelf supervised algorithms
- Explainability depends on the chosen supervised method; Feature engineering
Matrix Factorization: Define new inputs and loss functions for MF to handle sequences
+ Extensive literature available; Robust to data sparsity
- Non-trivial input and loss design; Concerns regarding scalability

5 EVALUATION OF SEQUENCE-AWARE RECOMMENDER SYSTEMS
	5.1 Common Evaluation Approaches for Recommender Systems
* offline evaluation: preferred in academia
* user studies: 
* field studies, ie A/B testing

	5.2 Offline Evaluation of Sequence-aware Recommenders
		5.2.1 Evaluation Methodologies
Use framework defined for evaluation of TARS

* Dataset partitioning: 
- event-level vs session-level: Decide if you break sessions when splitting. event-level means you break sessions, session-level means you assign entire sessions to train or valid based on the first date of the session.
- user-level vs community-level: community-level means you spit all users the same. user-level means you also split users into train/test. Test users are then split according to the specific date chosen; early data for test users is used to build their profile (user profile), then recent data is user to test (test data).
- How to split? Could be 80/20, or fixed date, or fixed nb of entries (eg, last k events)
- No common agreement on right way to do it. However for session-based or session-aware recsys makes sense to split at session-level.

* Def of target items:
Target is not just a rating or a top-k items list, but a sequence of events/actions. Diff variations of evaluation scheme:
- Sequence-agnostic prediction: order doesn't matther, so hide a few items in sequence and try to predict them.
- Given-N next-item prediction: based on first N predictions, predict immediate next action.
- Given-N next-item prediction with look-ahead: based on first N predictions, but order of predictions irrelevant
- Given-N next-item prediction with look-back: also provide info about what happened before N initial predictions 
Choice of target depends on scenario we're in: session-based recsys (given-N next-item preferred), dependence on short-term/long-term info (given-N next-item with look-back),{\ldots}

* Cross validation:
Need to use sequential cross-validation. Techniques for time series

		5.2.2 Evaluation Metrics.
Typical rely on standard metrics for regression or classification. Then, choice does not have much impact on quality of the results.
However, there is a need for development of specialized metrics for sequential recsys.

		5.2.3 Data Sets.
Provides a list of public datasets for diff industries

	5.3 On User Studies and Field Tests
User studies are rare in the literature. Also have their limitations (artificial set-up).
Limited reports of field tests for sequential recsys. But can show limitations of offline study. Highlights challenges of finding good proxy measures for offline study.

6 SUMMARY AND FUTURE DIRECTIONS
Open research questions:
- Intent detection: Can't be done right away. Need to understand context and intent. Can also change during the course of a session. , in the context of intent detection, very little research exists on how to give users the opportunity to correct the system's assumption in case they are wrong. 
- Combining short-term and longer-term profiles: Some works show that while the short-term intentions should be predominant in the selection of the recommendations, considering longer-term behavioral patterns and preferences of the individual user can be important. More work is needed in that area.
- Leveraing additional data and general trends: Generally, in the context of user profiling, researchers often rely on one or a few types of specific user interactions like item view events or check-ins at certain locations. In real-world applications, usually much richer types of information are available
- Toward standardized and more comprehensive evaluations: need to move beyond accuracy measures only.},
	author = {Quadrana, Massimo and Cremonesi, Paolo and Jannach, Dietmar},
	date-added = {2021-01-08 18:16:12 -0500},
	date-modified = {2021-01-18 14:49:26 -0500},
	journal = {ACM Computing Surveys (CSUR)},
	keywords = {recommender systems, time series},
	number = {4},
	pages = {1--36},
	publisher = {ACM New York, NY, USA},
	title = {Sequence-aware recommender systems},
	volume = {51},
	year = {2018},
	Bdsk-Url-1 = {https://re.public.polimi.it/retrieve/handle/11311/1084454/368312/2018-seqrec_survey.pdf}}

@article{ruthotto2017jinv,
	author = {Ruthotto, Lars and Treister, Eran and Haber, Eldad},
	date-added = {2021-01-07 16:50:52 -0500},
	date-modified = {2021-01-07 16:51:26 -0500},
	journal = {SIAM Journal on Scientific Computing},
	keywords = {julia, inverse problem, pde, differentiable programming},
	number = {5},
	pages = {S702--S722},
	publisher = {SIAM},
	title = {jinv--a flexible Julia package for PDE parameter estimation},
	volume = {39},
	year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1606.07399.pdf}}

@article{bezanson2019scientific,
	author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B},
	date-added = {2021-01-07 16:49:08 -0500},
	date-modified = {2021-01-07 16:49:24 -0500},
	journal = {SIAM News},
	keywords = {julia, differentiable programming},
	title = {Scientific Machine Learning: How Julia Employs Differentiable Programming to Do it Best},
	volume = {5},
	year = {2019},
	Bdsk-Url-1 = {https://sinews.siam.org/Details-Page/scientific-machine-learning-how-julia-employs-differentiable-programming-to-do-it-best}}

@article{egorov2017pomdps,
	author = {Egorov, Maxim and Sunberg, Zachary N and Balaban, Edward and Wheeler, Tim A and Gupta, Jayesh K and Kochenderfer, Mykel J},
	date-added = {2021-01-07 16:46:52 -0500},
	date-modified = {2021-01-07 16:47:39 -0500},
	journal = {The Journal of Machine Learning Research},
	keywords = {decision making, julia, uncertainty, differentiable programming},
	number = {1},
	pages = {831--835},
	publisher = {JMLR. org},
	title = {POMDPs. jl: A framework for sequential decision making under uncertainty},
	volume = {18},
	year = {2017},
	Bdsk-Url-1 = {https://www.jmlr.org/papers/volume18/16-300/16-300.pdf}}

@article{innes2019differentiable,
	author = {Innes, Mike and Edelman, Alan and Fischer, Keno and Rackauckas, Christopher and Saba, Elliot and Shah, Viral B and Tebbutt, Will},
	date-added = {2021-01-07 16:44:56 -0500},
	date-modified = {2021-01-07 16:46:03 -0500},
	journal = {CoRR, abs/1907.07587},
	keywords = {differentiable programming, machine learning, scientific computing, julia, **MUST-READ**},
	title = {A differentiable programming system to bridge machine learning and scientific computing},
	year = {2019},
	Bdsk-Url-1 = {https://deepai.org/publication/a-differentiable-programming-system-to-bridge-machine-learning-and-scientific-computing},
	Bdsk-Url-2 = {https://arxiv.org/pdf/1907.07587.pdf}}

@book{powell2020rl,
	author = {Warren B. Powell},
	date-added = {2021-01-07 15:57:52 -0500},
	date-modified = {2021-01-07 15:59:48 -0500},
	keywords = {reinforcement learning, optimization},
	publisher = {Wiley-Interscience},
	title = {Reinforcement Learning and Stochastic Optimization: A unified framework for sequential decisions},
	year = {2020},
	Bdsk-Url-1 = {https://castlelab.princeton.edu/wp-content/uploads/2020/01/Powell-RLSO-Jan202020.pdf}}

@article{franccois2018introduction,
	author = {Fran{\c{c}}ois-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G and Pineau, Joelle},
	date-added = {2021-01-07 15:49:18 -0500},
	date-modified = {2021-01-07 15:49:36 -0500},
	journal = {arXiv preprint arXiv:1811.12560},
	keywords = {reinforcement learning, deep learning},
	title = {An introduction to deep reinforcement learning},
	year = {2018},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1811.12560.pdf?fbclid=IwAR3sArXMkmy7yL_baZJ2QLa1Ud8LaC8x6s-0VvXb8WEkdjzBOnck-DI-yBA}}

@article{boyd2017multi,
	author = {Boyd, Stephen and Busseti, Enzo and Diamond, Steven and Kahn, Ronald N and Koh, Kwangmoo and Nystrup, Peter and Speth, Jan},
	date-added = {2021-01-06 00:49:49 -0500},
	date-modified = {2021-01-06 00:50:16 -0500},
	journal = {arXiv preprint arXiv:1705.00109},
	keywords = {finance, trading, optimization},
	read = {1},
	title = {Multi-period trading via convex optimization},
	year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1705.00109.pdf}}

@article{silver2016mastering,
	author = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
	date-added = {2021-01-05 00:37:42 -0500},
	date-modified = {2021-01-05 01:09:44 -0500},
	journal = {nature},
	keywords = {deep learning, reinforcement learning, monte carlo tree search, **MUST-READ**},
	number = {7587},
	pages = {484--489},
	publisher = {Nature Publishing Group},
	title = {Mastering the game of Go with deep neural networks and tree search},
	volume = {529},
	year = {2016},
	Bdsk-Url-1 = {http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15780-s16/www/AlphaGo.nature16961.pdf}}

@article{marcus2020next,
	author = {Marcus, Gary},
	date-added = {2021-01-04 14:37:43 -0500},
	date-modified = {2021-01-04 14:38:06 -0500},
	journal = {arXiv preprint arXiv:2002.06177},
	keywords = {**MUST-READ**, ai},
	title = {The next decade in ai: four steps towards robust artificial intelligence},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2002.06177v3.pdf}}

@article{chen2020self,
	abstract = {In unsupervised domain adaptation, existing theory focuses on situations where the
source and target domains are close. In practice, conditional entropy minimization
and pseudo-labeling work even when the domain shifts are much larger than those
analyzed by existing theory. We identify and analyze one particular setting where
the domain shift can be large, but these algorithms provably work: certain spurious
features correlate with the label in the source domain but are independent of the
label in the target. Our analysis considers linear classification where the spurious
features are Gaussian and the non-spurious features are a mixture of log-concave
distributions. For this setting, we prove that entropy minimization on unlabeled
target data will avoid using the spurious feature if initialized with a decently
accurate source classifier, even though the objective is non-convex and contains
multiple bad local minima using the spurious features. We verify our theory for
spurious domain shift tasks on semi-synthetic Celeb-A and MNIST datasets. Our
results suggest that practitioners collect and self-train on large, diverse datasets to
reduce biases in classifiers even if labeling is impractical.},
	author = {Chen, Yining and Wei, Colin and Kumar, Ananya and Ma, Tengyu},
	date-added = {2020-12-17 14:25:47 -0500},
	date-modified = {2020-12-17 14:26:22 -0500},
	journal = {Advances in Neural Information Processing Systems},
	keywords = {concept drift, deep learning, **MUST-READ**},
	title = {Self-training avoids using spurious features under domain shift},
	volume = {33},
	year = {2020},
	Bdsk-Url-1 = {https://proceedings.neurips.cc/paper/2020/file/f1298750ed09618717f9c10ea8d1d3b0-Paper.pdf}}

@article{wang2019machine,
	abstract = {Survival analysis is a subfield of statistics where the goal is to analyze and model the data where the outcome is the time until the occurrence of an event of interest. One of the main challenges in this context is the presence of instances whose event outcomes become unobservable after a certain time point or when some instances do not experience any event during the monitoring period. Such a phenomenon is called censoring which can be effectively handled using survival analysis techniques. Traditionally, statistical approaches have been widely developed in the literature to overcome this censoring issue. In addition, many machine learning algorithms are adapted to effectively handle survival data and tackle other challenging problems that arise in real-world data. In this survey, we provide a comprehensive and structured review of the representative statistical methods along with the machine learning techniques used in survival analysis and provide a detailed taxonomy of the existing methods. We also discuss several topics that are closely related to survival analysis and illustrate several successful applications in various real-world application domains. We hope that this paper will provide a more thorough understanding of the recent advances in survival analysis and offer some guidelines on applying these approaches to solve new problems that arise in applications with censored data.},
	author = {Wang, Ping and Li, Yan and Reddy, Chandan K},
	date-added = {2020-12-15 20:40:46 -0500},
	date-modified = {2020-12-15 22:32:56 -0500},
	journal = {ACM Computing Surveys (CSUR)},
	keywords = {survival analysis, machine learning},
	number = {6},
	pages = {1--36},
	publisher = {ACM New York, NY, USA},
	read = {1},
	title = {Machine learning for survival analysis: A survey},
	volume = {51},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1708.04649.pdf}}

@article{he2009learning,
	author = {He, Haibo and Garcia, Edwardo A},
	date-added = {2020-12-15 20:32:10 -0500},
	date-modified = {2020-12-15 20:32:24 -0500},
	journal = {IEEE Transactions on knowledge and data engineering},
	keywords = {imbalanced data, supervised learning},
	number = {9},
	pages = {1263--1284},
	publisher = {Ieee},
	title = {Learning from imbalanced data},
	volume = {21},
	year = {2009},
	Bdsk-Url-1 = {https://www.ele.uri.edu/faculty/he/PDFfiles/ImbalancedLearning.pdf}}

@article{chandola2009anomaly,
	abstract = {Anomaly detection is an important problem that has been researched within diverse research areas and application domains. Many anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key assumptions, which are used by the techniques to differentiate between normal and anomalous behavior. When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category. We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with.},
	annote = {1. INTRODUCTION
``Anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. These non-conforming patterns are often referred to as anomalies, outliers, discordant observations, exceptions, aberrations, surprises, peculiarities or contaminants in different application domains.''
	1.1 What are anomalies?
``The ``interestingness'' or real life relevance of anomalies is a key feature of anomaly detection''
``Another topic related to anomaly detection is novelty detection which aims at detecting previously unobserved (emergent, novel) patterns in the data, e.g., a new topic of discussion in a news group. The distinction between novel patterns and anomalies is that the novel patterns are typically incorporated into the normal model after being detected.'': sounds a lot like concept drift; I suspect boundary is blurry between both fields

	1.2 Challenges
General anomaly detection is really hard, so you must focus on a specific aspect. What aspect is driven by the domain, the data, the problem,{\ldots}

2. DIFFERENT ASPECTS OF AN ANOMALY DETECTION PROBLEM
	2.2 Type of Anomaly
		2.2.1 Point Anomalies
An individual instance is categorized as anomalous; most of the research in AD focuses on that problem
		2.2.2 Contextual Anomalies / Conditional Anomalies
Data instance is considered an anomaly in a certain context, but not in another (e.g., temperature throughout the year). Most common in time-series data or spatial data.
		2.2.3 Collective Anomalies
Individual events are not necessarily an anomly, but put together they form an anomaly

	2.3 Data Labels
The extent to which data labels are available drive the types of AD techniques we can use
		2.3.1 Supervised anomaly detection
Assume we have a labled dataset (normal / anormal), then build predictive model
Problem with class imbalance, and in obtaining accurate labels.
		2.3.2 Semi-Supervised anomaly detection
Assume we only have normal data, then build model for normal behaviour. Anomaly is when data doesn't match model.
		2.3.3 Unsupervised anomaly detection
Works w/o labels. Assumes that anomalies are far fewer than normal behaviours.
Semi-supervised techniques can be applied here by taking a sample of the data for training (again assumption that very few anomalies)

	2.4 Output of Anomaly Detection
		2.4.1 Scores
		2.4.2 Labels},
	author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
	date-added = {2020-12-15 19:26:20 -0500},
	date-modified = {2020-12-15 20:33:00 -0500},
	journal = {ACM computing surveys (CSUR)},
	keywords = {anomaly detection},
	number = {3},
	pages = {1--58},
	publisher = {ACM New York, NY, USA},
	title = {Anomaly detection: A survey},
	volume = {41},
	year = {2009},
	Bdsk-Url-1 = {http://cinslab.com/wp-content/uploads/2019/03/xiaorong.pdf}}

@inproceedings{gasthaus2019probabilistic,
	author = {Gasthaus, Jan and Benidis, Konstantinos and Wang, Yuyang and Rangapuram, Syama Sundar and Salinas, David and Flunkert, Valentin and Januschowski, Tim},
	booktitle = {The 22nd International Conference on Artificial Intelligence and Statistics},
	date-added = {2020-12-15 10:52:13 -0500},
	date-modified = {2020-12-15 10:52:36 -0500},
	keywords = {distributional forecast, forecasting, deep learning},
	pages = {1901--1910},
	title = {Probabilistic forecasting with spline quantile function rnns},
	year = {2019},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v89/gasthaus19a/gasthaus19a.pdf}}

@incollection{ricci2011introduction,
	annote = {1.2 Recommender Systems' Function
Discuss distinction of recsys from the service provider and user point of view.
List reasons for a service provide to user recsys
Also list tasks that can be accomplished from a recsys (directly or indirectly)
Roles/Tasks are quite diverse. Recsys need to be able to leverage vast quantity of information

1.3 Data and Knowledge Sources
Data refers to either user, item, or transaction (relation between user/item)
Highlights importance of the user model in order to personalize recommendations

1.4 Recommendation Techniques
Model recommendation as the prediction of the utility of an item for a user
Some recsys make a prediction w/o estimating the utility (e.g., knowledge-based recsys). But direct recommendation can become increasingly difficult when recommendation depends on many other factors (location, time,{\ldots})

Taxonomy of recsys (by Burke):
- Content based: recommend items similar to the ones that a user likes in the past. Classical content-based recsys match user features with item features. Modern approaches rely on semantic indexing.
- Collaborative Filtering: Base recommendation off similarities between past user transactions.
Simplest collab filtering approach = neighboorhood based -> can be user-based or item-based
Other approach = latent factor models (e.g, matrix decomposition)
- Demographic: make recommendation based on demographic attributes
- Knowledge-based: similarity function measures how the user's needs match with a specific recommendation. Typically works well at deployment, but performance degrades overtime if there is no learning component.
- Community based: recommendations based on your friends preference
- Hybrid: combination of previous types of recsys

Then discuss the importance of context in recommendation, and how to include it in the recsys: reduction-based, contextual post-filtering, or context modeling (full inclusion of context in the recommendation)

Data mining techniques can be used to improve recsys performance: dim redux, bayesian networks, svm, decision trees,{\ldots}

1.5 Recommender Systems Evaluation
Evaluations necessary at different stages, e.g., model selection, or evalatuion of model in production
Can be offline (model selection) or user-centric either online of focused study
Evaluation should be broader than just accuracy. They list: coverage, cold-start, confidence, trust, novelty, risk, serendipity
Also emphasize the important of explainability

1.6 Recommender Systems Applications
- Designing recsys: importance of domain: entertainment, content, e-commerce, services, social

1.7 Recsys and Human-Computer interaction
Recsys should aim at help decision making. It is therefore important to better understand the process of decision making for humans.
Adoption of a recsys is not just about its accuracy. Important factors: trust, transparency, privacy,{\ldots}
Main functionaliities of recsys: eliciting info to construct preference models, narrowing down large set of options, helping users select among small set of options, helping users navigate large space of options
Importance to build credibitiliy of the recsys

1.8 Advanced Topics
List more advanced topics that are covered in the books:
- group of users
- aggregating preferences
- use of newly produced data when recsys in prod -> active learning
- multi-criteria recsys
- novelty and diversity
- cross-domain recsys
- security and robust recommendations

1.9 Challenges
	1.9.1 Perference Acquisition and Profiling
Discuss a few topics: 
- implicit feedback
- actively collecting preference from users (active learning)
- make preference collection interface simple and intuitive
- acquiring personality/mood/emotion information
- cross-domain recsys

	1.9.2 Interaction
``the mechanism through which users provide their input and the means by which they receive the systems output, play a significant role and can play an even larger role in determining the success or failure of a recommender system.''
It's not just about the algorithm, it's also (maybe mainly) about what the user makes of the recommendations and how they ingest them.
Also, question about explainability
Assessment of the value, especially temporal aspect. In some domain, time is of the essence (e.g., news recommendation)

	1.9.3 New Recommendation Tasks
},
	author = {Ricci, Francesco and Rokach, Lior and Shapira, Bracha},
	booktitle = {Recommender systems handbook},
	date-added = {2020-12-15 00:15:09 -0500},
	date-modified = {2020-12-15 19:22:58 -0500},
	keywords = {recommender systems},
	pages = {1--35},
	publisher = {Springer},
	read = {1},
	title = {Introduction to recommender systems handbook},
	year = {2011}}

@article{gomes2019machine,
	author = {Gomes, Heitor Murilo and Read, Jesse and Bifet, Albert and Barddal, Jean Paul and Gama, Jo{\~a}o},
	date-added = {2020-12-14 23:51:57 -0500},
	date-modified = {2020-12-14 23:52:22 -0500},
	journal = {ACM SIGKDD Explorations Newsletter},
	keywords = {streaming, machine learning, **MUST-READ**},
	number = {2},
	pages = {6--22},
	publisher = {ACM New York, NY, USA},
	title = {Machine learning for streaming data: state of the art, challenges, and opportunities},
	volume = {21},
	year = {2019},
	Bdsk-Url-1 = {https://www.researchgate.net/profile/Heitor_Murilo_Gomes/publication/337581742_Machine_learning_for_streaming_data_state_of_the_art_challenges_and_opportunities/links/5ddf9342a6fdcc2837f10be6/Machine-learning-for-streaming-data-state-of-the-art-challenges-and-opportunities.pdf}}

@book{aggarwal2016recommender,
	annote = {Chapter 1: An Introduction to Recommender Systems

	1.1 Introduction
``The entity to which the recommendation is provided is referred to as the user, and the product being recommended is also referred to as an item. Therefore, recommendation analysis is often based on the previous interaction between users and items, because past interests and proclivities are often good indicators of future choices. A notable exception is the case of knowledge-based recommender systems, in which the recommendations are suggested on the basis of user-specified requirements rather than the past history of the user.''

	1.2 Goals of Recommender Systems
Recommendation can be formulated either as i) a prediction problem (predict the unknown user-item rating) and recommend the one with the highest predicted rating), or ii) directly as a ranking problem (directly predict the top-k recommended article).

Desired properties of a recommender system:
i) relevance
ii) novelty: recommended item should be new to the user
iii) serendipity (Serendipity is an unplanned fortunate discovery): items recom- mended are somewhat unexpected
iv) increasing recommendation diverity: top-k items recommended should be different, to prevent the user not liking all of them
-> I think these properties make sense in the context of e-commerce. But in the case of a service provider, you want to make sure that you provide what the user needs. So there is a single good answer that you want to capture. In that case, novelty, serendipity, and maybe even diversity do not necessarily make so much sense.

Examples of recommender systems:
* GroupLens Recommender System: pioneer of collaborative filtering + released large datasets; see also https://grouplens.org/
* Amazon.com Recommender System: uses implicit  (e.g., browsing history, purchase history) and explicit ratings
* Netflix Movie Recommender System: discuss Netflix prize, which represents an ``excellent example of proper evaluation design of recommendation algorithms''. Made significant contributions to latent factor models (Latent Factor models are a state of the art methodology for model-based collaborative filtering. The basic assumption is that there exist an unknown low-dimensional representation of users and items where user-item affinity can be modeled accurately. For example, the rating that a user gives to a movie might be assumed to depend on few implicit factors such as the user's taste across various movie genres. Matrix factorization techniques are a class of widely successful Latent Factor models that attempt to find weighted low-rank approximations to the user-item matrix, where weights are used to hold out missing entries. There is a large family of matrix factorization models based on choice of loss function to measure approximation quality, regularization terms to avoid overfitting, and other domain-dependent formulations.)
* Google News Personalization System: introduce notion of unary ratings (user can only express one sentiment; for instance, can't express they don't like an article)
* Facebook Friend Recommendations: social network is an interesting example as the goal of friend recommendation is to increase the user experience, not directly the company profits. In that sense, that resembles a service provider scenario as described above. ``This problem is also referred to as link prediction in the field of social network analysis. Such forms of recommendations are based on structural relationships rather than ratings data. Therefore, the nature of the underlying algorithms is completely different.''

	1.3 Basic Models of Recommender Systems
What data is used?
a) collaborative filtering methods rely on user-item interactions (ratings, buying behaviour,{\ldots})
b) content-based recommender methods rely on attribute information about users and/or items (textual profiles, relevant keywords,{\ldots})
c) knowledge-based recommender methods rely on user requirements.
d) hybrid systems: combine elements from traditional recommender systems

		1.3.1 Collaborative Filtering Models
a) Memory-based methods or neighboorhood based collaborative filtering alg:
Notion of neighboorhood can be measured at the user level (make recommendations based on similar users) or item level (make recommendations on similar item that were rated by that user)
``The advantages of memory-based techniques are that they are simple to implement and the resulting recommendations are often easy to explain. On the other hand, memory-based algorithms do not work very well with sparse ratings matrices''
b) Model-based method: ``Some examples of such model-based methods include deci- sion trees, rule-based models, Bayesian methods and latent factor models''

			1.3.1.1 Types of Ratings
utility matrix vs ratings matrix: ``While utility matrices are often set to be the same as the ratings matrices, it is possible for the application to explicitly transform the ratings to utility values based on domain-specific criteria. All collaborative filtering algorithms are then applied to the utility matrix rather than the ratings matrix. However, such an approach is rarely used in practice, and most collaborative filtering algorithms work directly with the ratings matrix.''

Discuss differences between implicit, explicit, unary ratings, and especially how the approach would differ

			1.3.1.2 Relationship with Missing Value Analysis
Collaborative filtering resembles missing value analysis, except that it applies to very large and sparse matrices. Which complicated the problem significantly.

			1.3.1.3 Collaborative Filtering as a Generalization of Classification and Regression Modeling
``Collaborative filtering is a generalization of classification/regression modeling in which the prediction is performed in entry-wise fashion rather than row-wise fashion.''

Also discuss the transductive nature of matrix completion, where you cannot easily extrapolate prediction ot new users or items. You can only deal with the matrix that was provided during training. This is by opposition to some more recent methods that are inductive.

		1.3.2 Content-Based Recommender Systems
``In content-based methods, the item descriptions, which are labeled with ratings, are used as training data to create a user-specific classification or regression modeling problem'' This sounds very much like a classification/regression problem, where you are trying to predict ratings using features from the user/item (but not other users ratings).
They talk about user-specific models. But I imagine that the user could be turned into an embedding that could be used as feature.
Could be useful for new items, provided they resemble existing items.
If you do use user-specific models, then you need a lot of data, and in particular a lot of data for any user before you can make recommendations for them. But again, this could be circumvented by learning a user-embedding.
Content-based recsys and knowledge-based recsys are often considered to be closely related.

		1.3.3 Knowledge-Based Recommender Systems
Typically used in low data regime, or for items that have little historical data.
User explicitly defines what he wants, and knowledge-based recsys task is to find the item that best match the user requirements.
2 types:
- Constraint-based recommender systems
- Case-based recommender systems
	
		1.3.4 Demographic Recommender Systems
Build a maping from demographic features to ratings/buying behaviour.

		1.3.5 Hybrid and Ensemble-Based Recommender Systems
Hybridize recommender systems by ensemble different types
	
		1.3.6 Evaluation of Recommender Systems
Evaluation for recsys resembles evaluation for classifiers with some key differences. Ch.7 talks about that

	1.4 Domain-Specific Challenges in Recommender Systems

		1.4.1 Context-Based Recommender Systems
Could be time, location, social info,{\ldots}

		1.4.2 Time-Sensitive Recommender Systems
Time can be included b/c: rating/user opinion evolves with time; or recommendation depends on time (of the day, year,..)
Time component adds another dimension to the problem which aggravates the sparsity problem. It is therefore necessary to have large datasets when considering the time component. For more, see chapt. 9

		1.4.3 Location-Based Recommender Systems
Location can matter b/c of location locality (preference depends on geography), or travel locality (user not willing to travel far)
Location-based recsys are gaining popularity b/c smartphones/GPS.

		1.4.4 Social Recommender Systems
Link/node prediction problem, or viral marketing/influence analysis (how to create a viral marketing campaign)

	1.5 Advanced Topics and Applications
		1.5.1 The Cold-Start Problem in Recommender Systems
Intially, don't have much data. Problematic to train a recsys, especially traditional collaborative filtering approach.
		1.5.2 Attack-Resistant Recommender Systems
		1.5.3 Group Recommender Systems
recsys for a group of users, not just a single user
		1.5.4 Multi-Criteria Recommender Systems
		1.5.5 Active Learning in Recommender Systems
		1.5.6 Privacy in Recommender Systems
		1.5.7 Application Domains
	1.6 Summary

*****************************************************
Chapter 7: Evaluating Recommender Systems

	7.1 Introduction
Evaluation can be done online (eg, A/B testing) or offline. Offline often the most common for research and development.
Offline typically focus on accuracy. Not the most important, but the only one with an objective measure.

	7.2 Evaluation Paradigms
		7.2.1 User Studies
Recruit guinea pigs and let them interact with the recsys.
Expensive, difficult, and often biased

		7.2.2 Online Evaluation
Still relies on real users, but this time for a recsys already deployed.
So less bias, and easier to recruit
Can measure conversion rate, ie, how often user click on recommended item
Led to dev of multi-arm bandit algo, where system tries different recommendation algorithms
However problematic for cold-start

		7.2.3 Offline Evaluation with Historical Data Sets
Most commonly used b/c has been studied a lot. Also, easiest in terms of requirements; you only need an historical dataset (e.g, ratings).
Disadvantage is that it can't account for actions of the user -> ignore serendipity, novelty,{\ldots}. which have important long term consequences

	7.3 General Goals of Evaluation Design
		7.3.1 Accuracy
Accuracy will be different for recsys that predicts ratings or that ranks items. Discussed in subsequent chapter

		7.3.2 Coverage
Metric used to understand how diverse the recommendations are.

		7.3.3 Confidence and Trust
Confidence is how much the recsys trust its recommendation. Need confidence interval for the recommendations. 
Trust is how much the user trust the recsys. Hard/impossible to measure offline

		7.3.4 Novelty
Easiest evaluation of novelty is direclty seeking opinion of the user.
Novelty can also be estimated offline. Idea is that novel recommenders would recomment actions/items that the user would take/buy later in the future (not right away). So remove all actions occuring after a certain time stamp t_0 and some actions that occured before; use that dataset for training. Then realize prediction for each user; predictions of actions that occured before t_0 decrease the novelty measure while actions that occured after t_0 increase the novelty measure.

		7.3.5 Serendipity
Easier evaluated online (collect user feedback).
Can be done offline by comparing (relevant) top-k predictions with top-k predictions of a primitive recsys (eg, content-based recsys). Note that we only compare relevant predictions.

		7.3.6 Diversity
Diversity related to novelty and serendipity.
Diversity measure applies to a set of recommendation (eg, top-k,{\ldots}). Requires a measure of similarity between 2 recommendations. Diversity can be the average pairwise similarity between recommendations.

		7.3.7 Robustness and Stability
Measures resistance to attacks or data drift.

		7.3.8 Scalability
Can be measured by training time, prediction time, and memory requirements.

	7.4 Design Issues in Offline Recommender Evaluation
Train/Validation/Test. 50%-25%-25% standard, but much smaller proportions for validation/test can be used when data is large (e.g, Netflix: 95.91%-1.36%-2.72%)

		7.4.1 Case Study of the Netflix Prize Data Set
Split test set in 2. When submitting results, only results on half of the test set was released (leaderboard). Other half was kept secret and used for actual ranking. Distribution of both halves was identical. Also distribution of probe set (sort of validation set) was identical to distribution of test set.

		7.4.2 Segmenting the Ratings for Training and Testing
Train/valid/test split can be achieved in 2 steps by repeating a process that split dataset in half: i) dataset -> train/test; ii) train -> train/valid
Hold-out vs Cross-validation: recommend cross-validation (split data into q packets, then run train/valid q times by using each packet as the valid set)

		7.4.3 Comparison with Classification Design
Obvious difference row-wise vs entry-wise.
More interesting difference is that for collaborative filtering, hidden entries in the matrix are not random. It's bc user did choose to consume these items. So positive bias for these entries compared to truly missing values (in classification). Sample selection bias

	7.5 Accuracy Metrics in Offline Evaluation
Can measure accuracy based off the ratings or the ranks. Latter covers all cases of recsys (some don't rate before ranking).

		7.5.1 Measuring the Accuracy of Ratings Prediction
Typical metrics: MSE, RMSE (eg, Netflix prize), MAE, and normalized versions.

			7.5.1.1 RMSE versus MAE
Depends on the application (outliers matter?)

			7.5.1.2 Impact of the Long Tail
Standard accuracy measures depend on a small number of very popular items. Unfortunately, merchant profits often depend on the long tail, the products that are not purchased as often. 
Can use weighted metrics to account for the long tail; weighted by profits, utility, {\ldots}

		7.5.2 Evaluating Ranking via Correlation
Two main ways to measure accuracy from ranks. 
i) Spearman rank correlation coefficient: Take the items for which ranks are masked, then rank them for both recsys and ground-truth. Then calculate Pearson's correlation of both. For ties, instead of picking a rank at random, use the average of all tie ranks.
ii) Kendall rank correlation coefficient: compute a credit for all pairwise entries in the testing dataset (per user), then calculate a score. Focus on comparing relative ranking of entries from recsys and ground-truth.

		7.5.3 Evaluating Ranking via Utility
Lists different techniques based on some measure of an item utility for a user. Measures then combine utility to the user with the rank of that item. There exists multiple variations (heuristic for the utility and relevance{\ldots}) of these measures.

		7.5.4 Evaluating Ranking via Receiver Operating Characteristic
Consider the case of implicit feedback. Can quantify relevance of an item as being consumed at some point by the user.
How many items should be recommended depend on how many are relevant. This leads to a trade-off.
Proposes approach: Vary the length of the recommend list and check precision-recall or ROC (recall-FPR)

		7.5.5 Which Ranking Measure is Best?
Recommends utility-based measures (even though ROC very popular) b/c they put more emphasis on top ranked items.

	7.6 Limitations of Evaluation Measures
Talk about common problems inherent to recsys (especially collaborative filtering).
Also discuss extensively question of temporality in the user ratings. Taste evolve over time. But also, what item was rated after which one matters. Model evaluation/selection could be done accounting for temporality of the ratings.

		7.6.1 Avoiding Evaluation Gaming
Careful not introducing additional bias (eg, using user-item coordinates from test set, even if values are not used)


**************************************************************************
Chapter 9: Time- and Location-Sensitive Recommender Systems

	9.1 Introduction
Strong relation to context-aware recsys (chapter 8). But time specific enough to have its own chapter
Time could be explicit (day of week, month,{\ldots} used as features), or implicit (sequentiality of actions, eg, clickstreams)

	9.2 Temporal Collaborative Filtering
3 different approaches to incorporate temporal info:
i) recency-based models: gives more weight to more recent samples
ii) periodic context-based models: filter recommendation based on temporal information
iii) Models that explicitly use time as an independent variable: best results, in particular time-SVD++ is SOTA

		9.2.1 Recency-Based Models
			9.2.1.1 Decay-Based Methods
Obvious application for neighboorhood-based methods
Question remains how to design the decay factors. They show a few approaches
Can also apply weight-base technique to matrix factorization methods. So can be extended to receny-based techniques

			9.2.1.2 Window-Based Methods
More like a filtering approach (drop all rating whose time stamp falls outside a given window)

		9.2.2 Handling Periodic Context
Best done with multidimensional contextual models (see ch. 8)
			9.2.2.1 Pre-Filtering and Post-Filtering
Pre-filter removes samples before making prediction
Post-filtereing makes prediction in the general case, then filter out predictions based on temporal information

			9.2.2.2 Direct Incorporation of Temporal Context
Incorporate temporal context in the model

		9.2.3 Modeling Ratings as a Function of Time
Ratings are modeled as a function of time.
Discuss temporal factor models.

			9.2.3.1 The Time-SVD++ Model
SVD++ model solves collaborative filtering through a partial SVD, framed as an optimization problem.
time-SVD++ extends this model to have some parameters that depend on time. Different parameters are assumed to depend on time in different ways:
* they use (time-)binning for item specific variables
* they use a functional approach for user related variables. So they assume a functional form and estimate the parameters
Note: I'm a little dubious of their functional forms. A power function does not extrapolate in a very sensible way{\ldots} Is this model only any good in-sample? I'm not sure I would trust very far into the future
Also, it is noteworthy by introducing a time parametrization we significantly increase the number of parameters. One has to be careful in a limited-data setting.

	9.3 Discrete Temporal Models
		9.3.1 Markovian Models
Build a order-k Markovian model (state=last k actions). Train (estimate transition probabilities) and use.
Problem: could be a lot of states
Some states could never be visited. One solution is to train order-k and all lower orders. Then if a state is not visited during training, use lower order model, and so on so forth

			9.3.1.1 Selective Markov Models
Problem with naive Markov approach (9.3.1) is the large state space. This can be addressed by pruning some states from the higher-order models. 3 different pruning techniques are presented:
i) support-pruned Markov model: cut-off all states that are unfrequently visited in the training set
ii) confidence-pruned Markov model: cut-off states from which model is not highly confident about its transition (so check whehter two highest transition probabilities are statistically different, using an iid Bernoulli model)
iii) error-pruned Markov model: compare errors (on validation set) from one state in higher-order model against error for immediately lower order model. Prune higher-order state if error greater. (note: don't use any statistical test, or use of cross-validation to measure these errors? probably overfitting)

			9.3.1.2 Other Markovian Alternatives
Mention Hidden Markov Models

		9.3.2 Sequential Pattern Mining
Describes field of sequential pattern mining: studies sub-sequence, their support and confidence. Training is about listing valid rules for these sub-sequences, then using these rules at predict time.
A priori, sub-sequences are not prevented from having gaps. But it is sometimes useful to limit the lenght of these gaps.

	9.4 Location-Aware Recommender Systems
Different types of locality: i) global locality (Spain vs Mexico); ii) travel locality (your neighboorhood); iii) combination

		9.4.1 Preference Locality
Introduces a quad-tree hierarchical, dynamic decomposition of space, which allows addition of new data points on the fly. Prediction can then be done at any desired level (lowest level, or level 0 ie no locality). A collaborative filtering system is trained within each cell.

		9.4.2 Travel Locality
Can be handled with a penalty (penalize distance from the user, eg)

		9.4.3 Combined Preference and Travel Locality
Combine both approaches

	9.5 Summary
It's just context-aware recsys :)},
	author = {Aggarwal, Charu C and others},
	date-added = {2020-12-10 13:08:49 -0500},
	date-modified = {2021-01-06 19:52:03 -0500},
	keywords = {recommender systems},
	publisher = {Springer},
	read = {1},
	title = {Recommender systems},
	volume = {1},
	year = {2016},
	Bdsk-Url-1 = {https://d1wqtxts1xzle7.cloudfront.net/63186644/2016_Book_RecommenderSystems20200503-102501-wsddcn.pdf?1588554359=&response-content-disposition=inline%3B+filename%3DRecommender_Systems_The_Textbook.pdf&Expires=1607627337&Signature=WrcTySU9oAwaPTthYNPMlB9jOwRKrxV520yltUx0NQcnUaPAuWip4lvFC5GvroFYTgwkJkz6J7rksBxq1Uv1PeQZp1E~MrhJhdimevjtr0n91Dj48CmT1ZGWMvvuEhz3eNt~Tb6EN27bOmq688pOV9v6sVgnPJMJ72fZQcBJ98DmfI2hmpYzg-vxcmHXcVJfq6KhDaIzS-fm0MppKJaH~4rkX8VrMSEVlfb~ciHAih34VJurAV-3H3SCX6DdolD-LYQMrWfH7PpwKOG8FG~8au3eUpVuiuRodseXyYVwntlztBu7FEPwuFa-EkjEU04LvS7JJmctAJtr5qsFccONmQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA}}

@misc{tran2020uncertainty,
	author = {Tran, Dustin and Snoek, Jasper and Lakshminarayanan, Balaji},
	date-added = {2020-12-10 11:31:41 -0500},
	date-modified = {2020-12-10 11:35:33 -0500},
	howpublished = {NeurIPS 2020 Tutorial},
	keywords = {uncertainty, out of distribution, deep learning},
	month = {december},
	title = {Practical Uncertainty Estimation & Out-of-Distribution Robustness in Deep Learning},
	year = {2020},
	Bdsk-Url-1 = {https://neurips.cc/media/Slides/nips/2020/virtual(07-13-30)-07-16-00UTC-16649-track2_practi.pdf}}

@article{wenzel2020hyperparameter,
	abstract = {Ensembles over neural network weights trained from different random initialization,
known as deep ensembles, achieve state-of-the-art accuracy and calibration. The
recently introduced batch ensembles provide a drop-in replacement that is more
parameter efficient. In this paper, we design ensembles not only over weights,
but over hyperparameters to improve the state of the art in both settings. For
best performance independent of budget, we propose hyper-deep ensembles, a
simple procedure that involves a random search over different hyperparameters,
themselves stratified across multiple random initializations. Its strong performance
highlights the benefit of combining models with both weight and hyperparameter
diversity. We further propose a parameter efficient version, hyper-batch ensembles,
which builds on the layer structure of batch ensembles and self-tuning networks.
The computational and memory costs of our method are notably lower than typical
ensembles. On image classification tasks, with MLP, LeNet, ResNet 20 and Wide
ResNet 28-10 architectures, we improve upon both deep and batch ensembles.},
	author = {Wenzel, Florian and Snoek, Jasper and Tran, Dustin and Jenatton, Rodolphe},
	date-added = {2020-12-10 11:13:39 -0500},
	date-modified = {2020-12-10 11:14:12 -0500},
	journal = {Advances in Neural Information Processing Systems},
	keywords = {ensemble, deep learning, uncertainty, hyperparameter,},
	title = {Hyperparameter ensembles for robustness and uncertainty quantification},
	volume = {33},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2006.13570.pdf}}

@article{taieb2012review,
	abstract = {Multi-step ahead forecasting is still an open challenge in time series forecasting. Several approaches
that deal with this complex problem have been proposed in the literature but an extensive comparison on a large number of tasks is still missing. This paper aims to fill this gap by reviewing existing
strategies for multi-step ahead forecasting and comparing them in theoretical and practical terms.
To attain such an objective, we performed a large scale comparison of these different strategies using a large experimental benchmark (namely the 111 series from the NN5 forecasting competition).
In addition, we considered the effects of deseasonalization, input variable selection, and forecast
combination on these strategies and on multi-step ahead forecasting at large. The following three
findings appear to be consistently supported by the experimental results: Multiple-Output strategies are the best performing approaches, deseasonalization leads to uniformly improved forecast
accuracy, and input selection is more effective when performed in conjunction with deseasonalization.},
	annote = {Discusses the issue of multi-step ahead forecasting

Needs to select model and introduces 3 different methodologies
i) best model on a test set
ii) average all models
iii) weighted average of all models with weight given by the inverse of the test set error

4.2.2. Forecasting performance evaluation
uses Friedman + post-hoc

},
	author = {Taieb, Souhaib Ben and Bontempi, Gianluca and Atiya, Amir F and Sorjamaa, Antti},
	date-added = {2020-12-09 22:20:06 -0500},
	date-modified = {2020-12-09 22:48:18 -0500},
	journal = {Expert systems with applications},
	keywords = {forecasting},
	number = {8},
	pages = {7067--7083},
	publisher = {Elsevier},
	read = {1},
	title = {A review and comparison of strategies for multi-step ahead time series forecasting based on the NN5 forecasting competition},
	volume = {39},
	year = {2012},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1108.3259.pdf}}

@article{d2020underspecification,
	abstract = {ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains.
We identify underspecification as a key reason for these failures. An ML pipeline is underspecified
when it can return many predictors with equivalently strong held-out performance in the training
domain. Underspecification is common in modern ML pipelines, such as those based on deep
learning. Predictors returned by underspecified pipelines are often treated as equivalent based
on their training domain performance, but we show here that such predictors can behave very
differently in deployment domains. This ambiguity can lead to instability and poor model behavior
in practice, and is a distinct failure mode from previously identified issues arising from structural
mismatch between training and deployment domains. We show that this problem appears in a
wide variety of practical ML pipelines, using examples from computer vision, medical imaging,
natural language processing, clinical risk prediction based on electronic health records, and medical
genomics. Our results show the need to explicitly account for underspecification in modeling
pipelines that are intended for real-world deployment in any domain.},
	author = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D and others},
	date-added = {2020-12-07 09:29:35 -0500},
	date-modified = {2020-12-07 09:30:33 -0500},
	journal = {arXiv preprint arXiv:2011.03395},
	keywords = {xai, concept drift, **MUST-READ**},
	title = {Underspecification Presents Challenges for Credibility in Modern Machine Learning},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2011.03395.pdf}}

@article{li2020fourier,
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	date-added = {2020-12-07 09:25:37 -0500},
	date-modified = {2020-12-07 09:26:03 -0500},
	journal = {arXiv preprint arXiv:2010.08895},
	keywords = {pde, deep learning, **MUST-READ**},
	title = {Fourier Neural Operator for Parametric Partial Differential Equations},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2010.08895.pdf}}

@article{tashman2000out,
	abstract = {In evaluations of forecasting accuracy, including forecasting competitions, researchers have paid attention to the selection
of time series and to the appropriateness of forecast-error measures. However, they have not formally analyzed choices in the implementation of out-of-sample tests, making it difficult to replicate and compare forecasting accuracy studies. In this
paper, I (1) explain the structure of out-of-sample tests, (2) provide guidelines for implementing these tests, and (3) evaluate
the adequacy of out-of-sample tests in forecasting software. The issues examined include series-splitting rules, fixed versus
rolling origins, updating versus recalibration of model coefficients, fixed versus rolling windows, single versus multiple test
periods, diversification through multiple time series, and design characteristics of forecasting competitions. For individual
time series, the efficiency and reliability of out-of-sample tests can be improved by employing rolling-origin evaluations,
recalibrating coefficients, and using multiple test periods. The results of forecasting competitions would be more
generalizable if based upon precisely described groups of time series, in which the series are homogeneous within group and heterogeneous between groups. Few forecasting software programs adequately implement out-of-sample evaluations,
especially general statistical packages and spreadsheet add-ins},
	author = {Tashman, Leonard J},
	date-added = {2020-12-07 09:24:14 -0500},
	date-modified = {2020-12-09 22:16:32 -0500},
	journal = {International journal of forecasting},
	keywords = {model selection, forecasting},
	number = {4},
	pages = {437--450},
	publisher = {Elsevier},
	read = {1},
	title = {Out-of-sample tests of forecasting accuracy: an analysis and review},
	volume = {16},
	year = {2000},
	Bdsk-Url-1 = {https://www.researchgate.net/profile/Len_Tashman/publication/247087596_Out-of_sample_tests_of_forecasting_accuracy_a_tutorial_and_review/links/5745ceec08ae9f741b430de3.pdf}}

@article{paleyes2020challenges,
	abstract = {In recent years, machine learning has received increased interest both as an academic research field and as a solution for real-world business problems. However,
the deployment of machine learning models in production systems can present a
number of issues and concerns. This survey reviews published reports of deploying machine learning solutions in a variety of use cases, industries and applications
and extracts practical considerations corresponding to stages of the machine learning deployment workflow. Our survey shows that practitioners face challenges at
each stage of the deployment. The goal of this paper is to layout a research agenda
to explore approaches addressing these challenges.},
	author = {Paleyes, Andrei and Urma, Raoul-Gabriel and Lawrence, Neil D},
	date-added = {2020-12-07 09:23:30 -0500},
	date-modified = {2020-12-14 23:50:21 -0500},
	journal = {arXiv preprint arXiv:2011.09926},
	keywords = {machine learning,},
	title = {Challenges in Deploying Machine Learning: a Survey of Case Studies},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2011.09926.pdf}}

@article{fridovich2020approximately,
	author = {Fridovich-Keil, Sara and Recht, Benjamin},
	date-added = {2020-12-03 21:14:09 -0500},
	date-modified = {2020-12-03 21:14:19 -0500},
	journal = {arXiv preprint arXiv:2011.04721},
	keywords = {optimization, line search},
	title = {Approximately Exact Line Search},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2011.04721.pdf}}

@article{gower2020variance,
	author = {Gower, Robert M and Schmidt, Mark and Bach, Francis and Richtarik, Peter},
	date-added = {2020-12-03 21:06:38 -0500},
	date-modified = {2020-12-03 21:06:58 -0500},
	journal = {Proceedings of the IEEE},
	keywords = {optimization, machine learning},
	number = {11},
	pages = {1968--1983},
	publisher = {IEEE},
	title = {Variance-reduced methods for machine learning},
	volume = {108},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2010.00892.pdf}}

@article{raj2020causal,
	author = {Raj, Anant and Bauer, Stefan and Soleymani, Ashkan and Besserve, Michel and Sch{\"o}lkopf, Bernhard},
	date-added = {2020-12-03 20:54:16 -0500},
	date-modified = {2020-12-03 20:55:00 -0500},
	journal = {arXiv preprint arXiv:2007.02938},
	keywords = {causality, feature selection},
	title = {Causal feature selection via orthogonal search},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2007.02938.pdf}}

@article{pizarro2002multiple,
	author = {Pizarro, Joaqu{\i}n and Guerrero, Elisa and Galindo, Pedro L},
	date-added = {2020-11-27 16:48:07 -0500},
	date-modified = {2020-11-27 16:48:27 -0500},
	journal = {Neurocomputing},
	keywords = {model selection, multiple comparison, deep learning},
	number = {1-4},
	pages = {155--173},
	publisher = {Elsevier},
	title = {Multiple comparison procedures applied to model selection},
	volume = {48},
	year = {2002},
	Bdsk-Url-1 = {https://sci2s.ugr.es/keel/pdf/specific/articulo/PizarroGuerreroGalindo.pdf}}

@article{levesque2018bayesian,
	annote = {	CHAPTER 1. Introduction

``With an equal number of observations, grid search is easily outperformed by a random search (Bergstra and Bengio, 2012)''

``Hyperparameter optimization is also gradually enabling the emergent field of automatic machine learning (AutoML). AutoML is the automation of the creation and configuration of machine learning pipelines, including the preprocessing and transformation of data, the selection of a machine learning algorithm and the tuning of all hyperparameters involved''

``Objective 1: Assess the impact of overfitting in hyperparameter optimization and propose strategies to reduce it.''
``Given the presence of overfitting in the optimization of hyperparameters, we will then propose some strategies to obtain better estimates of the generalization performance of hyperparameters.''

``Objective 2: Propose an application of hyperparameter optimization to generate heterogeneous ensembles of classifiers.''

``Objective 3: Propose methods for the optimization of hyperparameters with conditionality structures.''

	CHAPTER 2. Bayesian Hyperparameter Optimization

BO is a derivative-free optimization method that builds an approximation of the function it tries to minimize. Typically relies on Gaussian Processes or Random Forests. But the uncertainty estimation is a key element of BO and RF has been shown to underestimate the uncertainty. So they focus on GP in this thesis.

	2.1 Gaussian Processes
Nice intro to GP

	2.2 Acquisition Function
An acquisition function estimates the gain in variance reduction around a certain point. So basically, where should you probe next in order to reduce what you don't know the most.

	2.3 Hyperparameter Optimization
Introduces the bilevel optimization problem of hp optimization. Hp optim is calculated on the validation set
Shows pseudo-code of hp optimization using BO
The validation step can also be done via cross-validation. That is the evalution of the performance of the trained model out-of-sample can be done via cv and doesn't have to be done on a single validation set.

	2.4 Surrogate Model Hyperparameters
Discusses hp of the BO model

	2.5 Related Works

	2.6 Hyperparameter Optimization Examples

	CHAPTER 3 Evaluation of Generalization Performance in Hyperparameter Optimization

	3.2 Strategies to Limit Overfitting
	3.2.1 Reshuffling
	3.2.2 Selection split

},
	author = {L{\'e}vesque, Julien-Charles},
	date-added = {2020-11-27 16:34:01 -0500},
	date-modified = {2020-12-10 21:43:24 -0500},
	keywords = {bayesian, hyperparameter, model selection},
	title = {Bayesian hyperparameter optimization: overfitting, ensembles and conditional spaces},
	year = {2018},
	Bdsk-Url-1 = {https://corpus.ulaval.ca/jspui/bitstream/20.500.11794/28364/1/33862.pdf}}

@article{gneiting2007strictly,
	abstract = {Scoring rules assess the quality of probabilistic forecasts, by assigning a numerical score based on the predictive distribution and on the
event or value that materializes. A scoring rule is proper if the forecaster maximizes the expected score for an observation drawn from the
distribution F if he or she issues the probabilistic forecast F, rather than G = F. It is strictly proper if the maximum is unique. In prediction
problems, proper scoring rules encourage the forecaster to make careful assessments and to be honest. In estimation problems, strictly proper
scoring rules provide attractive loss and utility functions that can be tailored to the problem at hand. This article reviews and develops the
theory of proper scoring rules on general probability spaces, and proposes and discusses examples thereof. Proper scoring rules derive from
convex functions and relate to information measures, entropy functions, and Bregman divergences. In the case of categorical variables, we
prove a rigorous version of the Savage representation. Examples of scoring rules for probabilistic forecasts in the form of predictive densities
include the logarithmic, spherical, pseudospherical, and quadratic scores. The continuous ranked probability score applies to probabilistic
forecasts that take the form of predictive cumulative distribution functions. It generalizes the absolute error and forms a special case of
a new and very general type of score, the energy score. Like many other scoring rules, the energy score admits a kernel representation
in terms of negative definite functions, with links to inequalities of Hoeffding type, in both univariate and multivariate settings. Proper
scoring rules for quantile and interval forecasts are also discussed. We relate proper scoring rules to Bayes factors and to cross-validation,
and propose a novel form of cross-validation known as random-fold cross-validation. A case study on probabilistic weather forecasts in
the North American Pacific Northwest illustrates the importance of propriety. We note optimum score approaches to point and quantile
estimation, and propose the intuitively appealing interval score as a utility function in interval estimation that addresses width as well as
coverage},
	author = {Gneiting, Tilmann and Raftery, Adrian E},
	date-added = {2020-10-30 12:42:59 -0400},
	date-modified = {2020-10-30 12:43:33 -0400},
	journal = {Journal of the American statistical Association},
	keywords = {distributional forecast, uncertainty,},
	number = {477},
	pages = {359--378},
	publisher = {Taylor \& Francis},
	title = {Strictly proper scoring rules, prediction, and estimation},
	volume = {102},
	year = {2007},
	Bdsk-Url-1 = {http://www.eecs.harvard.edu/cs286r/courses/fall10/papers/Gneiting07.pdf}}

@incollection{feurer2019hyperparameter,
	author = {Feurer, Matthias and Hutter, Frank},
	booktitle = {Automated Machine Learning},
	date-added = {2020-10-28 10:38:24 -0400},
	date-modified = {2020-10-28 10:38:38 -0400},
	keywords = {hyperparameter, overfitting},
	pages = {3--33},
	publisher = {Springer, Cham},
	title = {Hyperparameter optimization},
	year = {2019},
	Bdsk-Url-1 = {https://library.oapen.org/bitstream/handle/20.500.12657/23012/1007149.pdf?sequence=1#page=15}}

@article{blum2015ladder,
	abstract = {The organizer of a machine learning competition faces the problem of maintaining
an accurate leaderboard that faithfully represents the quality of the best submission of
each competing team. What makes this estimation problem particularly challenging is its
sequential and adaptive nature. As participants are allowed to repeatedly evaluate their
submissions on the leaderboard, they may begin to overfit to the holdout data that supports
the leaderboard. Few theoretical results give actionable advice on how to design a reliable
leaderboard. Existing approaches therefore often resort to poorly understood heuristics such
as limiting the bit precision of answers and the rate of re-submission.
In this work, we introduce a notion of leaderboard accuracy tailored to the format of a
competition. We introduce a natural algorithm called the Ladder and demonstrate that it simultaneously supports strong theoretical guarantees in a fully adaptive model of estimation,
withstands practical adversarial attacks, and achieves high utility on real submission files
from an actual competition hosted by Kaggle.
Notably, we are able to sidestep a powerful recent hardness result for adaptive risk
estimation that rules out algorithms such as ours under a seemingly very similar notion
of accuracy. On a practical note, we provide a completely parameter-free variant of our
algorithm that can be deployed in a real competition with no tuning required whatsoever.},
	annote = {The authors propose a simple method that seems to prevent over-fitting on the hold-out set, and provides better consistency between public and private leaderboard. Their method applies to the case of sequential, repeated submissions, when each model is submitted one after the other.
In their parameter-free approach, they test whether the loss of the new model is statistically significantly lower than the current best loss (one-sided paired t-test). And if it is, they update their best loss, and if not, they keep the old one and don't publish the new loss.
? I'm surprised how this can make a difference? Or maybe this doesn't apply to hp optimization where you run you search in parallel.},
	author = {Blum, Avrim and Hardt, Moritz},
	date-added = {2020-10-27 22:39:17 -0400},
	date-modified = {2020-10-27 22:43:14 -0400},
	journal = {arXiv preprint arXiv:1502.04585},
	keywords = {overfitting, model selection},
	read = {1},
	title = {The ladder: A reliable leaderboard for machine learning competitions},
	year = {2015},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1502.04585.pdf}}

@article{freedman1983note,
	author = {Freedman, David A and Freedman, David A},
	date-added = {2020-10-27 16:26:28 -0400},
	date-modified = {2020-10-27 16:26:43 -0400},
	journal = {the american statistician},
	keywords = {model selection, overfitting},
	number = {2},
	pages = {152--155},
	publisher = {Taylor \& Francis Group},
	title = {A note on screening regression equations},
	volume = {37},
	year = {1983},
	Bdsk-Url-1 = {http://pluto.mscc.huji.ac.il/~mszucker/REGR/freedman.pdf}}

@inproceedings{rao2008dangers,
	author = {Rao, R Bharat and Fung, Glenn and Rosales, Romer},
	booktitle = {Proceedings of the 2008 SIAM international conference on data mining},
	date-added = {2020-10-27 16:13:56 -0400},
	date-modified = {2020-10-27 16:14:10 -0400},
	keywords = {cross validation, overfitting},
	organization = {SIAM},
	pages = {588--596},
	title = {On the dangers of cross-validation. An experimental evaluation},
	year = {2008},
	Bdsk-Url-1 = {http://people.csail.mit.edu/romer/papers/CrossVal_SDM08.pdf}}

@article{reunanen2003overfitting,
	author = {Reunanen, Juha},
	date-added = {2020-10-27 16:12:51 -0400},
	date-modified = {2020-10-27 22:46:35 -0400},
	journal = {Journal of Machine Learning Research},
	keywords = {overfitting, model selection},
	number = {Mar},
	pages = {1371--1382},
	title = {Overfitting in making comparisons between variable selection methods},
	volume = {3},
	year = {2003},
	Bdsk-Url-1 = {https://www.jmlr.org/papers/volume3/reunanen03a/reunanen03a.pdf}}

@article{dwork2015reusable,
	abstract = {Misapplication of statistical data analysis is a common cause of spurious discoveries in
scientific research. Existing approaches to ensuring the validity of inferences drawn from data
assume a fixed procedure to be performed, selected before the data are examined. In common
practice, however, data analysis is an intrinsically adaptive process, with new analyses
generated on the basis of data exploration, as well as the results of previous analyses on the
same data. We demonstrate a new approach for addressing the challenges of adaptivity based
on insights from privacy-preserving data analysis. As an application, we show how to safely
reuse a holdout data set many times to validate the results of adaptively chosen analyses.},
	annote = {Main paper:

``To preserve statistical validity, the only known safe approach is to collect new data for a fresh holdout set. This conservative approach is very costly and thus is frequently abused, resulting in overfitting to the holdout set''
? Does it only work if all data generated from same distribution?
? Does it only work with training and hold-out sets of the same size?

The main paper is pretty short and therefore light on details. But they claim they solve the problem of multiple testing by computing a certain functional in a way that allow multiple hypothesis testing. They take the example of variable selection where they sequentially select the variables to use by taking the variable the most correlated with the target. Their mechanism is called Thresholdout and includes a first step that compares the value of the statistic on the training and validation set. => This raises again the question of whether this could work when the data-generating process is different between the training and validation sets (hold-out set).

Appendix (or in this case, core of the paper):

Setting is really ``adaptive data analysis'', that is, where ``datasets are used repeatedly, with previous analyses informing subsequent
analyses'' (http://people.seas.harvard.edu/~madhusudan/courses/Spring2016/notes/thomas-notes-ada.pdf).
? Does that fit the context of hyperparameter optimization?

Always assume all data drawn from the same, fixed distribution (see Theorem 11). Plus their Thresholdout algo relies on a test of difference between the quantity evaluated on the training set and on the holdout set. 
? I don't see how that would work with non-stationary data.

Also works with Gaussian noise (instead of Laplacian noise).
},
	author = {Dwork, Cynthia and Feldman, Vitaly and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Roth, Aaron},
	date-added = {2020-10-27 14:54:07 -0400},
	date-modified = {2020-10-27 22:59:24 -0400},
	journal = {Science},
	keywords = {overfitting, backtesting},
	number = {6248},
	pages = {636--638},
	publisher = {American Association for the Advancement of Science},
	read = {1},
	title = {The reusable holdout: Preserving validity in adaptive data analysis},
	volume = {349},
	year = {2015},
	Bdsk-Url-1 = {http://nematilab.info/bmijc/assets/091218_paper.pdf},
	Bdsk-Url-2 = {https://science.sciencemag.org/content/sci/suppl/2015/08/05/349.6248.636.DC1/Dwork.SM.pdf}}

@article{he2020bayesian,
	author = {He, Bobby and Lakshminarayanan, Balaji and Teh, Yee Whye},
	date-added = {2020-10-27 09:26:34 -0400},
	date-modified = {2020-10-27 09:27:00 -0400},
	journal = {arXiv preprint arXiv:2007.05864},
	keywords = {deep learning, bayesian, uncertainty, ensemble,},
	title = {Bayesian Deep Ensembles via the Neural Tangent Kernel},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/abs/2007.05864}}

@article{gneiting2005weather,
	author = {Gneiting, Tilmann and Raftery, Adrian E},
	date-added = {2020-10-26 15:37:09 -0400},
	date-modified = {2020-10-26 15:37:31 -0400},
	journal = {Science},
	keywords = {forecasting, uncertainty, ensemble, distributional forecast},
	number = {5746},
	pages = {248--249},
	publisher = {American Association for the Advancement of Science},
	title = {Weather forecasting with ensemble methods},
	volume = {310},
	year = {2005},
	Bdsk-Url-1 = {https://sites.stat.washington.edu/raftery/Research/PDF/science2005.pdf}}

@article{kuleshov2018accurate,
	abstract = {Methods for reasoning under uncertainty are a
key building block of accurate and reliable machine learning systems. Bayesian methods provide a general framework to quantify uncertainty.
However, because of model misspecification and
the use of approximate inference, Bayesian uncertainty estimates are often inaccurate --- for
example, a 90% credible interval may not contain the true outcome 90% of the time. Here, we
propose a simple procedure for calibrating any
regression algorithm; when applied to Bayesian
and probabilistic models, it is guaranteed to
produce calibrated uncertainty estimates given
enough data. Our procedure is inspired by Platt
scaling and extends previous work on classification. We evaluate this approach on Bayesian linear regression, feedforward, and recurrent neural networks, and find that it consistently outputs
well-calibrated credible intervals while improving performance on time series forecasting and
model-based reinforcement learning tasks.},
	annote = {1. Introduction
Authors propose a new calibration methods for Bayesian neural networks, inspired by Platt scaling. They claim that given a sufficient amount of idd data, the calibration will be perfect.
Not sure if it matters, but the authors specifically focus on solving the miscalibration of credible intervals (Bayesian). Could they generate well-calibrated credible intervals, but their predictive distribution still be incorrect?

2. Calibrated Classification
2.1 Calibration
In section 2, they review calibration using binary classification.
They characterize a distributional output by its cdf only. They define a distributional forecast as a map that takes covariates as input and return the cdf of the predictive distribution.
Distinguish calibration vs sharpness. Sharp forecasts have probabilities close to 0 or 1 (binary classification). That is, it very confident about its predictions. ``Note that an ideal forecaster is both calibrated and predicts outcomes with 100% confidence''. => It sounds a bit weird, almost like anti-probablistic. Maybe they're talking about a context where there is no aleatoric uncertainty. Which would makes sense as they focus exclusively on Bayesian networks; so models that capture epistmic uncertainty.
2.2 Training calibrated classifiers
For binary classification, the evaluation is done via calibration curves (sometimes referred to as reliability diagram): by binning the predictions (i.e., H(x) \in [0,0.1],{\ldots}), the Mean Predicted Value, then the observed frequency against the predicted frequency (ideal is a straight line). They also discuss sharpness (predicted probabilities should be close to 0 or 1, instead of 0.5). => I'm surprised that they don't mention anything else (e.g., ECE, MCE). Although, their metrics are purely visual; there is nothing quantitative.

3. Calibrated Regression
3.1 Calibration
Define calibration for a regression, intuitively, via the confidence interval. Formally, they use the inverse quantile function to say that the frequency at which the true value should fall below a certain quantile should be equal to that quantile (in the limit of infinite data).
Refer to ``Probabilistic forecasts, calibration and sharpness'' (Gneiting et al, 2007) for probabilistic definition -> Ref paper.
Calibration vs Sharpness: sharpness for regression is defined as tight confidence intervalls, or small variance for the predictive distribution. => Once again, this is pretty hand-wavy.
3.2 Training calibrated regression models
Similar to Platt scaling, you train a recalibration model (R) such that after being combined with the forecasting model (H), you get perfect calibration: R(H(x)). R: [0,1]->[0,1] is a model trained on a calibration dataset. That calibration dataset is made of (F, F_hat) where F is the value of the predicted cdf evaluated at the true value y_t, and F_hat is the actual frequency at which F(y_t) fell bellow the value F for all points y_t. => So R is taking the model output (F_x(y_t), or H(x_t)(y_t)) and return the actual frequency. One comment is that this approach doesn't modify H(x_t) directly, that is the predictive distribution, but only H(x_t)(y_t). Which seems a bit limiting.
The intuition behind that recalibration is simply to move the predicted quantiles so that they match the oberved quantiles. => I doubt this could give decent results in low-data regime, which is most of the situations in forecasting.
=> Definition of perfect R: R(p) = P[Y <= F_X^{-1}(p)] valid since then P[Y <= (R o F_X)^{-1}(p)] = P[F_x(Y) <= R^{-1}(p)] = R(R^{-1}(p)) = p
Authors recommend to use isotonic regression for R (-> monotonically increasing function, typically piece-wise constant; non-parametric regression). 
Scaling should be trained on hold-out dataset. Or authors propose to train with k-fold (or sequential validation), then at predict time, use an average of all models. => That's an interesting idea for backtesting; why only using the most recent fold. We could instead use all models, maybe with varying weights.
3.5 Diagnostic tools
Calibration: pick a number of quantiles, then calculate frequency of observed values below each quantile, and on a graph. Similar to the classificaiton case, ideal calibration would return a straight line.
The authors propose a quantiative metric this time, taking the weights MSE between targeted quantiles and observed frequency: Sum_i w_i (q_i - q_i_hat)^2. Weights are 1 in the paper, but one could use weights given by the number of data falling into each quantile. => This resembles a bit the Expected Calibration Error (ECE), except that ECE takes the absolute error and here we use the squared difference.
Sharpness: They define sharpness as the mean of the variances of all predictive distributions.
},
	author = {Kuleshov, Volodymyr and Fenner, Nathan and Ermon, Stefano},
	date-added = {2020-10-26 10:27:58 -0400},
	date-modified = {2020-10-26 15:29:37 -0400},
	journal = {arXiv preprint arXiv:1807.00263},
	keywords = {uncertainty, deep learning, forecasting, reinforcement learning},
	read = {1},
	title = {Accurate uncertainties for deep learning using calibrated regression},
	year = {2018},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1807.00263.pdf}}

@article{cawley2007preventing,
	author = {Cawley, Gavin C and Talbot, Nicola LC},
	date-added = {2020-10-17 01:15:34 -0400},
	date-modified = {2020-10-17 01:15:56 -0400},
	journal = {Journal of Machine Learning Research},
	keywords = {model selection, overfitting, hyperparameter, bayesian, regularization},
	number = {Apr},
	pages = {841--861},
	title = {Preventing over-fitting during model selection via Bayesian regularisation of the hyper-parameters},
	volume = {8},
	year = {2007},
	Bdsk-Url-1 = {https://www.jmlr.org/papers/volume8/cawley07a/cawley07a.pdf}}

@article{benjamini1995controlling,
	author = {Benjamini, Yoav and Hochberg, Yosef},
	date-added = {2020-10-08 23:50:34 -0400},
	date-modified = {2020-11-27 16:11:22 -0500},
	journal = {Journal of the Royal statistical society: series B (Methodological)},
	keywords = {statistical test, multiple comparison, false discovery rate, p-value},
	number = {1},
	pages = {289--300},
	publisher = {Wiley Online Library},
	title = {Controlling the false discovery rate: a practical and powerful approach to multiple testing},
	volume = {57},
	year = {1995},
	Bdsk-Url-1 = {https://www.stat.purdue.edu/~doerge/BIOINFORM.D/FALL06/Benjamini%20and%20Y%20FDR.pdf}}

@inproceedings{salinas2019high,
	author = {Salinas, David and Bohlke-Schneider, Michael and Callot, Laurent and Medico, Roberto and Gasthaus, Jan},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2020-10-05 15:57:38 -0400},
	date-modified = {2020-10-05 15:58:02 -0400},
	keywords = {forecasting, uncertainty},
	pages = {6827--6837},
	title = {High-dimensional multivariate forecasting with low-rank Gaussian Copula Processes},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1910.03002.pdf}}

@article{tay2020efficient,
	author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
	date-added = {2020-10-05 10:54:01 -0400},
	date-modified = {2020-10-05 10:54:19 -0400},
	journal = {arXiv preprint arXiv:2009.06732},
	keywords = {transformer, nlp},
	title = {Efficient Transformers: A Survey},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2009.06732.pdf}}

@article{gonccalves190forecasting,
	author = {Gon{\c{c}}alves, Carla and Cavalcante, Laura and Brito, Margarida and Bessa, Ricardo J and Gama, Jo{\~a}o},
	date-added = {2020-10-02 09:53:10 -0400},
	date-modified = {2020-10-02 09:53:31 -0400},
	journal = {Electric Power Systems Research},
	keywords = {forecasting, quantile, extreme value theory},
	pages = {106636},
	publisher = {Elsevier},
	title = {Forecasting conditional extreme quantiles for wind energy},
	volume = {190},
	Bdsk-Url-1 = {https://www.smart4res.eu/wp-content/uploads/2020/07/PSCC_INESC_Gon%C3%A7alves.pdf}}

@article{cauchois2020robust,
	author = {Cauchois, Maxime and Gupta, Suyash and Ali, Alnur and Duchi, John C},
	date-added = {2020-10-02 09:50:07 -0400},
	date-modified = {2020-10-02 09:50:32 -0400},
	journal = {arXiv preprint arXiv:2008.04267},
	keywords = {concept drift, uncertainty},
	title = {Robust Validation: Confident Predictions Even When Distributions Shift},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2008.04267.pdf}}

@inproceedings{bengio2009curriculum,
	author = {Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
	booktitle = {Proceedings of the 26th annual international conference on machine learning},
	date-added = {2020-10-01 22:15:20 -0400},
	date-modified = {2020-10-01 22:15:39 -0400},
	keywords = {deep learning},
	pages = {41--48},
	title = {Curriculum learning},
	year = {2009},
	Bdsk-Url-1 = {https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/15972/Bengio%2C%202009%20Curriculum%20Learning.pdf?sequence=1&isAllowed=y}}

@article{rosenfeld2018incremental,
	author = {Rosenfeld, Amir and Tsotsos, John K},
	date-added = {2020-10-01 22:14:33 -0400},
	date-modified = {2020-10-01 22:14:54 -0400},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	keywords = {incremental learning, deep learning},
	publisher = {IEEE},
	title = {Incremental learning through deep adaptation},
	year = {2018},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1705.04228.pdf}}

@inproceedings{meng2020fast,
	author = {Meng, Si Yi and Vaswani, Sharan and Laradji, Issam Hadj and Schmidt, Mark and Lacoste-Julien, Simon},
	booktitle = {International Conference on Artificial Intelligence and Statistics},
	date-added = {2020-09-28 17:23:54 -0400},
	date-modified = {2020-09-28 17:24:13 -0400},
	keywords = {optimization},
	organization = {PMLR},
	pages = {1375--1386},
	title = {Fast and furious convergence: Stochastic second order methods under interpolation},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1910.04920.pdf}}

@inproceedings{vaswani2019painless,
	author = {Vaswani, Sharan and Mishkin, Aaron and Laradji, Issam and Schmidt, Mark and Gidel, Gauthier and Lacoste-Julien, Simon},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2020-09-28 17:21:17 -0400},
	date-modified = {2020-09-28 17:21:56 -0400},
	keywords = {optimization},
	pages = {3732--3745},
	title = {Painless stochastic gradient: Interpolation, line-search, and convergence rates},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1905.09997.pdf}}

@article{weinan2020neural,
	author = {Weinan, E, et al.},
	date-added = {2020-09-24 23:31:21 -0400},
	date-modified = {2020-09-24 23:32:49 -0400},
	keywords = {deep learning, machine learning},
	title = {Towards a Mathematical Understanding of Neural Network-Based Machine Learning: what we know and what we don't},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2009.10713v1.pdf}}

@book{gelman2013bayesian,
	author = {Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson, David B and Vehtari, Aki and Rubin, Donald B},
	date-added = {2020-09-24 15:32:46 -0400},
	date-modified = {2020-09-24 15:34:31 -0400},
	keywords = {bayesian},
	publisher = {CRC press},
	title = {Bayesian data analysis},
	year = {2013},
	Bdsk-Url-1 = {http://www.stat.columbia.edu/~gelman/book/BDA3.pdf}}

@article{murphy2007conjugate,
	author = {Murphy, Kevin P},
	date-added = {2020-09-24 15:31:44 -0400},
	date-modified = {2020-09-24 15:32:19 -0400},
	journal = {def},
	keywords = {bayesian, class notes},
	pages = {16},
	title = {Conjugate Bayesian analysis of the Gaussian distribution},
	volume = {1},
	year = {2007},
	Bdsk-Url-1 = {https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf}}

@article{hogan2020knowledge,
	abstract = {In this paper we provide a comprehensive introduction to knowledge graphs, which have recently garnered
significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic,
large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data
models and query languages that are used for knowledge graphs. We discuss the roles of schema, identity, and
context in knowledge graphs. We explain how knowledge can be represented and extracted using a combination
of deductive and inductive techniques. We summarise methods for the creation, enrichment, quality assessment,
refinement, and publication of knowledge graphs. We provide an overview of prominent open knowledge
graphs and enterprise knowledge graphs, their applications, and how they use the aforementioned techniques.
We conclude with high-level future research directions for knowledge graphs},
	author = {Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and d'Amato, Claudia and de Melo, Gerard and Gutierrez, Claudio and Gayo, Jos{\'e} Emilio Labra and Kirrane, Sabrina and Neumaier, Sebastian and Polleres, Axel and others},
	date-added = {2020-09-02 14:17:06 -0400},
	date-modified = {2020-09-02 14:17:38 -0400},
	journal = {arXiv preprint arXiv:2003.02320},
	keywords = {knowledge graphs, database},
	title = {Knowledge graphs},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2003.02320.pdf}}

@article{sandve2013ten,
	author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
	date-added = {2020-08-28 15:26:12 -0400},
	date-modified = {2020-08-28 15:26:35 -0400},
	journal = {PLoS Comput Biol},
	keywords = {reproducibility, scientific computing},
	number = {10},
	pages = {e1003285},
	publisher = {Public Library of Science},
	title = {Ten simple rules for reproducible computational research},
	volume = {9},
	year = {2013},
	Bdsk-Url-1 = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003285}}

@article{wilson2014best,
	author = {Wilson, Greg and Aruliah, Dhavide A and Brown, C Titus and Hong, Neil P Chue and Davis, Matt and Guy, Richard T and Haddock, Steven HD and Huff, Kathryn D and Mitchell, Ian M and Plumbley, Mark D and others},
	date-added = {2020-08-28 15:17:36 -0400},
	date-modified = {2020-08-28 15:18:24 -0400},
	journal = {PLoS Biol},
	keywords = {software engineering, scientific computing,},
	number = {1},
	pages = {e1001745},
	publisher = {Public Library of Science},
	title = {Best practices for scientific computing},
	volume = {12},
	year = {2014},
	Bdsk-Url-1 = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1001745}}

@article{arjovsky2019invariant,
	author = {Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and Lopez-Paz, David},
	date-added = {2020-08-27 14:01:20 -0400},
	date-modified = {2020-10-27 22:45:23 -0400},
	journal = {arXiv preprint arXiv:1907.02893},
	keywords = {optimization},
	title = {Invariant risk minimization},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1907.02893.pdf}}

@misc{spiliotis2020hierarchical,
	abstract = {Hierarchical forecasting methods have been widely used to
support aligned decision-making by providing coherent forecasts at different aggregation levels. Traditional hierarchical forecasting approaches,
such as the bottom-up and top-down methods, focus on a particular
aggregation level to anchor the forecasts. During the past decades, these
have been replaced by a variety of linear combination approaches that
exploit information from the complete hierarchy to produce more accurate
forecasts. However, the performance of these combination methods depends on the particularities of the examined series and their relationships.
This paper proposes a novel hierarchical forecasting approach based on
machine learning that deals with these limitations in three important
ways. First, the proposed method allows for a non-linear combination of
the base forecasts, thus being more general than the linear approaches.
Second, it structurally combines the objectives of improved post-sample
empirical forecasting accuracy and coherence. Finally, due to its non-linear
nature, our approach selectively combines the base forecasts in a direct
and automated way without requiring that the complete information
must be used for producing reconciled forecasts for each series and level.
The proposed method is evaluated both in terms of accuracy and bias
using two different data sets coming from the tourism and retail industries. Our results suggest that the proposed method gives superior point
forecasts than existing approaches, especially when the series comprising
the hierarchy are not characterized by the same patterns.},
	archiveprefix = {arXiv},
	author = {Evangelos Spiliotis and Mahdi Abolghasemi and Rob J Hyndman and Fotios Petropoulos and Vassilios Assimakopoulos},
	date-added = {2020-08-21 22:51:56 -0400},
	date-modified = {2020-08-21 22:52:36 -0400},
	eprint = {2006.02043},
	keywords = {forecasting, hierarchical, reconciliation, machine learning},
	primaryclass = {cs.LG},
	title = {Hierarchical forecast reconciliation with machine learning},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2006.02043.pdf}}

@article{cerqueira2019evaluating,
	abstract = {Performance estimation aims at estimating the loss that a predictive
model will incur on unseen data. These procedures are part of the pipeline
in every machine learning project and are used for assessing the overall
generalisation ability of predictive models. In this paper we address the
application of these methods to time series forecasting tasks. For independent and identically distributed data the most common approach is
cross-validation. However, the dependency among observations in time
series raises some caveats about the most appropriate way to estimate performance in this type of data and currently there is no settled way to do
so. We compare different variants of cross-validation and of out-of-sample
approaches using two case studies: One with 62 real-world time series and
another with three synthetic time series. Results show noticeable differences in the performance estimation methods in the two scenarios. In
particular, empirical experiments suggest that cross-validation approaches
can be applied to stationary time series. However, in real-world scenarios,
when different sources of non-stationary variation are at play, the most
accurate estimates are produced by out-of-sample methods that preserve
the temporal order of observations.},
	annote = {Abstract:
``In particular, empirical experiments suggest that cross-validation approaches can be applied to stationary time series. However, in real-world scenarios, when different sources of non-stationary variation are at play, the most accurate estimates are produced by out-of-sample methods that preserve the temporal order of observations.''

Introduction: 
``cross-validation is typically the most appropriate method. This is mainly due to its efficient use of data. However, there are some issues when the observations in the data are dependent, such as time series. These dependencies raise some caveats about using standard cross-validation in such data.''

In the Hyndman article, they use cross-validation and obtain good results, except when the model is under-specified. But they were using very littel data (200 points). In that case, you pretty have no other option. Another scenario when that would make sense is if the data is stationary.

The authors present the different data split approach amenable for time series problem: OOS, prequential, cross-validation. They discuss the usefulness of cross-validation: stationary time-series and/or small datasets.

Section 3:
Describe the different data used: real, and synthetic (identical to the ones used in Hyndman's paper)
Compare different data split strategies in estimating the test set error

Conclusion:
``Consequently, we believe that in these scenarios, Rep-Holdout is a better option as performance estimation method relative to cross-validation approaches.''
``Rep-Holdout OOS tested in nreps testing periods with a Monte Carlo simulation using 70% of the total observations t of the time series in each test. For each period, a random point is picked from the time series. The previous window comprising 60% of t is used for training and the following window of 10% of t is used for testing;''

``In a stationary setting the cross-validation variants are shown to have a competitive estimation ability. However, when non-stationarities are present, they systematically provide worse estimations than the out-of-sample approaches''},
	author = {Cerqueira, Vitor and Torgo, Luis and Mozetic, Igor},
	date-added = {2020-08-07 09:33:42 -0400},
	date-modified = {2020-12-03 00:14:06 -0500},
	journal = {arXiv preprint arXiv:1905.11744},
	keywords = {time series, model selection, forecasting},
	read = {1},
	title = {Evaluating time series forecasting models: An empirical study on performance estimation methods},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1905.11744.pdf}}

@article{efron1986bootstrap,
	author = {Efron, Bradley and Tibshirani, Robert},
	date-added = {2020-08-07 07:56:22 -0400},
	date-modified = {2020-08-07 07:56:59 -0400},
	journal = {Statistical science},
	keywords = {bootstrap, statistical test, statistics, confidence interval},
	pages = {54--75},
	publisher = {JSTOR},
	title = {Bootstrap methods for standard errors, confidence intervals, and other measures of statistical accuracy},
	year = {1986},
	Bdsk-Url-1 = {https://projecteuclid.org/download/pdf_1/euclid.ss/1177013815}}

@article{bergmeir2018note,
	abstract = {One of the most widely used standard procedures for model evaluation in classification and regression is
K-fold cross-validation (CV). However, when it comes to time series forecasting, because of the inherent
serial correlation and potential non-stationarity of the data, its application is not straightforward and
often omitted by practitioners in favor of an out-of-sample (OOS) evaluation. In this paper, we show
that the particular setup in which time series forecasting is usually performed using Machine Learning
methods renders the use of standard K-fold CV possible. We present theoretical insights supporting
our arguments. Furthermore, we present a simulation study where we show empirically that K-fold CV
performs favorably compared to both OOS evaluation and other time-series-specific techniques such as
non-dependent cross-validation.},
	annote = {``Usually, practitioners resort to usual out-of-sample (OOS) evaluation instead, where a section from the end of the series is withheld for evaluation. However, in this way, the benefits of CV, especially for small datasets, cannot be exploited''

In that paper, they have no problem using training data that is in the future compared to test data, even overlapping data. In the method, they propose, they place all data in rows (last column = target, first p columns = lagged values), then in their cross-validation they randomly leave out some of the rows. So there is overlap between the training set and the test set. They also present a verion that does not overlap:
``Non-dependent cross-validation. The same folds are used as for the 5-fold CV, but rows are removed from the training set if they have a lag distance smaller than p from a row in the test set, where p is the maximal model order''


Metrics:
''We evaluate this by assessing the error between PE and PE, across all series involved in the experiments. We use a mean absolute error (MAE) to assess the size of the effect and call this measure ``mean absolute predictive accuracy error'' (MAPAE).'' 

``Furthermore, to see if any bias is present, we use the mean of the predictive accuracy error (MPAE)''


Models:
In their experiments, they use AR models, and MLP.


Data:
`` In the first two experiments, we generate data from stationary AR(3) processes and invertible MA(1) processes, respectively''.
``The purpose of use-case 1 with AR(3) processes is to illustrate how the methods perform when the true model or very similar models as the DGP are used for forecasting. Use-case 2 shows a situation in which the true model is not among the forecasting models, but the models can still reasonably well fit the data. This is the case for an MA process which can approximate an AR process with a large number of lags. In practice, usually a relatively low number of AR lags is sufficient to model such data''
``The third use-case can be seen as the construction of a counterexample, i.e., as a situation where the cross-validation procedures break down. We use a seasonal AR process as the DGP with a significant lag 12 (seasonal lag 1). As the models taken into account only use up to the first five lags, the models should not be able to fit well such data.''
``For each of the three use-cases, 1000 Monte Carlo trials are performed. Series are generated with a total length of 200 values, and we use 70% of the data (140 observations) as in-set, the rest (60 observations) is withheld as the out-set'' -> that's a small out-set (test set); probably some variance there.

Conclusion:
``In this work we have investigated the use of cross-validation procedures for time series prediction evaluation when purely autoregressive models are used, which is a very common use-case when using Machine Learning procedures for time series forecasting.''
''In a theoretical proof, we showed that a normal K-fold cross-validation procedure can be used if the lag structure of the models is adequately specified.''
''In the experiments, we showed empirically that even if the lag structure is not correct, as long as the data are fitted well by the model, cross-validation without any modification is a better choice than OOS evaluation. Only if the models are heavily misspecified, are the cross-validation procedures to be avoided as in such a case they may yield a systematic underestimation of the error''
-> works better if we know the correct model in advance},
	author = {Bergmeir, Christoph and Hyndman, Rob J and Koo, Bonsoo},
	date-added = {2020-08-06 00:25:19 -0400},
	date-modified = {2020-12-03 00:09:59 -0500},
	journal = {Computational Statistics \& Data Analysis},
	keywords = {model selection, cross validation, time series},
	pages = {70--83},
	publisher = {Elsevier},
	read = {1},
	title = {A note on the validity of cross-validation for evaluating time series prediction},
	volume = {120},
	year = {2018},
	Bdsk-Url-1 = {https://www.monash.edu/business/econometrics-and-business-statistics/research/publications/ebs/wp10-15.pdf}}

@inproceedings{dietterich2000ensemble,
	abstract = {Ensemble methods are learning algorithms that construct a
set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian
averaging, but more recent algorithms include error-correcting output
coding, Bagging, and boosting. This paper reviews these methods and
explains why ensembles can often perform better than any single classifier. Some previous studies comparing ensemble methods are reviewed,
and some new experiments are presented to uncover the reasons that
Adaboost does not overfit rapidly},
	author = {Dietterich, Thomas G},
	booktitle = {International workshop on multiple classifier systems},
	date-added = {2020-08-02 19:54:07 -0400},
	date-modified = {2020-08-02 19:54:44 -0400},
	keywords = {ensemble},
	organization = {Springer},
	pages = {1--15},
	title = {Ensemble methods in machine learning},
	year = {2000},
	Bdsk-Url-1 = {ftp://nozdr.ru/biblio/kolxoz/Cs/CsLn/M/Multiple%20Classifier%20Systems,%201%20conf.,%20MCS%202000(LNCS1857,%20Springer,%202000)(ISBN%203540677046)(411s)_CsLn_.pdf#page=8}}

@inproceedings{nadeau2000inference,
	abstract = {In order to to compare learning algorithms, experimental results reported
in the machine learning litterature often use statistical tests of significance. Unfortunately, most of these tests do not take into account the
variability due to the choice of training set. We perform a theoretical
investigation of the variance of the cross-validation estimate of the generalization error that takes into account the variability due to the choice
of training sets. This allows us to propose two new ways to estimate
this variance. We show, via simulations, that these new statistics perform
well relative to the statistics considered by Dietterich (Dietterich, 1998). },
	author = {Nadeau, Claude and Bengio, Yoshua},
	booktitle = {Advances in neural information processing systems},
	date-added = {2020-08-02 19:35:14 -0400},
	date-modified = {2020-08-02 19:38:56 -0400},
	keywords = {model selection, statistical test},
	pages = {307--313},
	title = {Inference for the generalization error},
	year = {2000},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/1661-inference-for-the-generalization-error.pdf}}

@article{arlot2010survey,
	abstract = {Used to estimate the risk of an estimator or to perform model
selection, cross-validation is a widespread strategy because of its simplicity and its (apparent) universality. Many results exist on model selection
performances of cross-validation procedures. This survey intends to relate
these results to the most recent advances of model selection theory, with
a particular emphasis on distinguishing empirical statements from rigorous
theoretical results. As a conclusion, guidelines are provided for choosing the
best cross-validation procedure according to the particular features of the
problem in hand.},
	author = {Arlot, Sylvain and Celisse, Alain and others},
	date-added = {2020-08-02 19:32:22 -0400},
	date-modified = {2020-08-02 19:32:55 -0400},
	journal = {Statistics surveys},
	keywords = {model selection, cross validation},
	pages = {40--79},
	publisher = {The author, under a Creative Commons Attribution License},
	title = {A survey of cross-validation procedures for model selection},
	volume = {4},
	year = {2010},
	Bdsk-Url-1 = {https://projecteuclid.org/download/pdfview_1/euclid.ssu/1268143839}}

@article{adams2007bayesian,
	abstract = {Changepoints are abrupt variations in the
generative parameters of a data sequence.
Online detection of changepoints is useful in
modelling and prediction of time series in
application areas such as finance, biometrics, and robotics. While frequentist methods have yielded online filtering and prediction techniques, most Bayesian papers have
focused on the retrospective segmentation
problem. Here we examine the case where
the model parameters before and after the
changepoint are independent and we derive
an online algorithm for exact inference of the
most recent changepoint. We compute the
probability distribution of the length of the
current ``run,'' or time since the last changepoint, using a simple message-passing algorithm. Our implementation is highly modular so that the algorithm may be applied to
a variety of types of data. We illustrate this
modularity by demonstrating the algorithm
on three different real-world data sets.},
	author = {Adams, Ryan Prescott and MacKay, David JC},
	date-added = {2020-07-24 09:15:54 -0400},
	date-modified = {2020-07-24 09:16:19 -0400},
	journal = {arXiv preprint arXiv:0710.3742},
	keywords = {change point detection, bayesian},
	title = {Bayesian online changepoint detection},
	year = {2007},
	Bdsk-Url-1 = {https://arxiv.org/pdf/0710.3742.pdf}}

@article{lopez2019detection,
	abstract = {In this paper we address the problem of selection bias under multiple testing in the context of
investment strategies. We introduce an unsupervised learning algorithm that determines the
number of effectively uncorrelated trials carried out in the context of a discovery. This estimate
is critical for estimating the familywise false positive probability, and for filtering out false
investment strategies. },
	annote = {Proposes a way to reduce the Sharpe Ratio in order to avoid false positive in finanical backtests. This reduction is based on the number of trials that were conducted prior to identifying the chosen strategy.},
	author = {L{\'o}pez de Prado, Marcos and Lewis, Michael J},
	date-added = {2020-07-09 16:18:16 -0400},
	date-modified = {2020-07-09 16:18:51 -0400},
	journal = {Quantitative Finance},
	keywords = {overfitting, finance, backtesting},
	number = {9},
	pages = {1555--1565},
	publisher = {Taylor \& Francis},
	title = {Detection of false investment strategies using unsupervised learning methods},
	volume = {19},
	year = {2019},
	Bdsk-Url-1 = {http://smallake.kr/wp-content/uploads/2020/04/SSRN-id3167017.pdf}}

@article{meinshausen2006quantile,
	abstract = {Random forests were introduced as a machine learning tool in Breiman (2001) and have
since proven to be very popular and powerful for high-dimensional regression and classification. For regression, random forests give an accurate approximation of the conditional
mean of a response variable. It is shown here that random forests provide information
about the full conditional distribution of the response variable, not only about the conditional mean. Conditional quantiles can be inferred with quantile regression forests, a
generalisation of random forests. Quantile regression forests give a non-parametric and
accurate way of estimating conditional quantiles for high-dimensional predictor variables.
The algorithm is shown to be consistent. Numerical examples suggest that the algorithm
is competitive in terms of predictive power.},
	annote = {Shows a way to estimate the full conditional distribution for a random forest, by calculating empirical cdf from all the points in a leaf, instead of just using the mean},
	author = {Meinshausen, Nicolai},
	date-added = {2020-07-09 11:53:48 -0400},
	date-modified = {2020-07-09 11:54:57 -0400},
	journal = {Journal of Machine Learning Research},
	keywords = {quantile, random forest, distributional forecast},
	number = {Jun},
	pages = {983--999},
	read = {1},
	title = {Quantile regression forests},
	volume = {7},
	year = {2006},
	Bdsk-Url-1 = {http://www.jmlr.org/papers/volume7/meinshausen06a/meinshausen06a.pdf}}

@article{dietterich1998approximate,
	abstract = {This paper reviews ve approximate statistical tests for determining whether one learning algorithm out-performs another on a particular learning task. These tests are compared experimentally to determine their probability of incorrectly detecting a dierence when no dierence
exists (type I error). Two widely-used statistical tests are shown to have high probability of
Type I error in certain situations and should never be used. These tests are (a) a test for the
dierence of two proportions and (b) a paired-dierences t test based on taking several random
train/test splits. A third test, a paired-dierences t test based on 10-fold cross-validation, exhibits somewhat elevated probability of Type I error. A fourth test, McNemar's test, is shown
to have low Type I error. The fth test is a new test, 5x2cv, based on 5 iterations of 2-fold
cross-validation. Experiments show that this test also has acceptable Type I error. The paper
also measures the power (ability to detect algorithm dierences when they do exist) of these
tests. The cross-validated t test is the most powerful. The 5x2cv test is shown to be slightly
more powerful than McNemar's test. The choice of the best test is determined by the computational cost of running the learning algorithm. For algorithms that can be executed only
once, McNemar's test is the only test with acceptable Type I error. For algorithms that can
be executed ten times, the 5x2cv test is recommended, because it is slightly more powerful and
because it directly measures variation due to the choice of training set.},
	author = {Dietterich, Thomas G},
	date-added = {2020-06-10 07:52:19 -0400},
	date-modified = {2020-11-27 16:32:12 -0500},
	journal = {Neural computation},
	keywords = {model selection, statistical test, cross validation, **MUST-READ**},
	number = {7},
	pages = {1895--1923},
	publisher = {MIT Press},
	title = {Approximate statistical tests for comparing supervised classification learning algorithms},
	volume = {10},
	year = {1998},
	Bdsk-Url-1 = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.3325&rep=rep1&type=pdf}}

@inproceedings{maddox2019simple,
	abstract = {We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose
approach for uncertainty representation and calibration in deep learning. Stochastic
Weight Averaging (SWA), which computes the first moment of stochastic gradient
descent (SGD) iterates with a modified learning rate schedule, has recently been
shown to improve generalization in deep learning. With SWAG, we fit a Gaussian
using the SWA solution as the first moment and a low rank plus diagonal covariance
also derived from the SGD iterates, forming an approximate posterior distribution
over neural network weights; we then sample from this Gaussian distribution to
perform Bayesian model averaging. We empirically find that SWAG approximates
the shape of the true posterior, in accordance with results describing the stationary
distribution of SGD iterates. Moreover, we demonstrate that SWAG performs
well on a wide variety of tasks, including out of sample detection, calibration,
and transfer learning, in comparison to many popular alternatives including MC
dropout, KFAC Laplace, SGLD, and temperature scaling.},
	author = {Maddox, Wesley J and Izmailov, Pavel and Garipov, Timur and Vetrov, Dmitry P and Wilson, Andrew Gordon},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2020-06-04 09:32:41 -0400},
	date-modified = {2020-06-04 09:33:26 -0400},
	keywords = {bayesian, deep learning, uncertainty},
	pages = {13132--13143},
	title = {A simple baseline for bayesian uncertainty in deep learning},
	year = {2019},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/9472-a-simple-baseline-for-bayesian-uncertainty-in-deep-learning.pdf}}

@article{weinan2017proposal,
	abstract = {We discuss the idea of using continuous dynamical systems to model general
high-dimensional nonlinear functions used in machine learning. We also discuss the
connection with deep learning.},
	author = {Weinan, E},
	date-added = {2020-06-02 21:57:32 -0400},
	date-modified = {2020-06-02 21:58:07 -0400},
	journal = {Communications in Mathematics and Statistics},
	keywords = {neural ODE},
	number = {1},
	pages = {1--11},
	publisher = {Springer},
	title = {A proposal on machine learning via dynamical systems},
	volume = {5},
	year = {2017},
	Bdsk-Url-1 = {https://link.springer.com/content/pdf/10.1007/s40304-017-0103-z.pdf}}

@article{haber2017stable,
	abstract = {Deep neural networks have become invaluable tools for supervised machine learning, e.g.,
classification of text or images. While often offering superior results over traditional techniques
and successfully expressing complicated patterns in data, deep architectures are known to be
challenging to design and train such that they generalize well to new data. Critical issues with
deep architectures are numerical instabilities in derivative-based learning algorithms commonly
called exploding or vanishing gradients. In this paper, we propose new forward propagation
techniques inspired by systems of Ordinary Differential Equations (ODE) that overcome this
challenge and lead to well-posed learning problems for arbitrarily deep networks.
The backbone of our approach is our interpretation of deep learning as a parameter estimation problem of nonlinear dynamical systems. Given this formulation, we analyze stability
and well-posedness of deep learning and use this new understanding to develop new network
architectures. We relate the exploding and vanishing gradient phenomenon to the stability of
the discrete ODE and present several strategies for stabilizing deep learning for very deep networks. While our new architectures restrict the solution space, several numerical experiments
show their competitiveness with state-of-the-art networks.},
	author = {Haber, Eldad and Ruthotto, Lars},
	date-added = {2020-06-02 21:55:57 -0400},
	date-modified = {2020-06-02 21:56:24 -0400},
	journal = {Inverse Problems},
	keywords = {neural ODE},
	number = {1},
	pages = {014004},
	publisher = {IOP Publishing},
	title = {Stable architectures for deep neural networks},
	volume = {34},
	year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1705.03341.pdf}}

@inproceedings{chen2018neural,
	abstract = {We introduce a new family of deep neural network models. Instead of specifying a
discrete sequence of hidden layers, we parameterize the derivative of the hidden
state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant
memory cost, adapt their evaluation strategy to each input, and can explicitly trade
numerical precision for speed. We demonstrate these properties in continuous-depth
residual networks and continuous-time latent variable models. We also construct
continuous normalizing flows, a generative model that can train by maximum
likelihood, without partitioning or ordering the data dimensions. For training, we
show how to scalably backpropagate through any ODE solver, without access to its
internal operations. This allows end-to-end training of ODEs within larger models.},
	author = {Chen, Tian Qi and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
	booktitle = {Advances in neural information processing systems},
	date-added = {2020-06-02 15:28:55 -0400},
	date-modified = {2020-06-02 15:29:22 -0400},
	keywords = {neural ODE},
	pages = {6571--6583},
	title = {Neural ordinary differential equations},
	year = {2018},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/7892-neural-ordinary-differential-equations.pdf}}

@article{gholami2019anode,
	abstract = {Residual neural networks can be viewed as the
forward Euler discretization of an Ordinary Differential Equation (ODE) with a unit time step. This has recently motivated
researchers to explore other discretization approaches and train
ODE based networks. However, an important challenge of neural
ODEs is their prohibitive memory cost during gradient backpropogation. Recently a method proposed in [8], claimed that
this memory overhead can be reduced from O(LNt), where
Nt is the number of time steps, down to O(L) by solving
forward ODE backwards in time, where L is the depth of
the network. However, we will show that this approach may
lead to several problems: (i) it may be numerically unstable for
ReLU/non-ReLU activations and general convolution operators,
and (ii) the proposed optimize-then-discretize approach may lead
to divergent training due to inconsistent gradients for small
time step sizes. We discuss the underlying problems, and to
address them we propose ANODE, an Adjoint based Neural
ODE framework which avoids the numerical instability related
problems noted above, and provides unconditionally accurate
gradients. ANODE has a memory footprint of O(L) + O(Nt),
with the same computational cost as reversing ODE solve. We
furthermore, discuss a memory efficient algorithm which can
further reduce this footprint with a trade-off of additional
computational cost. We show results on Cifar-10/100 datasets
using ResNet and SqueezeNext neural networks.},
	author = {Gholami, Amir and Keutzer, Kurt and Biros, George},
	date-added = {2020-06-02 15:24:13 -0400},
	date-modified = {2020-06-02 15:24:46 -0400},
	journal = {arXiv preprint arXiv:1902.10298},
	keywords = {neural ODE},
	title = {Anode: Unconditionally accurate memory-efficient gradients for neural odes},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1902.10298.pdf}}

@article{onken2020discretize,
	abstract = {We compare the discretize-optimize (Disc-Opt) and optimize-discretize (Opt-Disc) approaches for
time-series regression and continuous normalizing flows using neural ODEs. Neural ODEs, first
described in Chen et al. (2018), are ordinary differential equations (ODEs) with neural network
components; these models have competitively solved a variety of machine learning applications.
Training a neural ODE can be phrased as an optimal control problem where the neural network
weights are the controls and the hidden features are the states. Every iteration of gradient-based
training involves solving an ODE forward in time and another backward in time, which can require
large amounts of computation, time, and memory. Gholami et al. (2019) compared the Opt-Disc
and Disc-Opt approaches for neural ODEs arising as continuous limits of residual neural networks
used in image classification tasks. Their findings suggest that Disc-Opt achieves preferable performance due to the guaranteed accuracy of gradients. In this paper, we extend this comparison to
neural ODEs applied to time-series regression and continuous normalizing flows (CNFs). Timeseries regression and CNFs differ from classification in that the actual ODE model is needed in the
prediction and inference phase, respectively. Meaningful models must also satisfy additional requirements, e.g., the invertibility of the CNF. As the continuous model satisfies these requirements
by design, Opt-Disc approaches may appear advantageous. Through our numerical experiments,
we demonstrate that with careful numerical treatment, Disc-Opt methods can achieve similar performance as Opt-Disc at inference with drastically reduced training costs. Disc-Opt reduced costs
in six out of seven separate problems with training time reduction ranging from 39% to 97%, and
in one case, Disc-Opt reduced training from nine days to less than one day.},
	author = {Onken, Derek and Ruthotto, Lars},
	date-added = {2020-06-02 15:23:00 -0400},
	date-modified = {2020-06-02 15:23:30 -0400},
	journal = {arXiv preprint arXiv:2005.13420},
	keywords = {neural ODE},
	title = {Discretize-Optimize vs. Optimize-Discretize for Time-Series Regression and Continuous Normalizing Flows},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2005.13420.pdf}}

@article{hatalis2017smooth,
	abstract = {Uncertainty analysis in the form of probabilistic
forecasting can significantly improve decision making processes
in the smart power grid for better integrating renewable energy sources such as wind. Whereas point forecasting provides
a single expected value, probabilistic forecasts provide more
information in the form of quantiles, prediction intervals, or
full predictive densities. This paper analyzes the effectiveness
of a novel approach for nonparametric probabilistic forecasting
of wind power that combines a smooth approximation of the
pinball loss function with a neural network architecture and
a weighting initialization scheme to prevent the quantile cross
over problem. A numerical case study is conducted using publicly available wind data from the Global Energy Forecasting
Competition 2014. Multiple quantiles are estimated to form
10%, to 90% prediction intervals which are evaluated using a
quantile score and reliability measures. Benchmark models such
as the persistence and climatology distributions, multiple quantile
regression, and support vector quantile regression are used for
comparison where results demonstrate the proposed approach
leads to improved performance while preventing the problem of
overlapping quantile estimates},
	annote = {It seems that this paper just put together a few pieces, but it's an interesting summary.
In particular, they discuss:
* a smooth approximation to the pinball loss, for more efficient training
* a ranking technique to work around quantile crossing},
	author = {Hatalis, Kostas and Lamadrid, Alberto J and Scheinberg, Katya and Kishore, Shalinee},
	date-added = {2020-06-02 10:19:55 -0400},
	date-modified = {2020-06-03 23:45:46 -0400},
	journal = {arXiv preprint arXiv:1710.01720},
	keywords = {uncertainty, pinball loss},
	read = {1},
	title = {Smooth pinball neural network for probabilistic forecasting of wind power},
	year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1710.01720.pdf}}

@inproceedings{lakshminarayanan2017simple,
	abstract = {Deep neural networks (NNs) are powerful black box predictors that have recently
achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian
NNs, which learn a distribution over weights, are currently the state-of-the-art
for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to
standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that
is simple to implement, readily parallelizable, requires very little hyperparameter
tuning, and yields high quality predictive uncertainty estimates. Through a series
of experiments on classification and regression benchmarks, we demonstrate that
our method produces well-calibrated uncertainty estimates which are as good or
better than approximate Bayesian NNs. To assess robustness to dataset shift, we
evaluate the predictive uncertainty on test examples from known and unknown
distributions, and show that our method is able to express higher uncertainty on
out-of-distribution examples. We demonstrate the scalability of our method by
evaluating predictive uncertainty estimates on ImageNet.},
	author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
	booktitle = {Advances in neural information processing systems},
	date-added = {2020-05-28 08:18:21 -0400},
	date-modified = {2020-05-28 08:19:00 -0400},
	keywords = {uncertainty, ensemble},
	pages = {6402--6413},
	title = {Simple and scalable predictive uncertainty estimation using deep ensembles},
	year = {2017},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/7219-simple-and-scalable-predictive-uncertainty-estimation-using-deep-ensembles.pdf}}

@inproceedings{liu2019accurate,
	abstract = {Ensemble learning is a standard approach to building machine learning systems
that capture complex phenomena in real-world data. An important aspect of
these systems is the complete and valid quantification of model uncertainty. We
introduce a Bayesian nonparametric ensemble (BNE) approach that augments an
existing ensemble model to account for different sources of model uncertainty.
BNE augments a model's prediction and distribution functions using Bayesian
nonparametric machinery. It has a theoretical guarantee in that it robustly estimates
the uncertainty patterns in the data distribution, and can decompose its overall
predictive uncertainty into distinct components that are due to different sources of
noise and error. We show that our method achieves accurate uncertainty estimates
under complex observational noise, and illustrate its real-world utility in terms of
uncertainty decomposition and model bias detection for an ensemble in predict air
pollution exposures in Eastern Massachusetts, USA.},
	annote = {They provide a complete decomposition of uncertainty in their model, which they illustrate in a nice diagram

							Overall Uncertainty
						/					\
			epistemic							aleatoric
			/		\
	parametric			structural
						/		\
					prediction		distribution
},
	author = {Liu, Jeremiah and Paisley, John and Kioumourtzoglou, Marianthi-Anna and Coull, Brent},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2020-05-27 11:28:17 -0400},
	date-modified = {2020-05-27 11:31:53 -0400},
	keywords = {uncertainty, bayesian, ensemble},
	pages = {8950--8961},
	title = {Accurate Uncertainty Estimation and Decomposition in Ensemble Learning},
	year = {2019},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/9097-accurate-uncertainty-estimation-and-decomposition-in-ensemble-learning.pdf}}

@inproceedings{tagasovska2019single,
	abstract = {We provide single-model estimates of aleatoric and epistemic uncertainty for deep
neural networks. To estimate aleatoric uncertainty, we propose Simultaneous
Quantile Regression (SQR), a loss function to learn all the conditional quantiles
of a given target variable. These quantiles can be used to compute well-calibrated
prediction intervals. To estimate epistemic uncertainty, we propose Orthonormal
Certificates (OCs), a collection of diverse non-constant functions that map all
training samples to zero. These certificates map out-of-distribution examples to
non-zero values, signaling epistemic uncertainty. Our uncertainty estimators are
computationally attractive, as they do not require ensembling or retraining deep
models, and achieve competitive performance.},
	annote = {They propose a model to handle both epistemic and aleatoric uncertainty.

y_true - y_predict,W = F_true(x1, x2)  - F_W*(x1)	=> approximation uncertainty
				+ F_W*(x1) - F_W(x1)		=> epistemic uncertainty
				+ epsilon					=> aleatoric uncertainty

They also mention approximation uncertainty, which could be defined as model misspecification. That is,
the erorr due to the fact that the model doesn't have enought capacity. In their own words, 
the ``approximation uncertainty describes the errors made by simplistic models unable to fit complex data 
(e.g., the error made by a linear model fitting a sinusoidal curve)''. However, they make the assumption that this
error is always zero since they focus on deep learning models that are known to have large capacity.

Note: I think there is also the case of hidden variables, and this could happen to any model. 
Some authors (e.g., WIkipedia) consider the case of hidden variables to be epistemic uncertainty, probably because
this is something we may be able to act upon (find out that data and measure it). This could be reconciled
if we consider approximation uncertainty as a special case of epistemic uncertainty (understood in a broad sense). That's
exactly what they do in the paper ``Accurate Uncertainty Estimation and Decomposition in Ensemble Learning''.

Aleatoric uncertainty:
They define aleatoric uncertainty as the uncertainty that accounts for the sotchasticity of the data.
They propose a method called Simultaneous Quantile Regression (SQR) which attempts to estimate all quantiles at once. 
It does so by training for different quantiles, chosent randomly at train time.

Epistemic uncertainty:
They say: ``epistemic uncertainty (from the Greek word episteme, meaning
``knowledge'') describes the errors associated to the lack of experience of our model at certain regions
of the feature space. Therefore, epistemic uncertainty is inversely proportional to the density of
training examples, and could be reduced by collecting data in those low density regions.''
Later in the paper, they clarify their position as the one of ``out-of-domain'' data, that is data
that was seen during training.
I think epistmic uncertainty, in general, includes everything you could theoretically act upon, or
throw more money at it (more data collection, more compute power,{\ldots}). But probably they assume
that everything within our control has already been tried. So only remains the lack of sufficient data 
in certain region of the data space, i.e., out-of-distribution samples.
How they handle epistemic uncertainty: they train certificates, that is functions that are trained to return
0 for all in-sample data and 1 for out-of-sample data, but are only trained on in-sample data (1-class classification).
They train an ensemble of certificates. You can use linear certificates or more complicated non-linear models. When 
using more complicated models (like DL), you need to constrain the certificates to prevent having them learn the zero map.
The solution is to impose orthogonality.
They apply their technique to out-of-dsitribution detection, and compare favorably with the BALD method (used in active learning).
However, I don't see how that technique could be used to estimate the variance error due to the epistemic uncertainty. Which is
what we would need to do uncertainty quantification.},
	author = {Tagasovska, Natasa and Lopez-Paz, David},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2020-05-27 08:25:52 -0400},
	date-modified = {2020-10-27 22:45:55 -0400},
	keywords = {uncertainty, deep learning},
	pages = {6414--6425},
	read = {1},
	title = {Single-Model Uncertainties for Deep Learning},
	year = {2019},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/8870-single-model-uncertainties-for-deep-learning.pdf}}

@article{krueger2016zoneout,
	abstract = {We propose zoneout, a novel method for regularizing RNNs. At each timestep,
zoneout stochastically forces some hidden units to maintain their previous values.
Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving
generalization. But by preserving instead of dropping hidden units, gradient
information and state information are more readily propagated through time, as
in feedforward stochastic depth networks. We perform an empirical investigation
of various RNN regularizers, and find that zoneout gives significant performance
improvements across tasks. We achieve competitive results with relatively simple
models in character- and word-level language modelling on the Penn Treebank
and Text8 datasets, and combining with recurrent batch normalization (Cooijmans
et al., 2016) yields state-of-the-art results on permuted sequential MNIST},
	author = {Krueger, David and Maharaj, Tegan and Kram{\'a}r, J{\'a}nos and Pezeshki, Mohammad and Ballas, Nicolas and Ke, Nan Rosemary and Goyal, Anirudh and Bengio, Yoshua and Courville, Aaron and Pal, Chris},
	date-added = {2020-05-26 22:19:34 -0400},
	date-modified = {2020-05-26 22:20:31 -0400},
	journal = {arXiv preprint arXiv:1606.01305},
	keywords = {regularization, rnn},
	title = {Zoneout: Regularizing rnns by randomly preserving hidden activations},
	year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1606.01305.pdf}}

@inproceedings{zhu2017deep,
	abstract = {Reliable uncertainty estimation for time series prediction is critical in many fields, including physics, biology,
and manufacturing. At Uber, probabilistic time series forecasting is used for robust prediction of number of trips during
special events, driver incentive allocation, as well as real-time
anomaly detection across millions of metrics. Classical time
series models are often used in conjunction with a probabilistic
formulation for uncertainty estimation. However, such models
are hard to tune, scale, and add exogenous variables to. Motivated by the recent resurgence of Long Short Term Memory
networks, we propose a novel end-to-end Bayesian deep model
that provides time series prediction along with uncertainty
estimation. We provide detailed experiments of the proposed
solution on completed trips data, and successfully apply it to
large-scale time series anomaly detection at Uber.},
	annote = {Also see the blog post: https://eng.uber.com/neural-networks-uncertainty-estimation/

They detail a forecasting model to handle uncertainty (epistemic and aleatoric):
* epistemic uncertainty (defined as model uncertainty and model misspecification) is handled via Dropout
* aleatoric uncertainty is handled by a constant variance estimated as the empirical variance of the model error on a validation set. For the prediciton, I believe they use the same model but w/o Dropout

Overall the model has a pre-trained encoder-decoder part. The encoder is used to generate features that are passed to a fully-connected network.
Variational Dropout is applied to the encoder, while standard Dropout is applied to the fc model.
},
	author = {Zhu, Lingxue and Laptev, Nikolay},
	booktitle = {2017 IEEE International Conference on Data Mining Workshops (ICDMW)},
	date-added = {2020-05-26 20:52:07 -0400},
	date-modified = {2020-05-27 12:04:34 -0400},
	keywords = {time series, forecasting, bayesian, uncertainty},
	organization = {IEEE},
	pages = {103--110},
	read = {1},
	title = {Deep and confident prediction for time series at uber},
	year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1709.01907.pdf}}

@inproceedings{guo2017calibration,
	abstract = {Confidence calibration -- the problem of predicting probability estimates representative of the
true correctness likelihood -- is important for
classification models in many applications. We
discover that modern neural networks, unlike
those from a decade ago, are poorly calibrated.
Through extensive experiments, we observe that
depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various
post-processing calibration methods on state-ofthe-art architectures with image and document
classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and
straightforward recipe for practical settings: on
most datasets, temperature scaling -- a singleparameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.},
	author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
	date-added = {2020-05-25 11:44:37 -0400},
	date-modified = {2020-10-27 22:45:44 -0400},
	keywords = {calibration, uncertainty, deep learning},
	organization = {JMLR. org},
	pages = {1321--1330},
	title = {On calibration of modern neural networks},
	year = {2017},
	Bdsk-Url-1 = {https://dl.acm.org/doi/pdf/10.5555/3305381.3305518?download=true}}

@inproceedings{vaswani2017attention,
	abstract = {The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer,
based solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including
ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
our model establishes a new single-model state-of-the-art BLEU score of 41.0 after
training for 3.5 days on eight GPUs, a small fraction of the training costs of the
best models from the literature.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	booktitle = {Advances in neural information processing systems},
	date-added = {2020-05-25 10:27:11 -0400},
	date-modified = {2020-09-24 18:45:53 -0400},
	keywords = {deep learning, attention},
	pages = {5998--6008},
	title = {Attention is all you need},
	year = {2017},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}}

@article{rasul2020multi,
	abstract = {Time series forecasting is often fundamental to
scientific and engineering problems and enables
decision making. With ever increasing data set
sizes, a trivial solution to scale up predictions is to
assume independence between interacting time series. However, modeling statistical dependencies
can improve accuracy and enable analysis of interaction effects. Deep learning methods are well
suited for this problem, but multi-variate models
often assume a simple parametric distribution and
do not scale to high dimensions. In this work
we model the multi-variate temporal dynamics of
time series via an autoregressive deep learning
model, where the data distribution is represented
by a conditioned normalizing flow. This combination retains the power of autoregressive models,
such as good performance in extrapolation into
the future, with the flexibility of flows as a general purpose high-dimensional distribution model,
while remaining computationally tractable. We
show that it improves over the state-of-the-art for
standard metrics on many real-world data sets
with several thousand interacting time-series},
	annote = {Paper a bit hard to read. They have a blog post that is much clearer: https://research.zalando.com/2020/03/

In its simplest form, their model is a multivariate version (MIMO) of DeepAR that does not constrain on the distribution to sample from (DeepAR asks you to pick a fixed distribution). Instead their parametrize the sampling stage via a normalizing flow, that is a sequence of simple, invertible transformations that map a given sample (typically isotropic Gaussian) to the predicted sample. That normalizing flow is parametrized by the output of a LSTM (like DeepAR).

They also have a mode where they replace the RNN part with an attention-based model},
	author = {Rasul, Kashif and Sheikh, Abdul-Saboor and Schuster, Ingmar and Bergmann, Urs and Vollgraf, Roland},
	date-added = {2020-05-25 10:20:39 -0400},
	date-modified = {2020-05-25 10:25:05 -0400},
	journal = {arXiv preprint arXiv:2002.06103},
	keywords = {time series, forecasting, distributional forecast,},
	read = {1},
	title = {Multi-variate Probabilistic Time Series Forecasting via Conditioned Normalizing Flows},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2002.06103.pdf}}

@article{chen2018confidence,
	abstract = {We propose a novel confidence scoring mechanism for deep neural networks based on a
two-model paradigm involving a base model
and a meta-model. The confidence score is
learned by the meta-model observing the base
model succeeding/failing at its task. As features to the meta-model, we investigate linear
classifier probes inserted between the various
layers of the base model. Our experiments
demonstrate that this approach outperforms
multiple baselines in a filtering task, i.e., task
of rejecting samples with low confidence. Experimental results are presented using CIFAR10 and CIFAR-100 dataset with and without
added noise. We discuss the importance of
confidence scoring to bridge the gap between
experimental and real-world applications},
	annote = {hey use Linear Classifier Probes [4] to extract inform-ation from within the hidden layers of an image classifier, and use these probes data, along withthe output of the classifier, to train a a confidence score predictor. They compare the select-ive prediction technique described in [24] against multiple variations of their model: with probesdata from the hidden layers or with only the output of the classifier; using a logistic regression ora gradient boosting model as their meta-model. The classifier and the meta-model are trainedon separate datasets. Performance is computed on a held-out test set. Besides this standard set-up, theyalsoattempttoverifytheperformanceoftheirconfidencescorepredictorunderdif-ferent regimes by corrupting their dataset. In one experiment, they introduce noise in the labelsof the training set, to reduce the accuracy of the classifier. In their second experiment, to assessthe performance of their confidence score under dataset shift, they pass, at test time, imageswith labels that were not in the training set. From their experiments, they conclude that a gradi-ent boosting model using the probes data slightly outperforms the other models. The differencein performance widens a bit more when testing on out-of-distribution data, and with a classifiertrained on noisy labels. They also tried to apply temperature scaling [27] as a pre-processingstep to the SGR method [24], but reported no improvement},
	author = {Chen, Tongfei and Navr{\'a}til, Ji{\v{r}}{\'\i} and Iyengar, Vijay and Shanmugam, Karthikeyan},
	date-added = {2020-05-22 08:38:29 -0400},
	date-modified = {2020-05-22 08:48:13 -0400},
	journal = {arXiv preprint arXiv:1805.05396},
	keywords = {confidence score},
	read = {1},
	title = {Confidence scoring using whitebox meta-models with linear classifier probes},
	year = {2018},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1805.05396.pdf}}

@inproceedings{geifman2017selective,
	abstract = {Selective classification techniques (also known as reject option) have not yet been
considered in the context of deep neural networks (DNNs). These techniques
can potentially significantly improve DNNs prediction performance by trading-off
coverage. In this paper we propose a method to construct a selective classifier
given a trained neural network. Our method allows a user to set a desired risk
level. At test time, the classifier rejects instances as needed, to grant the desired
risk (with high probability). Empirical results over CIFAR and ImageNet convincingly demonstrate the viability of our method, which opens up possibilities to
operate DNNs in mission-critical applications. For example, using our method an
unprecedented 2% error in top-5 ImageNet classification can be guaranteed with
probability 99.9%, and almost 60% test coverage.},
	annote = {The authors leverages the field of selective prediction (or reject option) to design a Selec-tionwithGuaranteedRisk(SGR)algorithmthatwillreturnaselectionfunctiongivenatrainedclas-sifier,arisktarget,aconfidenceparameter,andaconfidence-ratefunction. Thisconfidence-ratefunctions can be, for instance, derived from an approximate Bayesian inference methods, likeBayesian dropout, or a softmax response (SR). The resulting selection function calculated theacceptable threshold for the confidence-rate, given the risk parameters specified by the user.The selection function can then be used to decide whether a prediction should be trusted or not.Overall, it is an interesting method that is simple, requires no training, yet provides mathematicalguarantees about the properties of the confidence score.},
	author = {Geifman, Yonatan and El-Yaniv, Ran},
	booktitle = {Advances in neural information processing systems},
	date-added = {2020-05-22 08:28:41 -0400},
	date-modified = {2020-05-22 08:48:36 -0400},
	keywords = {classification},
	pages = {4878--4887},
	read = {1},
	title = {Selective classification for deep neural networks},
	year = {2017},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/7073-selective-classification-for-deep-neural-networks.pdf}}

@article{angelopoulosidentifying,
	author = {Angelopoulos, Anastasios Nikolas and Pathak, Reese and Varma, Rohit and Jordan, Michael I},
	date-added = {2020-05-21 14:54:52 -0400},
	date-modified = {2020-05-21 14:55:21 -0400},
	journal = {Harvard Data Science Review},
	keywords = {statistics, bias},
	title = {On Identifying and Mitigating Bias in the Estimation of the COVID-19 Case Fatality Rate},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2003.08592.pdf}}

@inproceedings{ren2019time,
	abstract = {Large companies need to monitor various metrics (for example,
Page Views and Revenue) of their applications and services in real
time. At Microsoft, we develop a time-series anomaly detection service which helps customers to monitor the time-series continuously
and alert for potential incidents on time. In this paper, we introduce the pipeline and algorithm of our anomaly detection service,
which is designed to be accurate, efficient and general. The pipeline
consists of three major modules, including data ingestion, experimentation platform and online compute. To tackle the problem
of time-series anomaly detection, we propose a novel algorithm
based on Spectral Residual (SR) and Convolutional Neural Network
(CNN). Our work is the first attempt to borrow the SR model from
visual saliency detection domain to time-series anomaly detection.
Moreover, we innovatively combine SR and CNN together to improve the performance of SR model. Our approach achieves superior
experimental results compared with state-of-the-art baselines on
both public datasets and Microsoft production data.},
	annote = {Introduce the pipeline and algorithm of our anomaly detection service

Pipelines comprise of 3 major modules:
*  data ingestion, 
* experimentation platform and 
* online compute

Challenges in designing an industrial service for time-series anomaly detection:
* Lack of labels => supervised setting inappropriate
* Generalization: may different patterns, and no method works for all time series patterns
* Efficiency/Scalability: needs model that can process lots of time series very fast (large accurate models are too slow)

Their method is based on
* saliency map: this is generated by some sort of average of the log(amplitude) in Fourier space (over a sliding window), then reverting that average back to time domain (with the phase untouched)
* followed by a CNN trained on synthetic saliency maps to automatically idenfity where an anomaly occured

Overall an interesting paper. They also mention a lot of the standard methods.
Their appliations seem to be mostly periodic time series. It would need to be tested on non-stationary time series. But in the case of trend stationarity, it seems the Fourier transform would be able to handle that.},
	author = {Ren, Hansheng and Xu, Bixiong and Wang, Yujing and Yi, Chao and Huang, Congrui and Kou, Xiaoyu and Xing, Tony and Yang, Mao and Tong, Jie and Zhang, Qi},
	booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
	date-added = {2020-05-21 11:39:12 -0400},
	date-modified = {2020-05-21 14:42:36 -0400},
	keywords = {anomaly detection, time series},
	pages = {3009--3017},
	read = {1},
	title = {Time-Series Anomaly Detection Service at Microsoft},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1906.03821.pdf}}

@mastersthesis{melancon2019uncertainty,
	author = {Gauthier Melan{\c c}on, Gabrielle},
	date-added = {2020-05-15 10:49:21 -0400},
	date-modified = {2020-05-21 14:57:06 -0400},
	keywords = {uncertainty},
	read = {1},
	school = {Ecole Polytechnique de Montr\'eal},
	title = {Quantifying Uncertainty in Systems - Two Practical Use Cases Using Machine Learning to Predict and Explain Systems Failure},
	year = {2019}}

@article{cawley2010over,
	abstract = {Model selection strategies for machine learning algorithms typically involve the numerical optimisation of an appropriate model selection criterion, often based on an estimator of generalisation
performance, such as k-fold cross-validation. The error of such an estimator can be broken down
into bias and variance components. While unbiasedness is often cited as a beneficial quality of a
model selection criterion, we demonstrate that a low variance is at least as important, as a nonnegligible variance introduces the potential for over-fitting in model selection as well as in training
the model. While this observation is in hindsight perhaps rather obvious, the degradation in performance due to over-fitting the model selection criterion can be surprisingly large, an observation that
appears to have received little attention in the machine learning literature to date. In this paper, we
show that the effects of this form of over-fitting are often of comparable magnitude to differences
in performance between learning algorithms, and thus cannot be ignored in empirical evaluation.
Furthermore, we show that some common performance evaluation practices are susceptible to a
form of selection bias as a result of this form of over-fitting and hence are unreliable. We discuss methods to avoid over-fitting in model selection and subsequent selection bias in performance
evaluation, which we hope will be incorporated into best practice. While this study concentrates
on cross-validation based model selection, the findings are quite general and apply to any model
selection practice involving the optimisation of a model selection criterion evaluated over a finite
sample of data, including maximisation of the Bayesian evidence and optimisation of performance
bounds.},
	annote = {	2. Kernel Ridge Regression

They use kernel ridge regression as it has a few hyperparameters to tune, and the leave-one-out cross-validation solution can be computed in close form. The hyperparameters are eta for the kernel and lambda for the regularization. You typically set these hyperparameters from an exhaustive grid search.

However in this paper, they optimize the hyperparameters in an iterative manner. They define the  Predicted REsidual Sum-of-Squares (PRESS) statistic, which the sum of squares of the leave-one-out cross-validation residual errors. ``The PRESS criterion can be optimised efficiently using scaled conjugate gradient descent (Williams, 1991) or Nelder-Mead simplex (Nelder and Mead, 1965) procedures.''

	3. Data Sets used in Empirical Demonstrations

They use synthetic and real datasets (UCI)

	3.2 A Suite of Benchmarks for Robust Performance Evaluation
``Each benchmark is based on a data set from the UCI machine learning repository, augmented by a set of 100 pre-defined partitions to form multiple realisations of the training and test sets (20 in the case of the larger image and splice data sets).'': but they don't say anything about that split (80/20, 60/40,{\ldots}?)

	4. Over-fitting in Model Selection

An important definition: ``Here we use ``over-fitting in model selection'' to mean minimisation of the model selection criterion beyond the point at which generalisation performance ceases to improve and subsequently begins to decline.''

Another interesting point to keep in mind is that by ``model selection'', the authors understand selecting the best set of hyperparameters for a given model.

Thanks to the iterative nature of their training procedure for Kernel Ridge Regression, they are able to reproduce the typical bias-variance trade-off plot, where you see the training over-fitting after a few iterations (i.e., cv estimate decreases all the time, but true test error goes down then up after a few iterations).
That example was with the synthetic dataset, and 1,000 realizations of small training sets (64 samples). Each training set is subject to a 4-fold cv.

But that means they do hyperparameter optimization (and not search). Their procedure is itereative. How does that compare to the typical hyperparameter search we do (grid search, random search,{\ldots})?

	4.1 Bias and Variance in Model Selection
``Unbiasedness provides the assurance that the minimum of the expected value of the model selection criterion, ED{g(;D)} coincides with the minimum of the test error, G(). However, in practice, we have only a finite sample of data, Di, over which to evaluate the model selection criterion, and so it is the minimum of g(;Di) that is of interest.''
They make a point that unbiasedness of your generalization error criteria is not all you need. Because in practice, with finite data, if this criteria has large variance, it won't be very useful. So you might want to accept some bias if you can have lower variance in exchange.
``This demonstrates that while unbiasedness is reassuring, as it means that the form of the model selection criterion is correct on average, the variance of the criterion is also vitally important as it is this that ensures that the minimum of the selection criterion evaluated on a particular sample will provide good generalisation.''

	4.2 The Effects of Over-fitting in Model Selection
``In this section, we investigate the effect of the variance of the model selection criterion using a more realistic example, again based on the synthetic benchmark, where the underlying generative model is known and so we are able to evaluate the true test error. It is demonstrated that over-fitting in model selection can cause both under-fitting and over-fitting of the training sample. ''

They compare the shape of the true test error against the shape of the estimated generalization error in a simple case with synthetic dataset.
``The simplest estimator for use in model selection is the error computed over an independent validation set, that is, the split-sample estimator''
They show that the split-sample estimator is unbiased (100 realizations). But each realization has large variance (especially for small sample size), and consequently the selected hyperparameters will vary a lot.

``Note that in this section we have deliberately employed a split-sample based model selection strategy with a relatively high variance, due to the limited size of the validation set. A straightforward way to reduce the variance of the model selection criterion is simply to increase the size of the validation sample over which it is evaluated. ''

``The variation in performance for different realisations of the benchmark suggests that evaluation of machine learning algorithms should always involve multiple partitions of the data to form training/validation and test sets, as the sampling of data for a single partition of the data might arbitrarily favour one classifier over another.''

``However, it should also be noted that a difference in performance between two algorithms is unlikely to be of practical significance, even if it is statistically significant, if it is smaller than the variation in performance due to the random sampling of the data, as it is probable that a greater improvement in performance would be obtained by further data collection than by selection of the optimal classifier.''

	4.3 Is Over-fitting in Model Selection Really a Genuine Concern in Practice?
Run similar analysis with the real-life datasets, and using RBF and ARD kernels. RBF is a special case of ARD, so theoretically you would expect the performance with ARD to be at least as good as the one with RBF. In practice, RBF beats ARD for all datasets but one. The hyperparameters are chosen from the iterative process described above. And because the search space is much larger with ARD, we end up over-fitting (lower optimized PRESS statistic, but greater teste error).

``This is a clear demonstration that over-fitting in model selection can be a significant problem in practical applications, especially where there are many hyper-parameters or where only a limited supply of data is available.''

	4.4 Avoiding Over-fitting in Model Selection
``t seems reasonable to suggest that over-fitting in model selection is possible whenever a model selection criterion evaluated over a finite sample of data is directly optimised. Like over-fitting in training, over-fitting in model selection is likely to be most severe when the sample of data is small and the number of hyper-parameters to be tuned is relatively large''

The authors conlude that ``the problem of over-fitting in model selection is of the same nature as that of over-fitting at the first level of inference''. Therefore, they recommend the same remedies: regularization, early stopping, model averaging, Bayesian model averaging, ensembling,{\ldots}
However, they do not study the impact of these procedures.

``Bengio and Grandvalet (2004) have shown there is no unbiased estimate of the variance of (k-fold) crossvalidation. ''

	5. Bias in Performance Estimation
`` The key [to avoid potentially significant bias in performance evaluation, arising due to over-fitting in model
selection] is to treat both training and model selection together, as integral parts of the model fitting procedure and ensure they are never performed separately at any point of the evaluation process''

5.1 An Unbiased Performance Evaluation Methodology


},
	author = {Cawley, Gavin C and Talbot, Nicola LC},
	date-added = {2020-05-06 17:29:12 -0400},
	date-modified = {2020-12-09 10:18:17 -0500},
	journal = {Journal of Machine Learning Research},
	keywords = {model selection, cross validation, overfitting},
	number = {Jul},
	pages = {2079--2107},
	read = {1},
	title = {On over-fitting in model selection and subsequent selection bias in performance evaluation},
	volume = {11},
	year = {2010},
	Bdsk-Url-1 = {https://jmlr.csail.mit.edu/papers/volume11/cawley10a/cawley10a.pdf}}

@inproceedings{snoek2019can,
	author = {Snoek, Jasper and Ovadia, Yaniv and Fertig, Emily and Lakshminarayanan, Balaji and Nowozin, Sebastian and Sculley, D and Dillon, Joshua and Ren, Jie and Nado, Zachary},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2020-05-03 00:53:17 -0400},
	date-modified = {2020-05-26 20:53:19 -0400},
	keywords = {concept drift, uncertainty, bayesian, ensemble},
	pages = {13969--13980},
	read = {1},
	title = {Can you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift},
	year = {2019},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/9547-can-you-trust-your-models-uncertainty-evaluating-predictive-uncertainty-under-dataset-shift.pdf}}

@book{spirtes2000causation,
	annote = {That's afaik the best reference for the (Fast Causal Inference) FCI algorithm },
	author = {Spirtes, Peter and Glymour, Clark N and Scheines, Richard and Heckerman, David},
	date-added = {2020-05-01 14:44:46 -0400},
	date-modified = {2020-05-21 14:43:47 -0400},
	keywords = {causality},
	publisher = {MIT press},
	read = {1},
	title = {Causation, prediction, and search},
	year = {2000}}

@inproceedings{ballard1987modular,
	author = {Ballard, Dana H},
	booktitle = {AAAI},
	date-added = {2020-04-30 15:39:59 -0400},
	date-modified = {2020-05-21 14:57:35 -0400},
	keywords = {autoencoder, deep learning},
	pages = {279--284},
	read = {1},
	title = {Modular Learning in Neural Networks.},
	year = {1987}}

@book{goodfellow2016deep,
	abstract = {Table of Contents
Acknowledgements
Notation
1 Introduction
Part I: Applied Math and Machine Learning Basics
2 Linear Algebra
3 Probability and Information Theory
4 Numerical Computation
5 Machine Learning Basics
Part II: Modern Practical Deep Networks
6 Deep Feedforward Networks
7 Regularization for Deep Learning
8 Optimization for Training Deep Models
9 Convolutional Networks
10 Sequence Modeling: Recurrent and Recursive Nets
11 Practical Methodology
12 Applications
Part III: Deep Learning Research
13 Linear Factor Models
14 Autoencoders
15 Representation Learning
16 Structured Probabilistic Models for Deep Learning
17 Monte Carlo Methods
18 Confronting the Partition Function
19 Approximate Inference
20 Deep Generative Models
Bibliography
Index},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	date-added = {2020-04-30 15:36:47 -0400},
	date-modified = {2020-05-21 14:44:17 -0400},
	keywords = {deep learning},
	publisher = {MIT press},
	read = {1},
	title = {Deep learning},
	year = {2016},
	Bdsk-Url-1 = {http://www.deeplearningbook.org/}}

@article{xu2010principal,
	abstract = {We consider the dimensionality-reduction problem (finding a subspace approximation of observed data) for
contaminated data in the high dimensional regime, where the number of observations is of the same magnitude as
the number of variables of each observation, and the data set contains some (arbitrarily) corrupted observations. We
propose a High-dimensional Robust Principal Component Analysis (HR-PCA) algorithm that is tractable, robust
to contaminated points, and easily kernelizable. The resulting subspace has a bounded deviation from the desired
one, achieves maximal robustness -- a breakdown point of 50% while all existing algorithms have a breakdown
point of zero, and unlike ordinary PCA algorithms, achieves optimality in the limit case where the proportion of
corrupted points goes to zero.},
	author = {Xu, Huan and Caramanis, Constantine and Mannor, Shie},
	date-added = {2020-04-30 14:27:10 -0400},
	date-modified = {2020-05-21 14:57:57 -0400},
	journal = {arXiv preprint arXiv:1002.4658},
	keywords = {pca, high dimension},
	read = {1},
	title = {Principal component analysis with contaminated data: The high dimensional case},
	year = {2010},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1002.4658.pdf}}

@article{goldberg2014word2vec,
	abstract = {The word2vec software of Tomas Mikolov and colleagues1 has gained a lot
of traction lately, and provides state-of-the-art word embeddings. The learning
models behind the software are described in two research papers [1, 2]. We
found the description of the models in these papers to be somewhat cryptic
and hard to follow. While the motivations and presentation may be obvious to
the neural-networks language-modeling crowd, we had to struggle quite a bit to
figure out the rationale behind the equations.
This note is an attempt to explain equation (4) (negative sampling) in ``Distributed Representations of Words and Phrases and their Compositionality'' by
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean [2].},
	author = {Goldberg, Yoav and Levy, Omer},
	date-added = {2020-04-30 12:27:28 -0400},
	date-modified = {2020-04-30 12:27:55 -0400},
	journal = {arXiv preprint arXiv:1402.3722},
	keywords = {nlp, word2vec, negative-sampling},
	title = {word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method},
	year = {2014},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1402.3722v1.pdf}}

@article{runge2019detecting,
	abstract = {Identifying causal relationships and quantifying their strength from observational time series data are key
problems in disciplines dealing with complex dynamical systems such as the Earth system or the human body.
Data-driven causal inference in such systems is challenging since datasets are often high dimensional and nonlinear with
limited sample sizes. Here, we introduce a novel method that flexibly combines linear or nonlinear conditional independence tests with a causal discovery algorithm to estimate causal networks from large-scale time series datasets.
We validate the method on time series of well-understood physical mechanisms in the climate system and the human
heart and using large-scale synthetic datasets mimicking the typical properties of real-world data. The experiments
demonstrate that our method outperforms state-of-the-art techniques in detection power, which opens up entirely
new possibilities to discover and quantify causal networks from time series across a range of research fields.},
	annote = {Paper where is introduced the PCMCI algorithm, which is an example of causal network learning algorithm specialized for time series data.
The authors also provide an implementation of their algorithm on github: https://github.com/jakobrunge/tigramite
Additional material: https://advances.sciencemag.org/content/advances/suppl/2019/11/21/5.11.eaau4996.DC1/aau4996_SM.pdf

The claim to fame of PCMCI is its capacity to handle high-dimensional datasets and highly interdependent time series, along with linear and nonlinear relationship in the data. In the paper, they also show some robustness to nonstationarity. The main shortcomings remain the reliance on the Causality Sufficiency hypothesis. The PCMCI algorithm builds on top of the PC algorithm. The latter is a Markov discovery algorithm that relies on the causal Markov property to identify parents of a variable. However, as shown in the paper, the PC algorithm cannot be used as is with time series. So the authors add a momentary condition independence (MCI) test after the PC step. The PC step (PC1) iteratively removes links that are independent conditioned on the most important drivers in the previous step. After that PC1 step, each variable is left with the causal parents and potentially some false positives. The MCI step tests each potential causal link and assess causal strenght. The MCI step conditions on the parents and the parents of the potential parent tested (to account for autocorrelation). The significance level to reject/accept the independence test in the MCI step is the one parameter of the method that can be adjusted; it can selected via typical techniques (information theory, or cross-validation). Any kinf of conditional independence test can be used in this algorithm, whether linear (e.g., linear partical correlation) or nonlinear (e.g., GPDC, or CMI).},
	author = {Runge, Jakob and Nowack, Peer and Kretschmer, Marlene and Flaxman, Seth and Sejdinovic, Dino},
	date-added = {2020-04-28 13:14:57 -0400},
	date-modified = {2020-05-21 14:44:37 -0400},
	journal = {Science Advances},
	keywords = {causality, time series},
	number = {11},
	pages = {eaau4996},
	publisher = {American Association for the Advancement of Science},
	read = {1},
	title = {Detecting and quantifying causal associations in large nonlinear time series datasets},
	volume = {5},
	year = {2019},
	Bdsk-Url-1 = {https://advances.sciencemag.org/content/advances/5/11/eaau4996.full.pdf}}

@article{runge2019inferring,
	abstract = {The heart of the scientific enterprise is a rational effort to understand the causes behind
the phenomena we observe. In large-scale complex dynamical systems such as the Earth
system, real experiments are rarely feasible. However, a rapidly increasing amount of
observational and simulated data opens up the use of novel data-driven causal methods
beyond the commonly adopted correlation techniques. Here, we give an overview of causal
inference frameworks and identify promising generic application cases common in Earth
system sciences and beyond. We discuss challenges and initiate the benchmark platform
causeme.net to close the gap between method users and developers.},
	annote = {Because intervention are hard (impossible?) to put in place in Earth science, there is a strong need to rely on recent data-driven methods (observational causal discovery). They give a few examples of situations where causality was used in Earth science, in particular examples where Granger causality and/or correlation-based methods failed to identify the correct causal links. Next, they introduced the main methods for causality inference with time seriesa. Many (?) causal inference methods for time series are grounded on a few common assumptions: time-order (past events influence future events, not the other way around), causal sufficiency (no unobserved common factor exists), for graphical models the Causal Markov condition (conditional independence on all variables who don't have a direct impact),{\ldots} They first introduce Granger causality, which is probably the foundational method of causal inference in time series. The high-level idea to assess whether variable X causes variable Y is to test whether knowledge of past values of X help improve or not the prediction of future values of Y (reduction in residual variance). Different variations of that idea have been developped over the years, mainly depending on the type of model that is used to predict Y: AR, nonlinear, or transfer entropy (a non-parametric statistic measuring the amount of directed time-asymmetric transfer of information between 2 random processes; the amount of information is measured using Shannon's entropy). One potential limitation of Granger causality is that it can only detect ``lagged causality'', i.e., causal relation coming from past data; if the idea is to apply a causality screening prior to forecast, then this is probably sufficient as we'll only rely on past information. On the other hand, Granger causality typically fails to identify conditionally independent links. Also, multivariate extensions of GC fail if too many variables are considered. Next, they discuss nonlinear state-space methods, in particular convergent cross-mapping (CCM) methods. These methods look for interactions in the underlying dynamic process that generated the data. They don't say much about that class of methods, except that CCM is not well suited for multivariate, purely stochastic processes, as it doesn't explicitly condition on other variables. Causal network learning algorithms use probabilistic graphical models (think Bayesian networks) to infer causality structure. They work well in large-scale applications. There are 2 main families of methods depending whether one starts from an empty graph and add links, or one starts from a fully connected graph and remove links. The decision to add/remove links is based on the results of some tests (e.g., conditional independence statistical test, or some score function,{\ldots}). They highlight 2 methods: PCMCI, designed to handle auto-correlated and nonlinear time series, and FCI which does not rely on the Causal Sufficiency assumption. Lastly, the structural causal model (SCM) framework, promoted by Judea Pearl. Causal graphs cannot always identify the direction of contemporaneous causality links (i.e., within the Markov equivalence class). SCM allows to make assumptions about the possible causal relationships we accept.},
	author = {Runge, Jakob and Bathiany, Sebastian and Bollt, Erik and Camps-Valls, Gustau and Coumou, Dim and Deyle, Ethan and Glymour, Clark and Kretschmer, Marlene and Mahecha, Miguel D and Mu{\~n}oz-Mar{\'\i}, Jordi and others},
	date-added = {2020-04-28 08:16:26 -0400},
	date-modified = {2020-04-28 13:16:12 -0400},
	journal = {Nature communications},
	keywords = {causality, time series},
	number = {1},
	pages = {1--13},
	publisher = {Nature Publishing Group},
	read = {1},
	title = {Inferring causation from time series in Earth system sciences},
	volume = {10},
	year = {2019},
	Bdsk-Url-1 = {https://www.nature.com/articles/s41467-019-10105-3.pdf}}

@inproceedings{varoquaux2019comparing,
	abstract = {Are two sets of observations drawn from the same distribution? This problem is
a two-sample test. Kernel methods lead to many appealing properties. Indeed
state-of-the-art approaches use the L2 distance between kernel-based distribution
representatives to derive their test statistics. Here, we show that Lp distances
(with p  1) between these distribution representatives give metrics on the space
of distributions that are well-behaved to detect differences between distributions
as they metrize the weak convergence. Moreover, for analytic kernels, we show
that the L1 geometry gives improved testing power for scalable computational
procedures. Specifically, we derive a finite dimensional approximation of the
metric given as the `1 norm of a vector which captures differences of expectations
of analytic functions evaluated at spatial locations or frequencies (i.e, features).
The features can be chosen to maximize the differences of the distributions and
give interpretable indications of how they differs. Using an `1 norm gives better
detection because differences between representatives are dense as we use analytic
kernels (non-zero almost everywhere). The tests are consistent, while much faster
than state-of-the-art quadratic-time kernel-based tests. Experiments on artificial
and real-world problems demonstrate improved power/time tradeoff than the state
of the art, based on `2 norms, and in some cases, better outright power than even
the most expensive quadratic-time tests},
	author = {Varoquaux, Gael and others},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2020-04-27 08:00:12 -0400},
	date-modified = {2020-05-21 14:57:26 -0400},
	keywords = {statistical test, out of distribution},
	pages = {12306--12316},
	read = {1},
	title = {Comparing distributions: $$\backslash$ell\_1 $ geometry improves kernel two-sample testing},
	year = {2019},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/9398-comparing-distributions-ell_1-geometry-improves-kernel-two-sample-testing.pdf}}

@article{josse2019consistency,
	abstract = {In many application settings, the data have missing features which
make data analysis challenging. An abundant literature addresses
missing data in an inferential framework: estimating parameters and
their variance from incomplete tables. Here, we consider supervisedlearning settings: predicting a target when missing values appear in
both training and testing data.
We show the consistency of two approaches in prediction. A striking
result is that the widely-used method of imputing with the mean prior
to learning is consistent when missing values are not informative. This
contrasts with inferential settings where mean imputation is pointed
at for distorting the distribution of the data. That such a simple
approach can be consistent is important in practice. We also show that
a predictor suited for complete observations can predict optimally on
incomplete data, through multiple imputation.
We analyze further decision trees. These can naturally tackle empirical risk minimization with missing values, due to their ability to
handle the half-discrete nature of incomplete variables. After comparing theoretically and empirically different missing values strategies
in trees, we recommend using the ``missing incorporated in attribute''
method as it can handle both non-informative and informative missing
values.},
	author = {Josse, Julie and Prost, Nicolas and Scornet, Erwan and Varoquaux, Ga{\"e}l},
	date-added = {2020-04-27 07:55:03 -0400},
	date-modified = {2020-04-27 07:55:31 -0400},
	journal = {arXiv preprint arXiv:1902.06931},
	keywords = {missing data, tree, supervised learning},
	title = {On the consistency of supervised learning with missing values},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1902.06931.pdf}}

@article{gonccalves2014comparative,
	author = {Gon{\c{c}}alves Jr, Paulo M and de Carvalho Santos, Silas GT and Barros, Roberto SM and Vieira, Davi CL},
	date-added = {2020-04-24 09:55:22 -0400},
	date-modified = {2020-04-24 09:56:27 -0400},
	journal = {Expert Systems with Applications},
	keywords = {concept drift, experiments design},
	number = {18},
	pages = {8144--8156},
	publisher = {Elsevier},
	title = {A comparative study on concept drift detectors},
	volume = {41},
	year = {2014},
	Bdsk-Url-1 = {https://www.researchgate.net/profile/Roberto_Barros/publication/264081451_A_Comparative_Study_on_Concept_Drift_Detectors/links/5acf5340a6fdcc87840fd665/A-Comparative-Study-on-Concept-Drift-Detectors.pdf}}

@inproceedings{zhao2019visual,
	abstract = {Technical and fundamental analysis are traditional tools
used to analyze stocks; however, the finance literature has
shown that the price movement of each individual stock is
highly correlated with that of other stocks, especially those
within the same sector. In this paper we propose a generalpurpose market representation that incorporates fundamental
and technical indicators and relationships between individual
stocks. We treat the daily stock market as a `market image'
where rows (grouped by market sector) represent individual
stocks and columns represent indicators. We apply a convolutional neural network over this market image to build market features in a hierarchical way. We use a recurrent neural
network, with an attention mechanism over the market feature maps, to model temporal dynamics in the market. Our
model outperforms strong baselines in both short-term and
long-term stock return prediction tasks. We also show another
use for our market image: to construct concise and dense market embeddings suitable for downstream prediction tasks.},
	author = {Zhao, Ran and Deng, Yuntian and Dredze, Mark and Verma, Arun and Rosenberg, David and Stent, Amanda},
	booktitle = {The Thirty-Second International Flairs Conference},
	date-added = {2020-04-23 13:26:14 -0400},
	date-modified = {2020-05-21 14:55:55 -0400},
	keywords = {time series, forecasting, cnn, rnn, attention},
	read = {1},
	title = {Visual Attention Model for Cross-sectional Stock Return Prediction and End-to-End Multimodal Market Representation Learning},
	year = {2019},
	Bdsk-Url-1 = {http://www.cs.jhu.edu/~mdredze/publications/2019_zhao_flairs.pdf}}

@article{bai2018empirical,
	abstract = {For most deep learning practitioners, sequence
modeling is synonymous with recurrent networks.
Yet recent results indicate that convolutional architectures can outperform recurrent networks on
tasks such as audio synthesis and machine translation. Given a new sequence modeling task or
dataset, which architecture should one use? We
conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence
modeling. The models are evaluated across a
broad range of standard tasks that are commonly
used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks
such as LSTMs across a diverse range of tasks
and datasets, while demonstrating longer effective
memory. We conclude that the common association between sequence modeling and recurrent
networks should be reconsidered, and convolutional networks should be regarded as a natural
starting point for sequence modeling tasks. To
assist related work, we have made code available
at http://github.com/locuslab/TCN},
	annote = {They provide code for a Temporal Convolutional Network unit at
https://github.com/locuslab/TCN

They compare TCN with LSTM and GRU and show that TCN does a better job at dealing with sequences.
Highly cited paper.},
	author = {Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
	date-added = {2020-04-23 13:00:56 -0400},
	date-modified = {2020-05-21 14:43:21 -0400},
	journal = {arXiv preprint arXiv:1803.01271},
	keywords = {cnn, rnn, time series, code},
	read = {1},
	title = {An empirical evaluation of generic convolutional and recurrent networks for sequence modeling},
	year = {2018},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1803.01271.pdf}}

@article{borovykh2017conditional,
	author = {Borovykh, Anastasia and Bohte, Sander and Oosterlee, Cornelis W},
	date-added = {2020-04-23 12:46:31 -0400},
	date-modified = {2020-05-21 14:59:02 -0400},
	journal = {arXiv preprint arXiv:1703.04691},
	keywords = {time series, forecasting, cnn},
	read = {1},
	title = {Conditional time series forecasting with convolutional neural networks},
	year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1703.04691.pdf?source=post_page---------------------------}}

@article{pearl2009causal,
	abstract = {This review presents empirical researchers with recent advances
in causal inference, and stresses the paradigmatic shifts that must be undertaken in moving from traditional statistical analysis to causal analysis of
multivariate data. Special emphasis is placed on the assumptions that underly all causal inferences, the languages used in formulating those assumptions, the conditional nature of all causal and counterfactual claims, and
the methods that have been developed for the assessment of such claims.
These advances are illustrated using a general theory of causation based
on the Structural Causal Model (SCM) described in Pearl (2000a), which
subsumes and unifies other approaches to causation, and provides a coherent mathematical foundation for the analysis of causes and counterfactuals.
In particular, the paper surveys the development of mathematical tools for
inferring (from a combination of data and assumptions) answers to three
types of causal queries: (1) queries about the effects of potential interventions, (also called ``causal effects'' or ``policy evaluation'') (2) queries about
probabilities of counterfactuals, (including assessment of ``regret,'' ``attribution'' or ``causes of effects'') and (3) queries about direct and indirect
effects (also known as ``mediation''). Finally, the paper defines the formal
and conceptual relationships between the structural and potential-outcome
frameworks and presents tools for a symbiotic analysis that uses the strong
features of both.},
	author = {Pearl, Judea and others},
	date-added = {2020-04-23 11:08:59 -0400},
	date-modified = {2020-05-21 14:44:04 -0400},
	journal = {Statistics surveys},
	keywords = {causality, structural causal model},
	pages = {96--146},
	publisher = {The author, under a Creative Commons Attribution License},
	read = {1},
	title = {Causal inference in statistics: An overview},
	volume = {3},
	year = {2009},
	Bdsk-Url-1 = {https://projecteuclid.org/download/pdfview_1/euclid.ssu/1255440554}}

@article{yao2020survey,
	abstract = {Causal inference is a critical research topic across many domains, such as statistics, computer science, education, public
policy and economics, for decades. Nowadays, estimating causal effect from observational data has become an appealing
research direction owing to the large amount of available data and low budget requirement, compared with randomized
controlled trials. Embraced with the rapidly developed machine learning area, various causal effect estimation methods for
observational data have sprung up. In this survey, we provide a comprehensive review of causal inference methods under the
potential outcome framework, one of the well known causal inference framework. The methods are divided into two categories
depending on whether they require all three assumptions of the potential outcome framework or not. For each category,
both the traditional statistical methods and the recent machine learning enhanced methods are discussed and compared.
The plausible applications of these methods are also presented, including the applications in advertising, recommendation,
medicine and so on. Moreover, the commonly used benchmark datasets as well as the open-source codes are also summarized,
which facilitate researchers and practitioners to explore, evaluate and apply the causal inference methods.},
	annote = {In that review paper, the methods are classified into 2 categories whether they rely on 3 assumptions (re-weighting methods, matching methods, tree-based methods, representation learning, multitask learning methods, meta learning methods), or if they relax some of them. 
These 3 assumptions are: 
* SUTVA (independence of the units receiving treatment, and unicity of treatments administred), 
* ignorability (treatment assignment done independently from expected outcome; so random), and 
* positivity (treatment assignmed randomly for all background).},
	author = {Yao, Liuyi and Chu, Zhixuan and Li, Sheng and Li, Yaliang and Gao, Jing and Zhang, Aidong},
	date-added = {2020-04-23 11:07:24 -0400},
	date-modified = {2020-04-28 13:16:33 -0400},
	journal = {arXiv preprint arXiv:2002.02770},
	keywords = {causality, potential outcome framework},
	read = {1},
	title = {A Survey on Causal Inference},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2002.02770.pdf}}

@inproceedings{sen2019think,
	abstract = {Forecasting high-dimensional time series plays a crucial role in many applications
such as demand forecasting and financial predictions. Modern datasets can have
millions of correlated time-series that evolve together, i.e they are extremely high
dimensional (one dimension for each individual time-series). There is a need
for exploiting global patterns and coupling them with local calibration for better
prediction. However, most recent deep learning approaches in the literature are
one-dimensional, i.e, even though they are trained on the whole dataset, during
prediction, the future forecast for a single dimension mainly depends on past values
from the same dimension. In this paper, we seek to correct this deficiency and
propose DeepGLO, a deep forecasting model which thinks globally and acts
locally. In particular, DeepGLO is a hybrid model that combines a global matrix
factorization model regularized by a temporal convolution network, along with
another temporal network that can capture local properties of each time-series and
associated covariates. Our model can be trained effectively on high-dimensional
but diverse time series, where different time series can have vastly different scales,
without a priori normalization or rescaling. Empirical results demonstrate that
DeepGLO can outperform state-of-the-art approaches; for example, we see more
than 25% improvement in WAPE over other methods on a public dataset that
contains more than 100K-dimensional time series.},
	author = {Sen, Rajat and Yu, Hsiang-Fu and Dhillon, Inderjit S},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2020-04-23 10:47:34 -0400},
	date-modified = {2020-05-21 14:59:23 -0400},
	keywords = {time series, matrix factorization, cnn, high dimension, rnn},
	pages = {4838--4847},
	read = {1},
	title = {Think globally, act locally: A deep neural network approach to high-dimensional time series forecasting},
	year = {2019},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/8730-think-globally-act-locally-a-deep-neural-network-approach-to-high-dimensional-time-series-forecasting.pdf}}

@inproceedings{yu2016temporal,
	abstract = {Time series prediction problems are becoming increasingly high-dimensional in
modern applications, such as climatology and demand forecasting. For example,
in the latter problem, the number of items for which demand needs to be forecast
might be as large as 50,000. In addition, the data is generally noisy and full of
missing values. Thus, modern applications require methods that are highly scalable,
and can deal with noisy data in terms of corruptions or missing values. However,
classical time series methods usually fall short of handling these issues. In this
paper, we present a temporal regularized matrix factorization (TRMF) framework
which supports data-driven temporal learning and forecasting. We develop novel
regularization schemes and use scalable matrix factorization methods that are
eminently suited for high-dimensional time series data that has many missing values.
Our proposed TRMF is highly general, and subsumes many existing approaches
for time series analysis. We make interesting connections to graph regularization
methods in the context of learning the dependencies in an autoregressive framework.
Experimental results show the superiority of TRMF in terms of scalability and
prediction quality. In particular, TRMF is two orders of magnitude faster than
other methods on a problem of dimension 50,000, and generates better forecasts on
real-world datasets such as Wal-mart E-commerce datasets.},
	annote = {Interesting matrix factorization technique that allows to define (or infer) a temporal dynamics for the latent variables (e.g., AR),
yielding a causal dimensionality reduction.
They apply their method to forecasting or missing values imputation.

LImitations:
* It only captures linear relationships
* you have to re-solve an optimization problem every time you want to append new data and therefore
need to compute their corresponding value in the latent space (see ``Updates for X'' in section 4). The optimization problem is
much smaller though.},
	author = {Yu, Hsiang-Fu and Rao, Nikhil and Dhillon, Inderjit S},
	booktitle = {Advances in neural information processing systems},
	date-added = {2020-04-23 09:31:20 -0400},
	date-modified = {2020-05-21 14:56:42 -0400},
	keywords = {time series, matrix factorization, high dimension},
	pages = {847--855},
	read = {1},
	title = {Temporal regularized matrix factorization for high-dimensional time series prediction},
	year = {2016},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/6160-temporal-regularized-matrix-factorization-for-high-dimensional-time-series-prediction.pdf}}

@inproceedings{chen2018dimensionality,
	author = {Chen, Minshuo and Yang, Lin and Wang, Mengdi and Zhao, Tuo},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2020-04-23 08:29:38 -0400},
	date-modified = {2020-04-23 08:30:10 -0400},
	keywords = {optimization, stochastic, nonconvex, time series},
	pages = {3496--3506},
	title = {Dimensionality reduction for stationary time series via stochastic nonconvex optimization},
	year = {2018},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/7609-dimensionality-reduction-for-stationary-time-series-via-stochastic-nonconvex-optimization.pdf}}

@article{keogh2001dimensionality,
	abstract = {The problem of similarity search in large time series databases has attracted much attention
recently. It is a non-trivial problem because of the inherent high dimensionality of the data. The
most promising solutions involve performing dimensionality reduction on the data, then indexing
the reduced data with a spatial access method. Three major dimensionality reduction techniques
have been proposed, Singular Value Decomposition (SVD), the Discrete Fourier transform
(DFT), and more recently the Discrete Wavelets Transform (DWT). In this work we introduce a
new dimensionality reduction technique which we call PAA (Piecewise Aggregate
Approximation). We theoretically and empirically compare it to the other techniques and
demonstrate its superiority. In addition to being competitive with or faster than the other methods
our approach has numerous advantages. It is simple to understand and implement, allows more
flexible distance measures including weighted Euclidean queries and the index can be built in
linear time.},
	annote = {Classic paper to do rapid indexing of large time series database.
Introduce PIecewise Aggregate Approximation which sub-samples the time series and
do similarity search on these sub-sampled time series.},
	author = {Keogh, Eamonn and Chakrabarti, Kaushik and Pazzani, Michael and Mehrotra, Sharad},
	date-added = {2020-04-22 17:54:05 -0400},
	date-modified = {2020-05-21 14:58:30 -0400},
	journal = {Knowledge and information Systems},
	keywords = {time series, indexing, high dimension, dimensionality reduction},
	number = {3},
	pages = {263--286},
	publisher = {Springer},
	read = {1},
	title = {Dimensionality reduction for fast similarity search in large time series databases},
	volume = {3},
	year = {2001},
	Bdsk-Url-1 = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/time_series_indexing.pdf}}

@article{duan2019ngboost,
	abstract = {We present Natural Gradient Boosting (NGBoost), an algorithm for generic probabilistic
prediction via gradient boosting. Typical regression models return a point estimate, conditional on covariates, but probabilistic regression
models output a full probability distribution over
the outcome space, conditional on the covariates. This allows for predictive uncertainty estimation --- crucial in applications like healthcare and weather forecasting. NGBoost generalizes gradient boosting to probabilistic regression
by treating the parameters of the conditional distribution as targets for a multiparameter boosting algorithm. Furthermore, we show how the
Natural Gradient is required to correct the training dynamics of our multiparameter boosting approach. NGBoost can be used with any base
learner, any family of distributions with continuous parameters, and any scoring rule. NGBoost
matches or exceeds the performance of existing
methods for probabilistic prediction while offering additional benefits in flexibility, scalability,
and usability.},
	author = {Duan, Tony and Avati, Anand and Ding, Daisy Yi and Basu, Sanjay and Ng, Andrew Y and Schuler, Alejandro},
	date-added = {2020-04-22 10:24:24 -0400},
	date-modified = {2020-06-03 23:45:18 -0400},
	journal = {arXiv preprint arXiv:1910.03225},
	keywords = {time series, distributional forecast, forecasting},
	read = {1},
	title = {NGBoost: Natural Gradient Boosting for Probabilistic Prediction},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1910.03225.pdf}}

@inproceedings{verleysen2005curse,
	abstract = {Modern data analysis tools have to work on high-dimensional data,
whose components are not independently distributed. High-dimensional spaces
show surprising, counter-intuitive geometrical properties that have a large
influence on the performances of data analysis tools. Among these properties,
the concentration of the norm phenomenon results in the fact that Euclidean
norms and Gaussian kernels, both commonly used in models, become
inappropriate in high-dimensional spaces. This papers presents alternative
distance measures and kernels, together with geometrical methods to decrease
the dimension of the space. The methodology is applied to a typical time series
prediction example. },
	author = {Verleysen, Michel and Fran{\c{c}}ois, Damien},
	booktitle = {International Work-Conference on Artificial Neural Networks},
	date-added = {2020-04-22 10:19:26 -0400},
	date-modified = {2020-04-22 10:20:13 -0400},
	keywords = {time series, data mining, high dimension},
	organization = {Springer},
	pages = {758--770},
	title = {The curse of dimensionality in data mining and time series prediction},
	year = {2005},
	Bdsk-Url-1 = {http://neuro.bstu.by/ai/To-dom/My_research/Papers-0/For-research/D-mining/Stock-market/iwann05mv.pdf}}

@article{xue2014jump,
	abstract = {This paper introduces a new nonparametric test to identify jump arrival times in high frequency
financial time series data. The asymptotic distribution of the test is derived. We demonstrate that the
test is robust for different specifications of price processes and the presence of the microstructure
noise. A Monte Carlo simulation is conducted which shows that the test has good size and power.
Further, we examine the multi-scale jump dynamics in US equity markets. The main findings are as
follows. First, the jump dynamics of equities are sensitive to data sampling frequency with significant
underestimation of jump intensities at lower frequencies. Second, although arrival densities of positive
jumps and negative jumps are symmetric across different time scales, the magnitude of jumps is
distributed asymmetrically at high frequencies. Third, only 20% of jumps occur in the trading session
from 9:30AM to 4:00 PM, suggesting that illiquidity during after-hours trading is a strong determinant
of jumps.},
	author = {Xue, Yi and Gencay, Ramazan and Fagan, Stephen},
	date-added = {2020-04-22 10:17:32 -0400},
	date-modified = {2020-04-22 10:18:04 -0400},
	journal = {Quantitative Finance},
	keywords = {time series, change point detection, wavelet},
	number = {8},
	pages = {1427--1444},
	publisher = {Taylor \& Francis},
	title = {Jump detection with wavelets for high-frequency financial time series},
	volume = {14},
	year = {2014},
	Bdsk-Url-1 = {https://www.researchgate.net/profile/Yi_Xue2/publication/268152725_Jump_detection_with_wavelets_for_high-frequency_financial_time_series/links/54630bad0cf2c0c6aec1c47a/Jump-detection-with-wavelets-for-high-frequency-financial-time-series.pdf}}

@article{detommaso2019stein,
	abstract = {Bayesian online changepoint detection (BOCPD) [1] offers a rigorous and viable
way to identify changepoints in complex systems. In this work, we introduce
a Stein variational online changepoint detection (SVOCD) method to provide a
computationally tractable generalization of BOCPD beyond the exponential family
of probability distributions. We integrate the recently developed Stein variational
Newton (SVN) method [5] and BOCPD to offer a full online Bayesian treatment for
a large number of situations with significant importance in practice. We apply the
resulting method to two challenging and novel applications: Hawkes processes and
long short-term memory (LSTM) neural networks. In both cases, we successfully
demonstrate the efficacy of our method on real data.},
	author = {Detommaso, Gianluca and Hoitzing, Hanne and Cui, Tiangang and Alamir, Ardavan},
	date-added = {2020-04-22 10:16:52 -0400},
	date-modified = {2020-06-04 12:50:41 -0400},
	journal = {arXiv preprint arXiv:1901.07987},
	keywords = {time series, change point detection, bayesian},
	read = {1},
	title = {Stein variational online changepoint detection with applications to Hawkes processes and neural networks},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1901.07987.pdf}}

@article{aminikhanghahi2017survey,
	author = {Aminikhanghahi, Samaneh and Cook, Diane J},
	date-added = {2020-04-22 10:15:29 -0400},
	date-modified = {2020-04-22 10:15:49 -0400},
	journal = {Knowledge and information systems},
	keywords = {time series, change point detection},
	number = {2},
	pages = {339--367},
	publisher = {Springer},
	title = {A survey of methods for time series change point detection},
	volume = {51},
	year = {2017},
	Bdsk-Url-1 = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5464762/pdf/nihms860556.pdf}}

@inproceedings{keogh2001online,
	abstract = {In recent years, there has been an explosion of interest in mining time series databases. As with most
computer science problems, representation of the data is the key to efficient and effective solutions. One of
the most commonly used representations is piecewise linear approximation. This representation has been
used by various researchers to support clustering, classification, indexing and association rule mining of
time series data. A variety of algorithms have been proposed to obtain this representation, with several
algorithms having been independently rediscovered several times. In this paper, we undertake the first
extensive review and empirical comparison of all proposed techniques. We show that all these algorithms
have fatal flaws from a data mining perspective. We introduce a novel algorithm that we empirically show
to be superior to all others in the literature.},
	author = {Keogh, Eamonn and Chu, Selina and Hart, David and Pazzani, Michael},
	booktitle = {Proceedings 2001 IEEE international conference on data mining},
	date-added = {2020-04-22 10:13:24 -0400},
	date-modified = {2020-04-22 10:28:58 -0400},
	keywords = {time series, online algorithm, data mining},
	organization = {IEEE},
	pages = {289--296},
	title = {An online algorithm for segmenting time series},
	year = {2001},
	Bdsk-Url-1 = {https://sfb876.tu-dortmund.de/PublicPublicationFiles/keogh_etal_2001a.pdf}}

@article{martinsson2020randomized,
	author = {Martinsson, Per-Gunnar and Tropp, Joel},
	date-added = {2020-04-22 10:10:33 -0400},
	date-modified = {2020-05-21 14:56:55 -0400},
	journal = {arXiv preprint arXiv:2002.01387},
	keywords = {numerical linear algebra, randomized, algorithm},
	read = {1},
	title = {Randomized numerical linear algebra: Foundations \& algorithms},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2002.01387.pdf}}

@article{demvsar2006statistical,
	abstract = {While methods for comparing two learning algorithms on a single data set have been scrutinized for
quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple
data sets, which is even more essential to typical machine learning studies, has been all but ignored.
This article reviews the current practice and then theoretically and empirically examines several
suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric
tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of
two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more
classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly
introduced CD (critical difference) diagrams.},
	annote = {2. Previous work
``None of the above studies deal with evaluating the performance of multiple classifiers and neither studies the applicability of the statistics when classifiers are tested over multiple data sets.''

``The most straightforward way to compare classifiers is to compute the average over all data sets; such averaging appears naive and is seldom used''

3. Statistics and Tests for Comparison of Classifiers
`` The only requirement is that the compiled results provide reliable estimates of the algorithms' performance on each data set. In the usual experimental setups, these numbers come from cross-validation or from repeated stratified random splits onto training and testing data sets.''

``In our task, multiple resampling from each data set is used only to assess the performance score and not its variance. The sources of the variance are the differences in performance over (independent) data sets and not on (usually dependent) samples, so the elevated Type 1 error is not an issue. Since multiple resampling does not bias the score estimation, various types of crossvalidation or leave-one-out procedures can be used without any risk.''

In this paper, Demsar assumes that the performance over each data set has been calculated faithfully and in unbiased manner. How this is done is opened for debate. He mentions a few classical techniques, mainly cross-validation or random resampling.

``Since running the algorithms on multiple data sets naturally gives a sample of independent measurements, such comparisons are even simpler than comparisons on a single data set.''
-> that makes it more complicated to use these approches for time-series backtesting (or sequential validation)

3.1 Comparison of Two Classifiers

In that section, they compare different tests to compare 2 classifiers on multiple datasets.
One issue they need to consider is that different data sets do not return measures that are all comparable (commensurability). Note that this may be the case for sequential validation (e.g., financial crisis).

3.1.1 Averaging over data sets
Generally not recommended

3.1.2 Paired t-test
''In our context, the t-test suffers from three weaknesses. The first is commensurability: the t-test only makes sense when the differences over the data sets are commensurate'' ( measurable by the same standard)
''The second problem with the t-test is that unless the sample size is large enough ( 30 data sets), the paired t-test requires that the differences between the two random variables compared are distributed normally''
``The third problem is that the t-test is, just as averaging over data sets, affected by outliers''

3.1.3 Wilcoxon signed-rank tests
More general that t-test. Better power when conditions of t-test not met.

3.1.4 Counts of wins, losses and ties: sign test
Binomial test on the number of wins for each model

3.2 Comparisons of Multiple Classifiers

``When so many tests are made, a certain proportion of the null hypotheses is rejected due to random chance, so listing them makes little sense. The issue of multiple hypothesis testing is a well-known statistical problem'': repeating a pair test makes not statistical sense

3.2.1 ANOVA
``repeated-measures ANOVA (or within-subjects ANOVA)''
``The ``related samples'' are again the performances of the classifiers measured across the same data sets, preferably using the same splits onto training and testing sets. The null-hypothesis being tested is that all classifiers perform the same and the observed differences are merely random.''

If the null (all models perform the same) is rejected, ``we can proceed with a post-hoc test to find out which classifiers actually differ. Of many such tests for ANOVA, the two most suitable for our situation are the Tukey test (Tukey, 1949) for comparing all classifiers with each other and the Dunnett test (Dunnett, 1980) for comparisons of all classifiers with the control (for instance, comparing the base classifier and some proposed improvements, or comparing the newly proposed classifier with several existing methods).''
Both methods resemble a t-test but with critcial values adjusted to account for the multiple testing.

Assumptions for ANOVA:
- normal samples
- same variance for all samples -> has even greater effect on post-hoc analysis

3.2.2 Friedman test
Non-parametric equivalent of ANOVA.
Rank all algorithms for each dataset, then compare average ranks of each models.

Post-hoc test: Nemenyi test
Less powerful than Friedman test. So in some cases, you make reject the null with the Friedman test, but fails to identify what methods differ.

Note: could run Friedman + Nemenyi, or could pick the best model as control then run Friedman + post-hoc test in the case of a control. Can use Bonferonni-Dunn test, or Holm test.
``When all classifiers are compared with a control classifier, we can instead of the Nemenyi test use one of the general procedures for controlling the family-wise error in multiple hypothesis testing, such as the Bonferroni correction or similar procedures. Although these methods are generally conservative and can have little power, they are in this specific case more powerful than the Nemenyi test, since the latter adjusts the critical value for making k(k1)/2 comparisons while when comparing with a control we only make k 1 comparisons.''

``Holm's procedure is more powerful than the Bonferroni-Dunn's and makes no additional assumptions about the hypotheses tested. The only advantage of the Bonferroni-Dunn test seems to be that it is easier to describe and visualize because it uses the same CD for all comparisons. In turn, Hochberg's and Hommel's methods reject more hypotheses than Holm's, yet they may under some circumstances exceed the prescribed family-wise error since they are based on the Simes conjecture which is still being investigated. It has been reported (Holland, 1991) that the differences between the enhanced methods are in practice rather small, therefore the more complex Hommel method offers no great advantage over the simple Holm method.''

4. Empirical Comparison of Tests
},
	author = {Dem{\v{s}}ar, Janez},
	date-added = {2020-04-21 11:53:43 -0400},
	date-modified = {2020-12-11 00:02:09 -0500},
	journal = {Journal of Machine learning research},
	keywords = {classification, statistical test, model selection, cross validation},
	number = {Jan},
	pages = {1--30},
	read = {1},
	title = {Statistical comparisons of classifiers over multiple data sets},
	volume = {7},
	year = {2006},
	Bdsk-Url-1 = {http://www.jmlr.org/papers/volume7/demsar06a/demsar06a.pdf}}

@article{pearl2019seven,
	author = {Pearl, Judea},
	date-added = {2020-04-21 09:34:41 -0400},
	date-modified = {2020-05-21 14:56:10 -0400},
	journal = {Communications of the ACM},
	keywords = {causality},
	number = {3},
	pages = {54--60},
	publisher = {ACM New York, NY, USA},
	read = {1},
	title = {The seven tools of causal inference, with reflections on machine learning},
	volume = {62},
	year = {2019},
	Bdsk-Url-1 = {http://ftp.cs.ucla.edu/pub/stat_ser/r481.pdf}}

@webpage{palachy2019,
	annote = {re-read quickly, then move on -> need to decide on what to read

probably want to focus on graphical models
1) DynoTEARS
2) PCMCI


Brain vomit:

Long, extensive, and interesting blog post. It tries to list all different methods to estimate causality in time series data.

All methods could be grouped in 2 categories:
* test based: runing statistical test to determinie if ts1 Granger-causes ts2
* graph based: generate a Bayesian NN 

There would need to be a summary table of the different methods with their strengths/weaknesses -> time consuming + is it the best approach? Should I just focus on PCMCI

A couple of references stand out from that blog post:
- Papana et al., 2013: file:///Users/bencrestel/Downloads/entropy-15-02635.pdf
- the work of Runge: for graphical model, check out PMCI (https://advances.sciencemag.org/content/advances/5/11/eaau4996.full.pdf / https://github.com/jakobrunge/tigramite)


Focus on graphical approaches immediately? Probably need a rationale to do so -> study other methods briefly, or check with Alex

What is the advantage of graphical methods over statistical tests?
* Granger causality: popular method due to its computational simplicity
* Graphical approach: often used to model Granger causality in multivariate setting

Connection stationarity - Causality??},
	date-added = {2020-04-20 23:13:21 -0400},
	date-modified = {2020-04-28 13:16:17 -0400},
	keywords = {causality, time series},
	lastchecked = {Apr. 2020},
	month = {Nov},
	read = {1},
	url = {https://towardsdatascience.com/inferring-causality-in-time-series-data-b8b75fe52c46#99db},
	year = {2019},
	Bdsk-Url-1 = {https://towardsdatascience.com/inferring-causality-in-time-series-data-b8b75fe52c46#99db}}

@unpublished{haugh2015,
	author = {Martin Haugh},
	date-added = {2020-04-20 07:42:28 -0400},
	date-modified = {2020-05-21 14:43:36 -0400},
	keywords = {EM, algorithm, class notes},
	note = {IEOR E4570: Machine Learning for OR&FE, Columbia},
	read = {1},
	title = {The EM Algorithm},
	year = {2015},
	Bdsk-Url-1 = {http://www.columbia.edu/~mh2078/MachineLearningORFE/EM_Algorithm.pdf}}

@article{scholkopf2019causality,
	abstract = {Graphical causal inference as pioneered by Judea Pearl arose from research on artificial intelligence
(AI), and for a long time had little connection to the field of machine learning. This article discusses
where links have been and should be established, introducing key concepts along the way. It argues
that the hard open problems of machine learning and AI are intrinsically related to causality, and
explains how the field is beginning to understand them.},
	author = {Sch{\"o}lkopf, Bernhard},
	date-added = {2020-04-06 14:36:14 -0400},
	date-modified = {2020-05-21 14:55:31 -0400},
	journal = {arXiv preprint arXiv:1911.10500},
	keywords = {causality},
	read = {1},
	title = {Causality for Machine Learning},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1911.10500.pdf}}

@incollection{ralanamahatana2005mining,
	abstract = {Much of the world's supply of data is in the form of time series. In the last decade,
there has been an explosion of interest in mining time series data. A number of new algorithms have been introduced to classify, cluster, segment, index, discover rules, and detect
anomalies/novelties in time series. While these many different techniques used to solve these
problems use a multitude of different techniques, they all have one common factor; they require some high level representation of the data, rather than the original raw data. These high
level representations are necessary as a feature extraction step, or simply to make the storage,
transmission, and computation of massive dataset feasible. A multitude of representations have
been proposed in the literature, including spectral transforms, wavelets transforms, piecewise
polynomials, eigenfunctions, and symbolic mappings. This chapter gives a high-level survey
of time series Data Mining tasks, with an emphasis on time series representations.},
	annote = {* Good high-level overview of data mining approaches for time series:
indexing, clustering, classification, prediction, summarization, anomaly detection, segmentation.
* For every approach, the treatment is very superficial though.},
	author = {Ralanamahatana, Chotirat Ann and Lin, Jessica and Gunopulos, Dimitrios and Keogh, Eamonn and Vlachos, Michail and Das, Gautam},
	booktitle = {Data mining and knowledge discovery handbook},
	date-added = {2020-03-26 15:42:44 -0400},
	date-modified = {2020-05-21 14:58:19 -0400},
	keywords = {time series, algorithm, data mining},
	pages = {1069--1103},
	publisher = {Springer},
	read = {1},
	title = {Mining time series data},
	year = {2005},
	Bdsk-Url-1 = {https://www.researchgate.net/profile/Chotirat_Ratanamahatana2/publication/227001229_Mining_Time_Series_Data/links/02bfe51188657160b5000000.pdf}}

@inproceedings{ren2019likelihood,
	abstract = {Discriminative neural networks offer little or no performance guarantees when
deployed on data not generated by the same process as the training distribution. On
such out-of-distribution (OOD) inputs, the prediction may not only be erroneous,
but confidently so, limiting the safe deployment of classifiers in real-world applications. One such challenging application is bacteria identification based on genomic
sequences, which holds the promise of early detection of diseases, but requires a
model that can output low confidence predictions on OOD genomic sequences from
new bacteria that were not present in the training data. We introduce a genomics
dataset for OOD detection that allows other researchers to benchmark progress on
this important problem. We investigate deep generative model based approaches
for OOD detection and observe that the likelihood score is heavily affected by
population level background statistics. We propose a likelihood ratio method for
deep generative models which effectively corrects for these confounding background statistics. We benchmark the OOD detection performance of the proposed
method against existing approaches on the genomics dataset and show that our
method achieves state-of-the-art performance. We demonstrate the generality of
the proposed method by showing that it significantly improves OOD detection
when applied to deep generative models of images.},
	annote = {Research stemmed from a OOD problem in genomic, and they try to generalize it to image classification. This project stems from the limitation of existing generative-based methods for OOD; they show examples in both domains where the likelihood OOD is at least as confident as in-distribution.
Their solution is to split input into background and semantic (independence assumption), and apply a likelihood ration only to the semantic part.

It's not clear why their assumptions would be valid outside (even inside) of genomics. For instance, backgroud play a big role in confounding image classifiers (e.g., wolf in front of grass).

However, there is a nice section at the beginning of section 4 that summarizes the ``Baseline methods for comparison'':
1. The maximum class probability, p(y|x) = maxk p(y = k|x). OOD inputs tend to have lower
scores than in-distribution data (Hendrycks & Gimpel, 2016).
2. The entropy of the predicted class distribution,  P_k p(y = k|x) log p(y = k|x). High entropy
of the predicted class distribution, and therefore a high predictive uncertainty, which suggests that
the input may be OOD.
3. The ODIN method proposed by Liang et al. (2017). ODIN uses temperature scaling (Guo et al.,
2017), adds small perturbations to the input, and applies a threshold to the resulting predicted
class to distinguish in- and out-of- distribution inputs. This method was designed for continuous
inputs and cannot be directly applied to discrete genomic sequences. We propose instead to add
perturbations to the input of the last layer that is closest to the output of the neural network.
4. The Mahalanobis distance of the input to the nearest class-conditional Gaussian distribution estimated from the in-distribution data. Lee et al. (2018) fit class-conditional Gaussian distributions
to the activations from the last layer of the neural network.
5. The classifier-based ensemble method that uses the average of the predictions from multiple
independently trained models with random initialization of network parameters and random
shuffling of training inputs (Lakshminarayanan et al., 2017).
6. The log-odds of a binary classifier trained to distinguish between in-distribution inputs from all
classes as one class and randomly perturbed in-distribution inputs as the other.
7. The maximum class probability over K in-distribution classes of a (K + 1)-class classifier where
the additional class is perturbed in-distribution.
8. The maximum class probability of a K-class classifier for in-distribution classes but the predicted
class distribution is explicitly trained to output uniform distribution on perturbed in-distribution
inputs. This is similar to using simulated OOD inputs from GAN (Lee et al., 2017) or using
auxiliary datasets of outliers (Hendrycks et al., 2018) for calibration purpose.
9. The generative model-based ensemble method that measures E[log p(x)]  Var[log p(x)] of
multiple likelihood estimations from independently trained model with random initialization and
random shuffling of the inputs. (Choi et al., 2018).},
	author = {Ren, Jie and Liu, Peter J and Fertig, Emily and Snoek, Jasper and Poplin, Ryan and Depristo, Mark and Dillon, Joshua and Lakshminarayanan, Balaji},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2020-03-26 14:41:15 -0400},
	date-modified = {2020-04-22 10:28:20 -0400},
	keywords = {out of distribution, generative model, deep learning},
	pages = {14680--14691},
	read = {1},
	title = {Likelihood ratios for out-of-distribution detection},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1906.02845.pdf}}

@article{tonekaboni2020went,
	abstract = {Multivariate time series models are poised to be
used for decision support in high-stakes applications, such as healthcare. In these contexts, it is
important to know which features at which times
most influenced a prediction. We demonstrate
a general approach for assigning importance to
observations in multivariate time series, based on
their counterfactual influence on future predictions. Specifically, we define the importance of an
observation as the change in the predictive distribution, had the observation not been seen. We integrate over plausible counterfactuals by sampling
from the corresponding conditional distributions
of generative time series models. We compare our
importance metric to gradient-based explanations,
attention mechanisms, and other baselines in simulated and clinical ICU data, and show that our
approach generates the most precise explanations.
Our method is inexpensive, model agnostic, and
can be used with arbitrarily complex time series
models and predictors},
	annote = {Proposes a method to assess the importance of multivariate temporal features. They do so, at each time step, for each feature, by measuring the KL divergence between the exact predictive distribution and the one you would obtain if you were to not observe that feature at that time step. To replace the feature (they still need it to derive a predictive distribution), they sample from a generatitve model they train on all features at previous time.
Problems:
1) you only the importance of a feature at the last time step
2) when sampling the missing feature, they do not account for possible correlation across features at that time step (only at previous time steps).},
	author = {Tonekaboni, Sana and Joshi, Shalmali and Duvenaud, David and Goldenberg, Anna},
	date-added = {2020-03-10 15:02:42 -0400},
	date-modified = {2020-04-22 10:29:31 -0400},
	journal = {arXiv preprint arXiv:2003.02821},
	keywords = {xai, time series, generative model},
	read = {1},
	title = {What went wrong and when? Instance-wise Feature Importance for Time-series Models},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2003.02821.pdf}}

@article{gholamiintegrated,
	abstract = {Finding the right Neural Network model and
training it for a new task requires considerable expertise
and extensive computational resources. Moreover, the process
often includes ad-hoc rules that do not generalize to different
application domains. These issues have limited the applicability
and usefulness of DNN models, especially for new learning
tasks. This problem is becoming more acute, as datasets and
models grow larger, which increases training time, making
random/brute force search approaches quickly untenable. In
large part, this situation is due to the first-order stochastic gradient descent (SGD) methods that are widely-used for training
DNNs. Despite SGD's well-known benefits, vanilla SGD tends
to perform poorly, and thus one introduces many (essentially
ad-hoc) knobs and hyper-parameters to make it work. It has
been found that these hyper-parameters are significantly more
sensitive to tuning in large scale training with SGD, and this
has impeded effective use of supercomputing systems. Here,
we argue that a multi-faceted approach is needed to address
these challenges by considering the full stack of neural network
architecture design, large scale training, and efficient inference
on edge platforms. This requires designing mechanisms to
better understand NN training and bridge the gap between
theoretical results for optimization, second order methods, and
high performance computing.},
	annote = {Gives an overview of the recent work of the 3 authors in Second-order methods and parallel computing. It's more of a marketing paper, but it gathers all their references},
	author = {Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
	date-added = {2020-03-09 10:50:01 -0400},
	date-modified = {2020-04-22 10:27:21 -0400},
	keywords = {Hessian, Newton method, deep learning},
	read = {1},
	title = {An Integrated Approach to Neural Network Design, Training, and Inference},
	year = {2020},
	Bdsk-Url-1 = {http://smartci.sci.utah.edu/images/whitepapers/2020_NSF_White_Paper_UCB.pdf}}

@article{lindsten2017probabilistic,
	abstract = {This is a text on probabilistic modeling for the master level course `Statistical Machine Learning' given at the
Department of Information Technology, Uppsala University during the spring term 2017 and it is a complement
to the course books James et al. (2013) and Hastie et al. (2009). It consists of three chapters and one appendix.
The three chapters cover an introduction to probabilistic modeling, probabilistic (Bayesian) linear regression,
and Gaussian processes. The appendix introduces the multivariate Gaussian distribution and presents key results
needed in the chapters. Consequently, the appendix has an important role in this document and should therefore
be studied carefully.},
	author = {Lindsten, Fredrik and Sch{\"o}n, Thomas B and Svensson, Andreas and Wahlstr{\"o}m, Niklas},
	date-added = {2020-02-20 11:10:47 -0500},
	date-modified = {2020-02-20 11:11:57 -0500},
	journal = {Uppsala: Uppsala University},
	keywords = {gaussian process, distributional forecast},
	read = {1},
	title = {Probabilistic modeling--linear regression \& Gaussian processes},
	year = {2017},
	Bdsk-Url-1 = {http://www.it.uu.se/edu/course/homepage/sml/literature/probabilistic_modeling_compendium.pdf}}

@article{marcus2018deep,
	abstract = {Although deep learning has historical roots going back decades, neither the term ``deep
learning'' nor the approach was popular just over five years ago, when the field was
reignited by papers such as Krizhevsky, Sutskever and Hinton's now classic 2012
(Krizhevsky, Sutskever, & Hinton, 2012)deep net model of Imagenet.
What has the field discovered in the five subsequent years? Against a background of
considerable progress in areas such as speech recognition, image recognition, and game
playing, and considerable enthusiasm in the popular press, I present ten concerns for deep
learning, and suggest that deep learning must be supplemented by other techniques if we
are to reach artificial general intelligence.},
	author = {Marcus, Gary},
	date-added = {2020-02-18 17:37:08 -0500},
	date-modified = {2020-02-18 17:37:54 -0500},
	journal = {arXiv preprint arXiv:1801.00631},
	keywords = {deep learning, symbolic AI},
	read = {1},
	title = {Deep learning: A critical appraisal},
	year = {2018},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1801.00631.pdf}}

@inproceedings{ma2019learning,
	abstract = {Time series clustering is an essential unsupervised technique in cases when category
information is not available. It has been widely applied to genome data, anomaly
detection, and in general, in any domain where pattern detection is important.
Although feature-based time series clustering methods are robust to noise and
outliers, and can reduce the dimensionality of the data, they typically rely on domain
knowledge to manually construct high-quality features. Sequence to sequence
(seq2seq) models can learn representations from sequence data in an unsupervised
manner by designing appropriate learning objectives, such as reconstruction and
context prediction. When applying seq2seq to time series clustering, obtaining a
representation that effectively represents the temporal dynamics of the sequence,
multi-scale features, and good clustering properties remains a challenge. How
to best improve the ability of the encoder is still an open question. Here we
propose a novel unsupervised temporal representation learning model, named
Deep Temporal Clustering Representation (DTCR), which integrates the temporal
reconstruction and K-means objective into the seq2seq model. This approach
leads to improved cluster structures and thus obtains cluster-specific temporal
representations. Also, to enhance the ability of encoder, we propose a fake-sample
generation strategy and auxiliary classification task. Experiments conducted on
extensive time series datasets show that DTCR is state-of-the-art compared to
existing methods. The visualization analysis not only shows the effectiveness of
cluster-specific representation but also shows the learning process is robust, even if
K-means makes mistakes.},
	annote = {Propose to cluster time series in latent space, using a seq2seq model based on multi-layer Dilated RNN, a k-means loss term, and a GAN-type classifier (real/fake). 
The k-means loss applies to the centroids but also the encoder; for stability reason, the centroids are updated less frequently than the network weights (every 10 epochs in the paper). 
They added a classifier to improve the quality of the reconstruction; they generate fake time series by randomly permutating the entries of the real ones.

Interesting paper overall. Seems a bit convoluted, but show SOTA results on a few of the UCR classification datasets.},
	author = {Ma, Qianli and Zheng, Jiawei and Li, Sen and Cottrell, Gary W},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2020-02-11 16:23:10 -0500},
	date-modified = {2020-04-22 11:21:36 -0400},
	keywords = {DTCR, clustering, k-means, seq2seq},
	pages = {3776--3786},
	read = {1},
	title = {Learning Representations for Time Series Clustering},
	year = {2019},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/8634-learning-representations-for-time-series-clustering.pdf}}

@article{aghabozorgi2015time,
	author = {Aghabozorgi, Saeed and Shirkhorshidi, Ali Seyed and Wah, Teh Ying},
	date-added = {2020-02-11 14:31:16 -0500},
	date-modified = {2020-05-21 14:59:28 -0400},
	journal = {Information Systems},
	keywords = {time series, clustering, algorithm},
	pages = {16--38},
	publisher = {Elsevier},
	read = {1},
	title = {Time-series clustering--a decade review},
	volume = {53},
	year = {2015},
	Bdsk-Url-1 = {https://wiki.smu.edu.sg/18191isss608g1/img_auth.php/f/fd/Time_Series_Clustering_A_Decade_Review.pdf}}

@inproceedings{yi2019not,
	abstract = {Handling missing data is one of the most fundamental problems in machine learning.
Among many approaches, the simplest and most intuitive way is zero imputation,
which treats the value of a missing entry simply as zero. However, many studies
have experimentally confirmed that zero imputation results in suboptimal performances in training neural networks. Yet, none of the existing work has explained
what brings such performance degradations. In this paper, we introduce the variable sparsity problem (VSP), which describes a phenomenon where the output
of a predictive model largely varies with respect to the rate of missingness in the
given input, and show that it adversarially affects the model performance. We first
theoretically analyze this phenomenon and propose a simple yet effective technique
to handle missingness, which we refer to as Sparsity Normalization (SN), that
directly targets and resolves the VSP. We further experimentally validate SN on
diverse benchmark datasets, to show that debiasing the effect of input-level sparsity
improves the performance and stabilizes the training of neural networks.},
	author = {Yi, Joonyoung and Lee, Juhyuk and Hwang, Sung Ju and Yang, Eunho},
	date-added = {2020-02-06 14:04:23 -0500},
	date-modified = {2020-04-22 07:42:25 -0400},
	keywords = {missing data, zero imputation, deep learning},
	read = {1},
	title = {Why Not to Use Zero Imputation? Correcting Sparsity Bias in Training Neural Networks},
	year = {2019},
	Bdsk-Url-1 = {https://openreview.net/pdf?id=BylsKkHYvH}}

@article{bauckhage2015numpy,
	abstract = {In this note, we study least squares optimization for parameter estimation. By means of the basic example of a linear regression task, we explore different formulations of the ordinary least squares problem, show how to solve it using NumPy or SciPy, and provide suggestions for practical applications.},
	author = {Bauckhage, Christian},
	date-added = {2020-02-05 15:21:33 -0500},
	date-modified = {2020-04-22 10:28:01 -0400},
	journal = {researchgate. net, Mar},
	keywords = {linear regression, numpy/scipy},
	title = {NumPy/SciPy Recipes for Data Science: Ordinary Least Squares Optimization},
	year = {2015},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAjLi4vLi4vLi4vLlRyYXNoL25wLXNwLXJlY2lwZXMtNS5wZGZPEQFcAAAAAAFcAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8TbnAtc3AtcmVjaXBlcy01LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAIAAAogY3UAAAAAAAAAAAAAAAAABi5UcmFzaAACAC0vOlVzZXJzOmJlbmNyZXN0ZWw6LlRyYXNoOm5wLXNwLXJlY2lwZXMtNS5wZGYAAA4AKAATAG4AcAAtAHMAcAAtAHIAZQBjAGkAcABlAHMALQA1AC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgArVXNlcnMvYmVuY3Jlc3RlbC8uVHJhc2gvbnAtc3AtcmVjaXBlcy01LnBkZgAAEwABLwAAFQACABH//wAAAAgADQAaACQASgAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAGq}}

@article{DBLP:journals/corr/abs-1803-03635,
	annote = {Conjecture that over-parametrized network are easier to train b/c they contain smaller networks with similar high accuracy},
	archiveprefix = {arXiv},
	author = {Jonathan Frankle and Michael Carbin},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/bib/journals/corr/abs-1803-03635},
	date-added = {2020-02-05 14:40:58 -0500},
	date-modified = {2020-02-05 14:42:58 -0500},
	eprint = {1803.03635},
	journal = {CoRR},
	keywords = {lottery ticket, over-parametrized, pruning},
	read = {1},
	timestamp = {Mon, 13 Aug 2018 16:48:29 +0200},
	title = {The Lottery Ticket Hypothesis: Training Pruned Neural Networks},
	url = {http://arxiv.org/abs/1803.03635},
	volume = {abs/1803.03635},
	year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1803.03635}}

@article{DBLP:journals/corr/abs-1906-04705,
	abstract = {Least-mean squares (LMS) solvers such as Linear / Ridge / Lasso-Regression,
SVD and Elastic-Net not only solve fundamental machine learning problems, but
are also the building blocks in a variety of other methods, such as decision trees
and matrix factorizations.
We suggest an algorithm that gets a finite set of n d-dimensional real vectors and
returns a weighted subset of d + 1 vectors whose sum is exactly the same. The
proof in Caratheodory's Theorem (1907) computes such a subset in O(n^2 d^2) time 
and thus not used in practice. Our algorithm computes this subset in O(nd) time,
using O(log n) calls to Caratheodory's construction on small but "smart" subsets.
This is based on a novel paradigm of fusion between different data summarization
techniques, known as sketches and coresets.
As an example application, we show how it can be used to boost the performance
of existing LMS solvers, such as those in scikit-learn library, up to x100. 
Generalization for streaming and distributed (big) data is trivial. Extensive experimental
results and complete open source code are also provided.},
	annote = {Claim major speed-ups compared to other sketching methods.
Honorable Mention Outstanding Paper Award @ NeurIPS 2019},
	archiveprefix = {arXiv},
	author = {Alaa Maalouf and Ibrahim Jubran and Dan Feldman},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/bib/journals/corr/abs-1906-04705},
	date-added = {2020-02-05 14:29:15 -0500},
	date-modified = {2020-04-22 10:20:35 -0400},
	eprint = {1906.04705},
	journal = {CoRR},
	keywords = {sketching, least mean squares, big data},
	read = {1},
	timestamp = {Fri, 14 Jun 2019 09:38:24 +0200},
	title = {Fast and Accurate Least-Mean-Squares Solvers},
	url = {http://arxiv.org/abs/1906.04705},
	volume = {abs/1906.04705},
	year = {2019},
	Bdsk-Url-1 = {http://arxiv.org/abs/1906.04705}}

@article{araya2017automated,
	annote = {Not clear how good of an idea that is. In the paper, they claim DNN are a good idea to approximate physics processes, but I'm not so sure how true that is.},
	author = {Araya-Polo, Mauricio and Dahlke, Taylor and Frogner, Charlie and Zhang, Chiyuan and Poggio, Tomaso and Hohl, Detlef},
	date-added = {2020-01-14 14:49:38 -0500},
	date-modified = {2020-01-14 22:41:34 -0500},
	journal = {The Leading Edge},
	keywords = {seismic processing, wasserstein, image segmentation, cnn},
	number = {3},
	pages = {208--214},
	publisher = {Society of Exploration Geophysicists},
	read = {1},
	title = {Automated fault detection without seismic processing},
	volume = {36},
	year = {2017}}

@inproceedings{Graves:2006aa,
	abstract = {Many real-world sequence learning tasks require the prediction of sequences of labels
from noisy, unsegmented input data. In
speech recognition, for example, an acoustic
signal is transcribed into words or sub-word
units. Recurrent neural networks (RNNs) are
powerful sequence learners that would seem
well suited to such tasks. However, because
they require pre-segmented training data,
and post-processing to transform their outputs into label sequences, their applicability
has so far been limited. This paper presents a
novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the
TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a
hybrid HMM-RNN.},
	annote = {Easy to digest summary of CTC: https://distill.pub/2017/ctc/. ``Connectionist Temporal Classification (CTC) is a way to get around not knowing the alignment between the input and the output''},
	author = {Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
	booktitle = {Proceedings of the 23rd international conference on Machine learning},
	date-added = {2019-12-10 13:50:44 -0800},
	date-modified = {2020-04-21 12:51:39 -0400},
	keywords = {deep learning, speech-to-text, ocr, ctc},
	organization = {ACM},
	pages = {369--376},
	read = {1},
	title = {Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks},
	year = {2006},
	Bdsk-Url-1 = {https://www.cs.toronto.edu/~graves/icml_2006.pdf}}

@article{mehra2019penalty,
	abstract = {Bilevel optimizations are at the center of several important machine learning problems such
as hyperparameter tuning, data denoising, fewshot learning, data poisoning. Different from simultaneous or multi-objective optimization, obtaining the exact descent direction for continuous bilevel optimization requires computing the
inverse of the hessian of the lower-level cost
function, even for first order methods. In this
paper, we propose a new method for solving
bilevel optimization, using the penalty function,
which avoids computing the inverse of the hessian. We prove convergence of the method under mild conditions and show that it computes
the exact hypergradient asymptotically. Small
space and time complexity of our method allows us to solve large-scale bilevel optimization problems involving deep neural networks
with up to 3.8M upper-level and 1.4M lowerlevel variables. We present results of our method
for data denoising on MNIST/CIFAR10/SVHN
datasets, for few-shot learning on Omniglot/MiniImagenet datasets and for training-data poisoning
on MNIST/Imagenet datasets. In all experiments,
our method outperforms or is comparable to previously proposed methods both in terms of accuracy
and run-time},
	author = {Mehra, Akshay and Hamm, Jihun},
	date-modified = {2019-12-10 13:52:10 -0800},
	journal = {arXiv preprint arXiv:1911.03432},
	keywords = {bilevel optimization, penalty method,},
	read = {1},
	title = {Penalty Method for Inversion-Free Deep Bilevel Optimization},
	url = {https://arxiv.org/pdf/1911.03432.pdf},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1911.03432.pdf}}

@article{pedregosa2016hyperparameter,
	abstract = {Most models in machine learning contain at least
one hyperparameter to control for model complexity. Choosing an appropriate set of hyperparameters is both crucial in terms of model accuracy and computationally challenging. In this
work we propose an algorithm for the optimization of continuous hyperparameters using inexact gradient information. An advantage of this
method is that hyperparameters can be updated
before model parameters have fully converged.
We also give sufficient conditions for the global
convergence of this method, based on regularity
conditions of the involved functions and summability of errors. Finally, we validate the empirical
performance of this method on the estimation of
regularization constants of `2-regularized logistic regression and kernel Ridge regression. Empirical benchmarks indicate that our approach is
highly competitive with respect to state of the art
methods.},
	author = {Pedregosa, Fabian},
	date-modified = {2020-12-15 19:23:41 -0500},
	journal = {arXiv preprint arXiv:1602.02355},
	keywords = {hyperparameter},
	read = {1},
	title = {Hyperparameter optimization with approximate gradient},
	url = {https://arxiv.org/pdf/1602.02355.pdf},
	year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1602.02355.pdf}}

@article{lorraine2019optimizing,
	abstract = {We propose an algorithm for inexpensive
gradient-based hyperparameter optimization
that combines the implicit function theorem
(IFT) with efficient inverse Hessian approximations. We present results on the relationship between the IFT and differentiating through optimization, motivating our algorithm. We use the proposed approach to
train modern network architectures with millions of weights and millions of hyperparameters. We learn a data-augmentation network---
where every weight is a hyperparameter tuned
for validation performance---that outputs augmented training examples; we learn a distilled
dataset where each feature in each datapoint
is a hyperparameter; and we tune millions
of regularization hyperparameters. Jointly
tuning weights and hyperparameters with our
approach is only a few times more costly in
memory and compute than standard training.},
	annote = {Not super novel, more like putting together a lot of good ideas. But it's well explained and interesting. I'm still not sure about Laplacian approximation, though. Need to read the actual paper for that.},
	author = {Lorraine, Jonathan and Vicol, Paul and Duvenaud, David},
	date-modified = {2020-04-30 09:36:48 -0400},
	journal = {arXiv preprint arXiv:1911.02590},
	keywords = {hyperparameter, bilevel optimization, inverse Hessian approximation},
	read = {1},
	title = {Optimizing Millions of Hyperparameters by Implicit Differentiation},
	url = {https://arxiv.org/pdf/1911.02590.pdf},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1911.02590.pdf}}
